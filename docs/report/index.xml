<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Cybertraining – Reports</title>
    <link>/report/</link>
    <description>Recent content in Reports on Cybertraining</description>
    <generator>Hugo -- gohugo.io</generator>
    
	  <atom:link href="/report/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Report: </title>
      <link>/report/cloudmesh-openapi/cloudmesh/openapi/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/cloudmesh-openapi/cloudmesh/openapi/readme/</guid>
      <description>
        
        
        &lt;h1 id=&#34;openapi-function-generator&#34;&gt;Openapi Function generator&lt;/h1&gt;
&lt;h2 id=&#34;activity-log&#34;&gt;Activity Log&lt;/h2&gt;
&lt;h2 id=&#34;week-of-mar-9---mar-16&#34;&gt;Week of Mar 9 - Mar 16&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Andrew Goldfarb&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Worked with Ishan and Jonathan to finalize the start stop
functionality.&lt;/li&gt;
&lt;li&gt;Added functionality to delete the process entry from the
registry upon stop command.&lt;/li&gt;
&lt;li&gt;Debugged weird start error for my personal machine where the
start functionality was running two bash terminals causing the
start function to fail.&lt;/li&gt;
&lt;li&gt;Met with Professor to discuss proper implementation of the
start/stop and how to tie into registry functionality.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;week-prior-to-mar-9th&#34;&gt;Week prior to Mar 9th&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;bkgerreis&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Jonathan Beckford&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Prateek&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Andrew Goldfarb&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;Edited the stop function to take process PID and use os.kill to
stop the process based on the name of the python file. However,
according to Ishan this is still not working.&lt;/li&gt;
&lt;li&gt;Resolved conflicts between main and our working branch&lt;/li&gt;
&lt;li&gt;Began work on assigning a default name if the user does not provide
one for server start. Potetially, a function to assign an alias
name to the whole process to amke it easier to reference.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;install-for-development&#34;&gt;Install for development&lt;/h2&gt;
&lt;p&gt;cloudmesh-installer git pull analytics&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd cloudmesh-openapi
pip install -e .
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;keep-up-to-date&#34;&gt;Keep up to date&lt;/h2&gt;
&lt;p&gt;explain how to set up and use upstream sync&lt;/p&gt;
&lt;h3 id=&#34;project-meeting&#34;&gt;Project Meeting&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://iu.zoom.us/rec/share/4dIpJZ-p8ztIHpH_q1HAZ6wzL6iiaaa8h3QX8_YMzRkn8tBfY_mRIe8z3j-3cZ_9?startTime=1581987567000&#34;&gt;Mon 17 Feb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://iu.zoom.us/rec/share/_8ZLKK7Z6zpLb53f73_UW4EFBY_iX6a8gydM_vVbzRu2MhrC_sUCKhChUkLzgEK8?startTime=1582591839000&#34;&gt;Mon 24 Feb&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;basic-function-generator&#34;&gt;Basic Function Generator&lt;/h3&gt;
&lt;h4 id=&#34;prateek-shaw----code-link&#34;&gt;Prateek Shaw -  code link.&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cloudmesh-community/sp20-516-229/tree/main/cloudmesh-openapi&#34;&gt;https://github.com/cloudmesh-community/sp20-516-229/tree/main/cloudmesh-openapi&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;created a basic function that will return the OpenAPI YAML file
of given python function including parameters.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;sp20-516-237----jonathan-beckford&#34;&gt;SP20-516-237 &amp;ndash; Jonathan Beckford&lt;/h4&gt;
&lt;p&gt;I created a class that generates the OpenAPI yaml file. I also created
a sample program that defines an example function, instantiates my
OpenAPI generator class and passes in the sample function as input. I
figured this would make things really easy to just paste any new
sample function for testing purposes. I also included the parameters
as was requested. I also ran my output yaml through the swagger
validator (&lt;a href=&#34;https://editor.swagger.io/&#34;&gt;https://editor.swagger.io/&lt;/a&gt;) to make sure it was compliant
and it was.&lt;br&gt;
&lt;a href=&#34;https://github.com/cloudmesh-community/sp20-516-237/tree/main/projectAI/generateOpenAPI&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;sp20-516-231---brian-kegerreis&#34;&gt;sp20-516-231 - Brian Kegerreis&lt;/h4&gt;
&lt;p&gt;I created a function to generate an OpenAPI spec including a rough
attempt at response types (only supports text/plain media types at
this point)
&lt;a href=&#34;https://github.com/cloudmesh-community/sp20-516-231/blob/main/openapi-exercises/example_echo.py&#34;&gt;https://github.com/cloudmesh-community/sp20-516-231/blob/main/openapi-exercises/example_echo.py&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;server-start&#34;&gt;Server Start&lt;/h3&gt;
&lt;h4 id=&#34;andrew-goldfarb---sp20-516-234&#34;&gt;Andrew Goldfarb - SP20-516-234&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cloudmesh-community/sp20-516-234/tree/open-api-exercise/openAPI&#34;&gt;https://github.com/cloudmesh-community/sp20-516-234/tree/open-api-exercise/openAPI&lt;/a&gt;
I have created a basic function that returns the IP address of the
server running the function to tell if it is running on the device
itself or connected to the internet running while running the
function.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/cloudmesh-openapi/cloudmesh/openapi/scikitlearn/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/cloudmesh-openapi/cloudmesh/openapi/scikitlearn/readme/</guid>
      <description>
        
        
        &lt;h1 id=&#34;sklearngeneratorfile-high-level-overview&#34;&gt;SKlearnGeneratorFile High level Overview&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The SklearnGeneratorFile.py is the generator function which outputs the python file for given
Sckit-learn Library.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The function takes two inputs&lt;/p&gt;
&lt;p&gt;1.input_sklibrary&lt;/p&gt;
&lt;p&gt;2.model_tag&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Examples of the inputs are&lt;/p&gt;
&lt;p&gt;input_sklibrary = sklearn.linear_model.LinearRegression(Full model specification)
model_tag = any name which you want the tag the model instance like LinReg1&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;This Version of Scikit-learn service accepts csv files in UTF-8 format only.It is the user responsibility to make
sure the files are in UTF-8 format.It is the user responsibility to split the data in to train and test datasets.
Split data functionality is not currently supported.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;scikit-learn uses numpydoc format in the docstring so the scraping of the parameters and docstrings
are done using docscrape from numpydoc.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;All the templates used in the code are based on X and y inputs scikit-learn takes and also based on the
return type&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;pytests-for-scikit-learn-tests&#34;&gt;Pytests for Scikit learn tests.&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The below pytest generates the .py file used by generator to do a OPENAPI specification.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/blob/main/tests/Scikitlearn-tests/test_06c_sklearngeneratortest.py&#34;&gt;Pytestcode&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt; pytest -v --capture&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;no tests/Scikitlearn_tests/test_06c_sklearngeneratortest.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The below pytest tests the methods generated .&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/blob/main/tests/Scikitlearn-tests/test_06d_sklearngeneratortest.py&#34;&gt;Pytestcode&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;pytest -v --capture&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;no tests/Scikitlearn_tests/test_06d_sklearngeneratortest.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/cloudmesh-openapi/deprecated/openapi/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/cloudmesh-openapi/deprecated/openapi/readme/</guid>
      <description>
        
        
        &lt;h1 id=&#34;openapi-function-generator&#34;&gt;Openapi Function generator&lt;/h1&gt;
&lt;h2 id=&#34;activity-log&#34;&gt;Activity Log&lt;/h2&gt;
&lt;h2 id=&#34;week-of-mar-9---mar-16&#34;&gt;Week of Mar 9 - Mar 16&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Andrew Goldfarb&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Worked with Ishan and Jonathan to finalize the start stop
functionality.&lt;/li&gt;
&lt;li&gt;Added functionality to delete the process entry from the
registry upon stop command.&lt;/li&gt;
&lt;li&gt;Debugged weird start error for my personal machine where the
start functionality was running two bash terminals causing the
start function to fail.&lt;/li&gt;
&lt;li&gt;Met with Professor to discuss proper implementation of the
start/stop and how to tie into registry functionality.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;week-prior-to-mar-9th&#34;&gt;Week prior to Mar 9th&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;bkgerreis&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Jonathan Beckford&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Prateek&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Andrew Goldfarb&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;Edited the stop function to take process PID and use os.kill to
stop the process based on the name of the python file. However,
according to Ishan this is still not working.&lt;/li&gt;
&lt;li&gt;Resolved conflicts between main and our working branch&lt;/li&gt;
&lt;li&gt;Began work on assigning a default name if the user does not provide
one for server start. Potetially, a function to assign an alias
name to the whole process to amke it easier to reference.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;install-for-development&#34;&gt;Install for development&lt;/h2&gt;
&lt;p&gt;cloudmesh-installer git pull analytics&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd cloudmesh-openapi
pip install -e .
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;keep-up-to-date&#34;&gt;Keep up to date&lt;/h2&gt;
&lt;p&gt;explain how to set up and use upstream sync&lt;/p&gt;
&lt;h3 id=&#34;project-meeting&#34;&gt;Project Meeting&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://iu.zoom.us/rec/share/4dIpJZ-p8ztIHpH_q1HAZ6wzL6iiaaa8h3QX8_YMzRkn8tBfY_mRIe8z3j-3cZ_9?startTime=1581987567000&#34;&gt;Mon 17 Feb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://iu.zoom.us/rec/share/_8ZLKK7Z6zpLb53f73_UW4EFBY_iX6a8gydM_vVbzRu2MhrC_sUCKhChUkLzgEK8?startTime=1582591839000&#34;&gt;Mon 24 Feb&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;basic-function-generator&#34;&gt;Basic Function Generator&lt;/h3&gt;
&lt;h4 id=&#34;prateek-shaw----code-link&#34;&gt;Prateek Shaw -  code link.&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cloudmesh-community/sp20-516-229/tree/main/cloudmesh-openapi&#34;&gt;https://github.com/cloudmesh-community/sp20-516-229/tree/main/cloudmesh-openapi&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;created a basic function that will return the OpenAPI YAML file
of given python function including parameters.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;sp20-516-237----jonathan-beckford&#34;&gt;SP20-516-237 &amp;ndash; Jonathan Beckford&lt;/h4&gt;
&lt;p&gt;I created a class that generates the OpenAPI yaml file. I also created
a sample program that defines an example function, instantiates my
OpenAPI generator class and passes in the sample function as input. I
figured this would make things really easy to just paste any new
sample function for testing purposes. I also included the parameters
as was requested. I also ran my output yaml through the swagger
validator (&lt;a href=&#34;https://editor.swagger.io/&#34;&gt;https://editor.swagger.io/&lt;/a&gt;) to make sure it was compliant
and it was.
&lt;a href=&#34;https://github.com/cloudmesh-community/sp20-516-237/tree/main/projectAI/generateOpenAPI&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;sp20-516-231---brian-kegerreis&#34;&gt;sp20-516-231 - Brian Kegerreis&lt;/h4&gt;
&lt;p&gt;I created a function to generate an OpenAPI spec including a rough
attempt at response types (only supports text/plain media types at
this point)
&lt;a href=&#34;https://github.com/cloudmesh-community/sp20-516-231/blob/main/openapi-exercises/example_echo.py&#34;&gt;https://github.com/cloudmesh-community/sp20-516-231/blob/main/openapi-exercises/example_echo.py&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;server-start&#34;&gt;Server Start&lt;/h3&gt;
&lt;h4 id=&#34;andrew-goldfarb---sp20-516-234&#34;&gt;Andrew Goldfarb - SP20-516-234&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cloudmesh-community/sp20-516-234/tree/open-api-exercise/openAPI&#34;&gt;https://github.com/cloudmesh-community/sp20-516-234/tree/open-api-exercise/openAPI&lt;/a&gt;
I have created a basic function that returns the IP address of the
server running the function to tell if it is running on the device
itself or connected to the internet running while running the
function.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/cloudmesh-openapi/docker/ubuntu19.10/todo/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/cloudmesh-openapi/docker/ubuntu19.10/todo/</guid>
      <description>
        
        
        &lt;ul&gt;
&lt;li&gt;
&lt;p&gt;make clean: only delete the artifacts created here, the clean wipes
currently everything&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;when using this in consecutive order, would cms init not wipe out the data&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;should we not mount the .cloudmesh and other data dire into the conatiner.
This way we can use the host system for developments. Maybe we need to&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;support both ways&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;leverage cmsd
we have told the class that we have cmsd taht starts up cloudmesh in
a container. Develop a new directory docker-cmsd and use that&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/cloudmesh-openapi/docker/ubuntu20.04-sklearn/todo/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/cloudmesh-openapi/docker/ubuntu20.04-sklearn/todo/</guid>
      <description>
        
        
        &lt;ul&gt;
&lt;li&gt;
&lt;p&gt;THIS IS JUST THE SKELETON AND HAS NOT BEEN RUN ONCE IT HAS BUGS&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;THIS HAS NOT YET THE SKLERAN START STOP&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;make clean: only delete the artifacts created here, the clean wipes
currently everything&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;when using this in consecutive order, would cms init not wipe out the data&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;should we not mount the .cloudmesh and other data dire into the conatiner.
This way we can use the host system for developments. Maybe we need to&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;support both ways&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;leverage cmsd
we have told the class that we have cmsd taht starts up cloudmesh in
a container. Develop a new directory docker-cmsd and use that&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/cloudmesh-openapi/docker/ubuntu20.04/todo/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/cloudmesh-openapi/docker/ubuntu20.04/todo/</guid>
      <description>
        
        
        &lt;ul&gt;
&lt;li&gt;
&lt;p&gt;THIS IS JUST THE SKELETON AND HAS NOT BEEN RUN ONCE IT HAS BUGS&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;make clean: only delete the artifacts created here, the clean wipes
currently everything&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;when using this in consecutive order, would cms init not wipe out the data&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;should we not mount the .cloudmesh and other data dire into the conatiner.
This way we can use the host system for developments. Maybe we need to&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;support both ways&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;leverage cmsd
we have told the class that we have cmsd taht starts up cloudmesh in
a container. Develop a new directory docker-cmsd and use that&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/cloudmesh-openapi/paper/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/cloudmesh-openapi/paper/</guid>
      <description>
        
        
        &lt;h1 id=&#34;benchmarking-multi-cloud-auto-generated-ai-services&#34;&gt;Benchmarking Multi-Cloud Auto Generated AI Services&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/actions&#34;&gt;&lt;img src=&#34;https://github.com/cloudmesh/cloudmesh-openapi/workflows/Check%20Report/badge.svg&#34; alt=&#34;Check Report&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;NOTE:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This document is maintained at:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/blob/main/paper/_index.md&#34;&gt;https://github.com/cloudmesh/cloudmesh-openapi/blob/main/paper/_index.md&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://laszewski.github.io&#34;&gt;Gregor von Laszewski&lt;/a&gt;,
Richard Otten,
&lt;a href=&#34;https://github.com/aporlowski&#34;&gt;Anthony Orlowski&lt;/a&gt;, &lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-310/&#34;&gt;fa20-523-310&lt;/a&gt;,
&lt;a href=&#34;https://github.com/calewils&#34;&gt;Caleb Wilson&lt;/a&gt;, &lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-348/&#34;&gt;fa20-523-348&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Corresponding author: &lt;a href=&#34;mailto:laszewski@gmail.com&#34;&gt;laszewski@gmail.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-348/blob/main/project/project.md&#34;&gt;Edit&lt;/a&gt;&lt;/p&gt;


&lt;div class=&#34;pageinfo pageinfo-primary&#34;&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;In this work we are benchmarking auto generated cloud REST services on
various clouds. In today&amp;rsquo;s application scientist want to share their
services with a wide number of colleagues while not only offering the
services as bare metal programs, but exposing the functionality as a
software as a service. For this reason a tool has been developed that
takes a regular python function and converts it automatically into a
secure REST service. We will create a number of AI REST services while
using examples from ScikitLearn and benchmark the execution of the
resulting REST services on various clouds. The code will be
accompanied by benchmark enhanced unit tests as to allow replication
of the test on the users computer. A comparative study of the results
is included in our evaluation.&lt;/p&gt;
&lt;p&gt;Contents&lt;/p&gt;
&lt;div class=&#34;toc&#34;&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#1-introduction&#34;&gt;1. Introduction&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#2-background-and-related-research&#34;&gt;2. Background and Related Research&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#21-cloudmesh&#34;&gt;2.1 Cloudmesh&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#22-rest&#34;&gt;2.2 REST&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#23-vm-cloud-providers&#34;&gt;2.3 VM Cloud providers&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#24-containers-and-microservices&#34;&gt;2.4 Containers and Microservices&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#3-architecture&#34;&gt;3. Architecture&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#31-basic-auth-security&#34;&gt;3.1 Basic Auth Security&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#32-utilizing-pickle-as-an-alternative-to-mongodb-for-out-of-the-box-functionality&#34;&gt;3.2 Utilizing Pickle as an alternative to MongoDB for &amp;ldquo;out-of-the-box&amp;rdquo; functionality&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#33-cloud-provider-hosted-ai-service&#34;&gt;3.3 Cloud Provider Hosted AI Service&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#34-multi-cloud-hosted-ai-service&#34;&gt;3.4 Multi-Cloud Hosted AI Service&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#4-benchmarks&#34;&gt;4. Benchmarks&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#41-algorithms-and-datasets&#34;&gt;4.1. Algorithms and Datasets&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#42-cloud-providers&#34;&gt;4.2. Cloud Providers&lt;/a&gt;
          &lt;ul&gt;
            &lt;li&gt;&lt;/li&gt;
          &lt;/ul&gt;
        &lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#43-result-comparision&#34;&gt;4.3. Result Comparision&lt;/a&gt;
          &lt;ul&gt;
            &lt;li&gt;&lt;a href=&#34;#431-vm-selection&#34;&gt;4.3.1 VM Selection&lt;/a&gt;&lt;/li&gt;
            &lt;li&gt;&lt;a href=&#34;#432-eigenfaces-svm-example&#34;&gt;4.3.2 Eigenfaces-SVM Example&lt;/a&gt;&lt;/li&gt;
            &lt;li&gt;&lt;a href=&#34;#4321-single-cloud-provider-service-benchmarking&#34;&gt;4.3.2.1 Single Cloud Provider Service Benchmarking&lt;/a&gt;&lt;/li&gt;
            &lt;li&gt;&lt;a href=&#34;#4322-multi-cloud-service-benchmarking&#34;&gt;4.3.2.2 Multi-Cloud Service Benchmarking&lt;/a&gt;&lt;/li&gt;
            &lt;li&gt;&lt;a href=&#34;#433-pipelined-anova-svm-example&#34;&gt;4.3.3 Pipelined Anova SVM Example&lt;/a&gt;&lt;/li&gt;
            &lt;li&gt;&lt;a href=&#34;#434-caleb-example&#34;&gt;4.3.4 Caleb Example&lt;/a&gt;&lt;/li&gt;
          &lt;/ul&gt;
        &lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#5-conclusion&#34;&gt;5. Conclusion&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#6-limitations&#34;&gt;6. Limitations&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#appendix-a---setup&#34;&gt;APPENDIX A. - Setup&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#a1-deployment&#34;&gt;A.1. Deployment&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#a2--pipiline-anova-svm&#34;&gt;A.2.  Pipiline ANOVA SVM&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#a3--eigenfaces-svm-facial-recognition&#34;&gt;A.3.  Eigenfaces SVM Facial Recognition&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#a4-using-unit-tests-for-benchmarking&#34;&gt;A.4. Using unit tests for Benchmarking&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#a5-basic-auth-example&#34;&gt;A.5. Basic Auth Example&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#a6-switching-between-pickledb-and-mongodb&#34;&gt;A.6 Switching between PickleDB and MongoDB&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#appendix-b---code-location&#34;&gt;APPENDIX B. - Code Location&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#appendix-c---cloudmesh-links&#34;&gt;APPENDIX C. - Cloudmesh Links&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#appendix-d---plan&#34;&gt;APPENDIX D. - Plan&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#acknowledgements&#34;&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Keywords:&lt;/strong&gt; cloudmesh, AI service, REST, multi-cloud&lt;/p&gt;
&lt;h2 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;We will develop benchmark tests that are pytest replications of
Sklearn artificial intelligent algorithms. These pytests will then be
ran on different cloud services to benchmark different statistics on
how they run and how the cloud performs. The team will obtain cloud
service accounts from AWS, Azure, Google, and OpenStack. To deploy the
pytests, the team will use Cloudmesh and its Openapi based REST
services to benchmark the performance on different cloud
services. Benchmarks will include components like data transfer time,
model train time, model prediction time, and more. The final project
will include scripts and code for others to use and replicate our
tests. The team will also make a report consisting of research and
findings.&lt;/p&gt;
&lt;h2 id=&#34;2-background-and-related-research&#34;&gt;2. Background and Related Research&lt;/h2&gt;
&lt;h3 id=&#34;21-cloudmesh&#34;&gt;2.1 Cloudmesh&lt;/h3&gt;
&lt;p&gt;Cloudmesh &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; is a service that enables users to access multi-cloud
environments easily. Cloudmesh is an evolution of previous tools that
have been used by many users. Cloudmesh makes interacting with
clouds easy by creating a service mashup to access common cloud
services across numerous cloud platforms. Cloudmesh contains a
sophisticated command shell, a database to store jason objects
representing virtual machines, storage and a registry of REST
services &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;. Cloudmesh has a sophisticated plugin concept that is easy to
use and leverages python namespaces while being able to integrate
plugins from different source code directories &lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;. Installation of
Cloudmesh is available for macOS, Linux, Windows, and Rasbian &lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h3 id=&#34;22-rest&#34;&gt;2.2 REST&lt;/h3&gt;
&lt;p&gt;REST is an acronym for representational state transfer. REST often
uses the HTTP protocol for the CRUD functions which create, read,
update, and delete resources. It is important to note that REST is not
a standard, but it is a software architectural style for building
network services. When referred to as a part of the HTTP protocol, REST has the
methods of GET, PUT, POST, and DELETE. These methods are used to
implement the CRUD functions on collections and items that REST
introduces &lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Collection of resources&lt;/strong&gt; &lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;: Assume the URI,
&lt;code&gt;http://.../resources/&lt;/code&gt;, identifies a collection of resources. The
following CRUD functions would be implemented:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;GET&lt;/strong&gt;: List the URIs and details about the collection’s items.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PUT&lt;/strong&gt;: Replace the collection with a different collection.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;POST&lt;/strong&gt;: Make a new entry in the collection. The operation returns
new entry’s URI and assigns it automatically.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DELETE&lt;/strong&gt;: Delete the collection.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Single Resource&lt;/strong&gt; &lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;: Assume the URI,
&lt;code&gt;http://.../resources/item58&lt;/code&gt;, identifies a single resource in a
collection. The following CRUD functions would be implemented:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;GET&lt;/strong&gt;: Fetch a representation of the item in the collection,
extracted in the appropriate media type.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PUT&lt;/strong&gt;: Replace the item in the collection. If the item does not
exist, then create the item.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;POST&lt;/strong&gt;: Typically, not used. Treat the item as a collection and
make a new entry in it.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DELETE&lt;/strong&gt;: Delete the item in the collection.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Because REST has a defined structure, there are tools that manage
programming to REST specifications.  Here are different categories&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;REST Specification Frameworks&lt;/strong&gt;: Frameworks to define REST service
specifications for generating REST services in a language and
framework independently, include: Swagger 2.0 &lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;,
OpenAPI 3.0 &lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;, and RAML &lt;sup id=&#34;fnref:9&#34;&gt;&lt;a href=&#34;#fn:9&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;REST programming language support&lt;/strong&gt;: Tools and services for
targeting specific programming languages, include: Flask Rest
&lt;sup id=&#34;fnref:10&#34;&gt;&lt;a href=&#34;#fn:10&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;10&lt;/a&gt;&lt;/sup&gt;, Django Rest Services &lt;sup id=&#34;fnref:11&#34;&gt;&lt;a href=&#34;#fn:11&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;11&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;REST documentation-based tools&lt;/strong&gt;: These tools document REST
specifications. One such tool is Swagger &lt;sup id=&#34;fnref:12&#34;&gt;&lt;a href=&#34;#fn:12&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;12&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;REST design support tools&lt;/strong&gt;: These tools support the design
process in developing REST services while extracting on top of the
programming languages.  These tools also define reusable to create
clients and servers for particular targets.These tools include
Swagger &lt;sup id=&#34;fnref:12&#34;&gt;&lt;a href=&#34;#fn:12&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;12&lt;/a&gt;&lt;/sup&gt; , additional swagger tools are available at
OpenAPI Tools &lt;sup id=&#34;fnref:13&#34;&gt;&lt;a href=&#34;#fn:13&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;13&lt;/a&gt;&lt;/sup&gt; to generate code from OpenAPI
specifications &lt;sup id=&#34;fnref:14&#34;&gt;&lt;a href=&#34;#fn:14&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;14&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;23-vm-cloud-providers&#34;&gt;2.3 VM Cloud providers&lt;/h3&gt;
&lt;p&gt;Cloud computing providers offer their customers on-demand self-service computing resources that are rapidly elastic and accessible via broad network access &lt;sup id=&#34;fnref:15&#34;&gt;&lt;a href=&#34;#fn:15&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;15&lt;/a&gt;&lt;/sup&gt;. They accomplish this through the economies of scale achieved by resource pooling (serving multiple customers on the same hardware) and using measured services for fine grained customer billing &lt;sup id=&#34;fnref:15&#34;&gt;&lt;a href=&#34;#fn:15&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;15&lt;/a&gt;&lt;/sup&gt;. Cloud providers offer these resources in multiple service models including infrastructure as a service, platform as a service, software as a service, and, recently, function as a service &lt;sup id=&#34;fnref:15&#34;&gt;&lt;a href=&#34;#fn:15&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;15&lt;/a&gt;&lt;/sup&gt;. These providers are rapidly offering new platforms and services ranging from bare-metal machines to AI development platforms like Google’s TensorFlow Enterprise platform &lt;sup id=&#34;fnref:16&#34;&gt;&lt;a href=&#34;#fn:16&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;16&lt;/a&gt;&lt;/sup&gt;, and AI services such as Amazon’s text-to-speech service &lt;sup id=&#34;fnref:17&#34;&gt;&lt;a href=&#34;#fn:17&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;17&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;Customers can take advantage of cloud computing to reduce overhead expenses, increase their speed and scale of service deployment, and reduce development requirements by using cloud providers’ platforms or services. For example, customers’ developing AI systems can utilize clouds to handle big data inputs for which private infrastructure would be too costly or slow to implement. However, having multiple competing cloud providers leads to situations where service availability, performance, and cost may vary. Customer’s must navigate these heterogeneous solutions to meet their business needs while avoiding provider lock-in and managing organizational risk. This may require comparing or using multiple cloud providers to meet various objectives.&lt;/p&gt;
&lt;p&gt;Cloudmesh works with a variety of cloud providers including Amazon Web Services, Microsoft Azure, Google Cloud Platform, and Oracle’s OpenStack based providers such as the academic research serving Chameleon Cloud.&lt;/p&gt;
&lt;h3 id=&#34;24-containers-and-microservices&#34;&gt;2.4 Containers and Microservices&lt;/h3&gt;
&lt;p&gt;Cloudmesh uses the internet architectures of microservices and containers to organize its functions and code. Microservice architecture is a variant of service oriented architecture. Cloudmesh uses microservices to arrange its app in loosley coupled services. These services are fine grained and protocals are lightweight. Containers are data structures whose instances are collections of other objects. Cloudmesh uses containers to store objects in an organized way that follows specific access rules. Containers are characterized by three properties: accessing objects in the container, storage of the objects, and traversal of the objects. Cloudmesh is designed to work on containers in docker, and kubernetes.&lt;/p&gt;
&lt;h2 id=&#34;3-architecture&#34;&gt;3. Architecture&lt;/h2&gt;
&lt;h3 id=&#34;31-basic-auth-security&#34;&gt;3.1 Basic Auth Security&lt;/h3&gt;
&lt;p&gt;Cloudmesh OpenAPI supports configuration of a single authorized user through
basic authentication. Basic authentication is a simple authentication scheme built
into the HTTP protocol. The client sends HTTP requests with the &lt;code&gt;Authorization&lt;/code&gt;
header that contains the word &lt;code&gt;Basic&lt;/code&gt; followed by a space and a base64-encoded
string of the format &lt;code&gt;username:password&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;From wikipedia, basic auth is &amp;ldquo;a method for an HTTP user agent (e.g. a web browser) to provide a user name and password when making a request&amp;rdquo;. (&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi&#34;&gt;more on this&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;A cloudmesh user can create an OpenAPI server whose endpoints are only accessible
as an authorized user. Currently, when basic auth is used as the authentication mechanism,
all endpoints are secured with this method. While this can be benficial to lock down an API,
it is limited in the sense that is is &amp;ldquo;all or nothing&amp;rdquo;: either all endpoints are secured or none at all.
This is something that can be improved upon in the future.&lt;/p&gt;
&lt;p&gt;For an example of basic auth usage, see &lt;a href=&#34;#a5-basic-auth-example&#34;&gt;Appendix A.5.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Read more about Basic Auth usage with OpenAPI &lt;a href=&#34;https://swagger.io/docs/specification/authentication/basic-authentication/&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;32-utilizing-pickle-as-an-alternative-to-mongodb-for-out-of-the-box-functionality&#34;&gt;3.2 Utilizing Pickle as an alternative to MongoDB for &amp;ldquo;out-of-the-box&amp;rdquo; functionality&lt;/h3&gt;
&lt;p&gt;Currently, the installation and setup of cloudmesh openapi involves the installation of MongoDB and the configuration of mongo variables. This is documented &lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi#installation&#34;&gt;here.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;There have been serveral recent cloudmesh projects involving Raspberry Pis. Unfortunately, the minimum version of MongoDB required for openapi is not available to the Raspberry Pi. Thus cloudmesh-openapi is not available to Pi users with MongoDB.&lt;/p&gt;
&lt;p&gt;In an effort to provide this software to all those that are interested regardless of OS/machine, we have added a new default storage mechanism that functions &amp;ldquo;out-of-the-box&amp;rdquo; with cloudmesh-openapi. This storage mechanism is implemented with python&amp;rsquo;s native Pickle at the heart. All interfaces associated with MongoDB interactions have been extended to support switching to PickleDB. Thus, this addition is backwards compatible with previous versions of cloudmesh-openapi and requires little changes in the existing code base to support. Since Pickle is native to python, it is supported on any platform running python.&lt;/p&gt;
&lt;p&gt;It is important to note that there are essentially no security mechanisms with Pickle. We provide this option for users to test their APIs on different machines with little to no setup, but we do not recommend its usage in a production server.&lt;/p&gt;
&lt;p&gt;See &lt;a href=&#34;#a6-switching-between-pickledb-and-mongodb&#34;&gt;Appendix A.6&lt;/a&gt; to see how to switch between DB protocols.&lt;/p&gt;
&lt;h3 id=&#34;33-cloud-provider-hosted-ai-service&#34;&gt;3.3 Cloud Provider Hosted AI Service&lt;/h3&gt;
&lt;p&gt;A user deploys Cloudmesh OpenAPI on a virtual machine from a cloud provider, and uses it to host auto-generated, RESTful, AI services. A user constructs an AI service as a set of Python functions that implement a workflow, for example, downloading data from a remote server, training an AI model, uploading a new sample for prediction, and running a prediction on that sample. Cloudmesh OpenAPI hosts user provided Python functions on a web server that is accessible using standard HTTP request methods. In Figure 1 we show a remote client accessing a Cloudmesh OpenAPI server to execute an AI service workflow. In this example, the user deployed Cloudmesh OpenAPI on a virtual machine from a single cloud provider. Cloudmesh OpenAPI provides the choice of multiple supported providers to allow users to meet their specific administrative requirements.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cloudmesh/cloudmesh-openapi/raw/main/images/ai-service-workflow.png&#34; alt=&#34;AI Service Workflow&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; A client running an AI service workflow, generated and hosted by Cloudmesh OpenAPI, on a cloud provider virtual machine. Requests for each function invocation are made using standard HTTP request methods including function arguments.&lt;/p&gt;
&lt;h3 id=&#34;34-multi-cloud-hosted-ai-service&#34;&gt;3.4 Multi-Cloud Hosted AI Service&lt;/h3&gt;
&lt;p&gt;Cloudmesh with Cloudmesh OpenAPI provides a framework to deploy AI services to multiple clouds. One use case for a multi-cloud deployment is to benchmark cloud provider VM performance. A user can use these tools to script the deployment of virtual machines with different providers, virtual machine sizes, or operating systems.  In Figure 2, a user has deployed an AI service hosted by Cloudmesh OpenAPI on three separate cloud providers, AWS, Azure, and Google. The user makes standard HTTP method requests to access the services simultaneously, and gathers responses and benchmark statistics. With the Cloudmesh benchmark utility, the user can measure the runtime of each AI service function and collect key information such as virtual machine memory usage. This information provides the user key insight for future hosting decisions. We distinguish this example from Figure 1, where the AI service is deployed on a single cloud provider.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cloudmesh/cloudmesh-openapi/raw/main/images/multi-cloud-ai-service.png&#34; alt=&#34;Mult-Cloud AI Services&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 2:&lt;/strong&gt; A client simultaneously accesses an AI service hosted on three seperate cloud providers, AWS, Azure, and Google to benchmark provider performance.&lt;/p&gt;
&lt;h2 id=&#34;4-benchmarks&#34;&gt;4. Benchmarks&lt;/h2&gt;
&lt;h3 id=&#34;41-algorithms-and-datasets&#34;&gt;4.1. Algorithms and Datasets&lt;/h3&gt;
&lt;p&gt;This project uses a number of simple example algorithms and
datasets. We have chosen to use the examples included in Scikit Learn as
they are widely known and can be used by others to replicate our
benchmarks easily. Nevertheless, it will be possible to integrate
easily other data sources, as well as algorithms due to the generative
nature of our base code for creating REST services.&lt;/p&gt;
&lt;p&gt;Within Skikit Learn we have chosen the following examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Pipelined ANOVA SVM&lt;/strong&gt;: An example code that shows a pipeline running
successively a univariate feature selection with anova and then a
SVM of the selected features &lt;sup id=&#34;fnref:18&#34;&gt;&lt;a href=&#34;#fn:18&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;18&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Eigenfaces SVM Facial Recognition&lt;/strong&gt;: A facial recognition example that first  utilizes principle component analysis (PCA) to generate eigenfaces from the training image data, and then trains and tests a SVM model &lt;sup id=&#34;fnref:19&#34;&gt;&lt;a href=&#34;#fn:19&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;19&lt;/a&gt;&lt;/sup&gt;. This example uses the real world &amp;ldquo;Labeled Faces in the Wild&amp;rdquo; dataset consisting of labeled images of famous individuals gathered from the internet &lt;sup id=&#34;fnref:20&#34;&gt;&lt;a href=&#34;#fn:20&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;20&lt;/a&gt;&lt;/sup&gt;     .&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;42-cloud-providers&#34;&gt;4.2. Cloud Providers&lt;/h3&gt;
&lt;p&gt;Cloudmesh openapi works with virtual machine providers. It is necessary to select similar virtual machines for benchmarking. It is important to select clouds with similar features and virtual machines for benchmarking. Cloudmesh works with these cloud provider virtual machine services: AWS, Azure, Google, OpenStac, Oracle, and Rasberry Pi Cluster.&lt;/p&gt;
&lt;h5 id=&#34;execution-on-raspbian&#34;&gt;Execution on Raspbian&lt;/h5&gt;
&lt;h5 id=&#34;execution-on-a-kubernetes-cluster&#34;&gt;Execution on a Kubernetes Cluster&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://opensource.com/article/20/3/kubernetes-raspberry-pi-k3s&#34;&gt;https://opensource.com/article/20/3/kubernetes-raspberry-pi-k3s&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;43-result-comparision&#34;&gt;4.3. Result Comparision&lt;/h3&gt;
&lt;p&gt;In this section we will discuss the setup and execution of a benchmark for three example AI services.&lt;/p&gt;
&lt;h4 id=&#34;431-vm-selection&#34;&gt;4.3.1 VM Selection&lt;/h4&gt;
&lt;p&gt;When benchmarking cloud performance, it is important to identify and control deployment parameters that can affect the performance results. This enables one to analyze comparable services or identify opportunities for service improvement for varying deployment features such as machine size, location, network, or storage hardware. These examples aimed to create similar machines across all three clouds and measure service performance. See Table 1 for a summary of the parameters controlled in these benchmark examples.&lt;/p&gt;
&lt;p&gt;One key component is the virtual machine size, which determines the number of vCPUs, the amount of memory, attached storage types, and resource sharing policies. Resource sharing policies include shared core machine varieties which providers offer at less expensive rates and allow the virtual machines to burst over its base clock rate in exchange for credits or the machines inherent bursting factor &lt;sup id=&#34;fnref:21&#34;&gt;&lt;a href=&#34;#fn:21&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;21&lt;/a&gt;&lt;/sup&gt; &lt;sup id=&#34;fnref:22&#34;&gt;&lt;a href=&#34;#fn:22&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;22&lt;/a&gt;&lt;/sup&gt;. For this example, we chose three similar machine sizes that had comparable vCPUs, comparable underlying processors, memory, price, and were not a shared core variety. We installed the same Ubuntu 20.04 operating system on all three clouds.&lt;/p&gt;
&lt;p&gt;Another factor that can affect performance, particularly in network latency, is the zone and region selected. We deploy all benchmark machines to zones on the east coast of the United States. This helps control variations caused by network routing latency and provides more insight into the inherent network performance of the individual cloud services.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Table 1:&lt;/strong&gt; Selected VM parameters for benchmark measurement. Clouds were tested at least twice, and were run sequentially between the hours of approximately 1945 EST and 0330 EST starting with Google and ending with Azure. * For the Eigenfaces SVM example, only 60 runs were conducted on Azure due to a failed VM deployment from factors outside of the benchnmark scripts control.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;AWS&lt;/th&gt;
&lt;th&gt;Azure&lt;/th&gt;
&lt;th&gt;Google&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Size (flavor)&lt;/td&gt;
&lt;td&gt;m4.large&lt;/td&gt;
&lt;td&gt;Standard_D2s_v3&lt;/td&gt;
&lt;td&gt;n1-standard-2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;vCPU&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Memory (GB)&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;7.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Image&lt;/td&gt;
&lt;td&gt;ami-0dba2cb6798deb6d8&lt;/td&gt;
&lt;td&gt;Canonical:0001-com-ubuntu-server-focal:20_04-lts:20.04.202006100&lt;/td&gt;
&lt;td&gt;ubuntu-2004-lts&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;OS&lt;/td&gt;
&lt;td&gt;Ubuntu 20.04 LTS&lt;/td&gt;
&lt;td&gt;Ubuntu 20.04 LTS&lt;/td&gt;
&lt;td&gt;Ubuntu 20.04 LTS&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Region&lt;/td&gt;
&lt;td&gt;us-east-1&lt;/td&gt;
&lt;td&gt;eastus&lt;/td&gt;
&lt;td&gt;us-east1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Zone&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;td&gt;us-east1-b&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Price ($/hr)&lt;/td&gt;
&lt;td&gt;0.1&lt;/td&gt;
&lt;td&gt;0.096&lt;/td&gt;
&lt;td&gt;0.0949995&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Runs/Test&lt;/td&gt;
&lt;td&gt;90&lt;/td&gt;
&lt;td&gt;60*&lt;/td&gt;
&lt;td&gt;90&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;432-eigenfaces-svm-example&#34;&gt;4.3.2 Eigenfaces-SVM Example&lt;/h4&gt;
&lt;p&gt;We provide two example benchmarks for the Eigenfaces SVM example. The first deploys and measures the AI service on a single cloud provider at a time, and the second deploys a multi-cloud AI service, and then measures the service across the clouds in parallel.&lt;/p&gt;
&lt;h4 id=&#34;4321-single-cloud-provider-service-benchmarking&#34;&gt;4.3.2.1 Single Cloud Provider Service Benchmarking&lt;/h4&gt;
&lt;p&gt;The benchmark script for the Eigenfaces SVM example uses Cloudmesh to create virtual machines and setup a Cloudmesh OpenAPI environment sequentially across the three measured clouds including Amazon, Azure, and Google. After the script sets up the environment, it runs a series of pytests that generate and launch the Eigenfaces-SVM OpenAPI service, and then conduct runtime measurements of various service functions.&lt;/p&gt;
&lt;p&gt;The benchmark runs the pytest in two configurations. After the benchmark script sets up a virtual machine environment, it runs the first pytest locally on the OpenAPI server and measures five runtimes:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Download and extraction of remote image data from ndownloader.figshare.com/files/5976015&lt;/li&gt;
&lt;li&gt;The model training time when run as an OpenAPI service&lt;/li&gt;
&lt;li&gt;The model training time when run as the Scikit-learn example without OpenAPI involvement&lt;/li&gt;
&lt;li&gt;The time to upload an image from the server to itself&lt;/li&gt;
&lt;li&gt;The time to predict and return the target label of the uploaded image&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The benchmark runs the second pytest iteration from the remote client it is running on and interacts with the deployed OpenAPI service over the internet. It tests two runtimes:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The time to upload an image to the remote OpenAPI server&lt;/li&gt;
&lt;li&gt;The time to run the predict function on the remote OpenAPI server, and return the target label  of the uploaded image&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In Figure 1 we compare the download and extraction time of the labeled faces in the wild dataset. This data set is approximately 233 MBs compressed, which allows us to measure a non-trivial data transfer. Lower transfer times imply the cloud has higher throughput from the data server, less latency to the data server, or that it provides access to a higher performing internal network. The standard deviation is displayed to compare the variation in the download times.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cloudmesh/cloudmesh-openapi/raw/main/images/sample_graph_1.png&#34; alt=&#34;Download Data Runtime&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 3:&lt;/strong&gt; Donwnload (233MB) and extraction (~275MB) of remote image data from ndownloader.figshare.com/files/5976015.&lt;/p&gt;
&lt;p&gt;In Figure 2 we measure the training time of the Eigenfaces-SVM model both as an OpenAPI service and as the basic Scikit-learn example. This allows us to measure runtime overhead added by OpenAPI compared to the source example. Here the two functions are identical except that the OpenAPI train function makes an additional function call to store the model to disk using joblib. This is necessary to share the model across the train and predict functions. The standard deviation is displayed to compare the variation in the training times.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cloudmesh/cloudmesh-openapi/raw/main/images/sample_graph_2.png&#34; alt=&#34;Train Runtime&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 4:&lt;/strong&gt; Compares the eigenfaces-svm model training time running both as an OpenAPI service, and as the raw Scikit-learn example. There are two bars per cloud provider. The bold bars are the training time of the model when hosted as a Cloudmesh OpenAPI function. The pastel bars are the training time of the Scikit-learn example code without Cloudmesh OpenAPI involvement. The bars plot mean runtimes and the error bar reflects the standard deviation of the runtimes.&lt;/p&gt;
&lt;p&gt;In Figure 3 we measure the time to upload an image to the server both from itself, and from a remote client. This allows us to compare the function runtime as experienced by the server, and as experienced by a remote client. The difference helps determine the network latency between the benchmark client and the cloud service. The standard deviation is displayed to compare the variation in the upload times.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cloudmesh/cloudmesh-openapi/raw/main/images/sample_graph_3.png&#34; alt=&#34;Upload Runtime&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 5:&lt;/strong&gt; Runtime of the upload function when run locally from the OpenAPI server and from a remote client. There are two bars per cloud provider. The bold bars are the runtime of the upload function as experienced by the server, and the pastel as experienced by the remote client. The bars plot mean runtimes and the error bar reflects the standard deviation of the runtimes.&lt;/p&gt;
&lt;p&gt;In Figure 4 we measure the time to call the predict function on the uploaded image. Again we run this once from the local server itself, and a second time from a remote client to determine as experienced runtimes. The standard deviation is displayed to compare the variation in the predict times.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cloudmesh/cloudmesh-openapi/raw/main/images/sample_graph_4.png&#34; alt=&#34;Predict Runtime&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 6:&lt;/strong&gt; Runtime of the predict function when run locally from the OpenAPI server and from a remote client. There are two bars per cloud provider. The bold bars are the runtime of the predict function as experienced by the server, and the pastel as experienced by the remote client. The bars plot mean runtimes and the error bar reflects the standard deviation of the runtimes.&lt;/p&gt;
&lt;p&gt;Table 2 presents a full listing of test results.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Table 2:&lt;/strong&gt; Test results for the Eigenfaces SVM benchmark. Type &amp;ldquo;local: denotes the test was run locally on the Cloudmesh Openapi server, and &amp;ldquo;remote&amp;rdquo; denotes a remote host interacted with the server using the python requets library.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;test&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;type&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;cloud&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mean (s)&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;min (s)&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;max (s)&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;std (s)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;test_download_data&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;local&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;aws&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;20.583&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17.233&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;31.798&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.76933&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;test_download_data&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;local&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;azure&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;20.8087&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13.564&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;42.701&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.9407&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;test_download_data&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;local&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;google&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;18.0035&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17.062&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;19.381&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.478574&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;test_predict&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;local&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;aws&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0299111&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.015&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.051&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00401288&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;test_predict&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;local&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;azure&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0245333&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.013&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.029&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00306848&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;test_predict&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;local&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;google&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0285889&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.014&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.058&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00420554&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;test_predict&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;remote&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;aws&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.401722&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.259&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.804&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.175369&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;test_predict&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;remote&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;azure&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.358733&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.244&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.134117&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;test_predict&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;remote&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;google&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.358189&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.27&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.815&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.158345&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;test_scikitlearn_train&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;local&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;aws&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;35.8861&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;35.113&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;46.451&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.7666&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;test_scikitlearn_train&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;local&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;azure&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;40.1319&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;34.946&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;43.961&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.28506&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;test_scikitlearn_train&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;local&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;google&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;42.1328&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;41.771&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;42.493&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.12829&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;test_train&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;local&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;aws&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;35.725&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;34.908&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;46.505&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.72603&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;test_train&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;local&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;azure&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;40.2847&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;35.302&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;47.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.31544&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;test_train&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;local&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;google&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;42.0448&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;41.516&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;45.932&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.709089&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;test_upload&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;local&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;aws&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00673333&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.006&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.012&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000866667&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;test_upload&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;local&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;azure&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00575&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.005&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.007&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000469929&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;test_upload&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;local&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;google&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00688889&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.006&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.009&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000406733&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;test_upload&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;remote&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;aws&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.428256&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.163&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.134&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.205095&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;test_upload&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;remote&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;azure&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.322283&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.153&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.498&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.151721&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;test_upload&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;remote&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;google&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.310822&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.184&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.729&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.180025&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;4322-multi-cloud-service-benchmarking&#34;&gt;4.3.2.2 Multi-Cloud Service Benchmarking&lt;/h4&gt;
&lt;p&gt;In this benchmark our script first acquires VMs, install Cloudmesh OpenAPI, and launch the Eigenfaces SVM AI service on three separate cloud providers. Because Cloudmesh has limited parallel computing support, the script deploys the VMs in a serial manner. After the services are running, we then run our tests in a parallel manner as depicted in Figure 2. Testing in parallel provides faster benchmark results, and better equalizes benchmark testing conditions. The benchmark conducts requests to each cloud in parallel, so they should experience similar network conditions. For example, in a serial testing model, the remote data server may experience varying loads resulting in different load times. Our parallel tests better equalize these conditions by having each cloud download the data at the same time.&lt;/p&gt;
&lt;p&gt;In Figure 7 we depict the combined runtime of our benchmark tests. This allows us to compare the complete execution time of an AI service workflow.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cloudmesh/cloudmesh-openapi/raw/main/images/ai_service_workflow_runtime.png&#34; alt=&#34;AI Service Workflow Runtime&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 7:&lt;/strong&gt; Mean runtime of the Eigenfaces SVM workflow deployed as a multi-cloud service. We compute the means from 30 runs of a workflow that included 1 download data invocation, 1 train invocation, 30 upload invocations, and 30 predict invocations. We run the workflows in parallel on the separate clouds using a multiprocessing on an 8-core machine.&lt;/p&gt;
&lt;p&gt;In Table 3 we provide complete test results.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Table 3:&lt;/strong&gt; Test results for the Eigenfaces SVM benchmark deployed as a mutli-cloud service.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;test&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;type&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;cloud&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mean (s)&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;min (s)&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;max (s)&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;std (s)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;test_download_data&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;remote&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;aws&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;20.5102&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17.566&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;34.42&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.82372&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;test_download_data&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;remote&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;azure&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;18.604&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13.489&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;32.645&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.53201&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;test_download_data&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;remote&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;google&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17.8976&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17.133&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;21.861&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.853191&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;test_predict&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;remote&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;aws&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.148&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.59&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.417&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.572103&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;test_predict&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;remote&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;azure&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.9337&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.398&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.654&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.737981&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;test_predict&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;remote&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;google&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.13497&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.744&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.366&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.602431&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;test_train&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;remote&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;aws&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;35.6067&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;35.245&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;39.531&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.734663&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;test_train&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;remote&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;azure&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;35.8856&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;35.077&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;39.998&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.94563&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;test_train&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;remote&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;google&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;41.981&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;41.578&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;45.706&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.70646&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;test_upload&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;remote&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;aws&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10.0755&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.895&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16.524&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.37981&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;test_upload&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;remote&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;azure&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8.45627&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.72&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13.915&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.05165&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;test_upload&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;remote&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;google&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8.8688&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.39&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15.443&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.51598&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;433-pipelined-anova-svm-example&#34;&gt;4.3.3 Pipelined Anova SVM Example&lt;/h4&gt;
&lt;h4 id=&#34;434-caleb-example&#34;&gt;4.3.4 Caleb Example&lt;/h4&gt;
&lt;h2 id=&#34;5-conclusion&#34;&gt;5. Conclusion&lt;/h2&gt;
&lt;h2 id=&#34;6-limitations&#34;&gt;6. Limitations&lt;/h2&gt;
&lt;p&gt;Azure has updated their libraries and discontinued the version 4.0
Azure libraries. We updated Cloudmesh to use the new library, but not all features, such as virtual machine delete, are implemented or verified.&lt;/p&gt;
&lt;h2 id=&#34;appendix-a---setup&#34;&gt;APPENDIX A. - Setup&lt;/h2&gt;
&lt;h3 id=&#34;a1-deployment&#34;&gt;A.1. Deployment&lt;/h3&gt;
&lt;p&gt;The project is easy to replicate with our detailed instructions. First you must install Cloudmesh OpenAPI whihch can be done by the follwoing steps:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python -m venv ~/ENV3
source ~/ENV3/bin/activate 
mkdir cm
cd cm
pip install cloudmesh-installer
cloudmesh-installer get openapi 
cms help
cms gui quick
# fill out mongo variables
# make sure autinstall is True
cms config set cloudmesh.data.mongo.MONGO_AUTOINSTALL=True
cms admin mongo install --force
# Restart a new terminal to make sure mongod is in your path
cms init
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;As a first example we like to test if the deployment works by using a
number of simple commands we execute in a terminal.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd ~/cm/cloudmesh-openapi

cms openapi generate get_processor_name \
    --filename=./tests/server-cpu/cpu.py

cms openapi server start ./tests/server-cpu/cpu.yaml

curl -X GET &amp;quot;http://localhost:8080/cloudmesh/get_processor_name&amp;quot; \
     -H &amp;quot;accept: text/plain&amp;quot;
cms openapi server list

cms openapi server stop cpu
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The output will be a string containing your computer.&lt;/p&gt;
&lt;p&gt;TODO: how does the string look like&lt;/p&gt;
&lt;p&gt;Next you can test a more sophiticated example. Here we generate from a
python function a rest servive. We consider the following function
definition in which a float is returned as a simple integer&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;def add(x: float, y: float) -&amp;gt; float:
    &amp;quot;&amp;quot;&amp;quot;
    adding float and float.
    :param x: x value
    :type x: float
    :param y: y value
    :type y: float
    :return: result
    :return type: float
    &amp;quot;&amp;quot;&amp;quot;
    result = x + y

    return result
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once we execute the following lines in a terminal, the result of the
addition will be calculated in the REST service and it is returned as
a string.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi generate add --filename=./tests/add-float/add.py
cms openapi server start ./tests/add-float/add.yaml 
curl -X GET &amp;quot;http://localhost:8080/cloudmesh/add?x=1&amp;amp;y=2&amp;quot; -H  &amp;quot;accept: text/plain&amp;quot;
# This command returns
&amp;gt; 3.0
cms openapi server stop add
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;As we often also need the information as a REST service, we provide in
our next example a jsonified object specification.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from flask import jsonify

def add(x: float, y: float) -&amp;gt; str:
    &amp;quot;&amp;quot;&amp;quot;
    adding float and float.
    :param x: x value
    :type x: float
    :param y: y value
    :type y: float
    :return: result
    :return type: float
    &amp;quot;&amp;quot;&amp;quot;
    result = {&amp;quot;result&amp;quot;: x + y}

    return jsonify(result)

&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The result will include a json string returned by the service.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi generate add --filename=./tests/add-json/add.py
cms openapi server start ./tests/add-json/add.yaml 
curl -X GET &amp;quot;http://localhost:8080/cloudmesh/add?x=1&amp;amp;y=2&amp;quot; -H  &amp;quot;accept: text/plain&amp;quot;
# This command returns
&amp;gt; {&amp;quot;result&amp;quot;:3.0}
cms openapi server stop add
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;These examples are used to demonstrate the ease of use as well as the
functionality for those that want to replicate our work.&lt;/p&gt;
&lt;h3 id=&#34;a2--pipiline-anova-svm&#34;&gt;A.2.  Pipiline ANOVA SVM&lt;/h3&gt;
&lt;p&gt;This example demonstrates how to deploy a simple machine learning example onto a server using cloudmesh-openapi. The specific implementation details that this example is based on can be found &lt;a href=&#34;https://scikit-learn.org/stable/auto_examples/feature_selection/plot_feature_selection_pipeline.html&#34;&gt;here.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The model being implemented is, in essence, an SVM with extra features to improve the model.  An SVM (support vector machine) is a supervised learning model with associated learning algorithms used for classification and regression analysis. This model has become one of the most robust prediction methods widely used in problems conerning classification and the like.&lt;/p&gt;
&lt;p&gt;The Pipeline and ANOVA aspects are extensions to the SVM to improve the overall model. The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. ANOVA on the other hand is an acronym for Analysis of Variance. It is an omnibus test, meaning it tests for a difference overall between all groups. In the context of an SVM, this information is useful as an SVM mainly classifies data into separate groups.&lt;/p&gt;
&lt;p&gt;We can now proceed as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ pwd
~/cm/cloudmesh-openapi

$ cms openapi generate PipelineAnovaSVM \
      --filename=./tests/Scikitlearn-experimental/sklearn_svm.py \
      --import_class --enable_upload

$ cms openapi server start ./tests/Scikitlearn-experimental/sklearn_svm.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;After running these commands, we opened a web user interface. In the
user interface, we uploaded the file iris data located in
~/cm/cloudmesh-openapi/tests/ Scikitlearn-experimental/iris.data&lt;/p&gt;
&lt;p&gt;We then trained the model on this data set by inserting the name of
the file we uploaded &lt;code&gt;iris.data&lt;/code&gt;. Next, we tested the model by
clicking on make_prediction and giving it the name of the file
iris.data and the parameters &lt;code&gt;5.1&lt;/code&gt;, &lt;code&gt;3.5&lt;/code&gt;, &lt;code&gt;1.4&lt;/code&gt;, &lt;code&gt;0.2&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The response we received was &lt;code&gt;Classification: [&#39;Iris-setosa&#39;]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Lastly, we close the server:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ cms openapi server stop sklearn_svm
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This process can easily be replicated when we create more service
examples that we derive from existing sklearn examples. We benchmark
these tests while wrapping them into pytests and run them on various
cloud services.&lt;/p&gt;
&lt;h3 id=&#34;a3--eigenfaces-svm-facial-recognition&#34;&gt;A.3.  Eigenfaces SVM Facial Recognition&lt;/h3&gt;
&lt;p&gt;Next we demonstrate how to run the Eigenfaces SVM example locally, and
then how to run its associated benchmark script.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ pwd
~/cm/cloudmesh-openapi

$ git checkout benchmark # todo required until merged into main

$ cms openapi generate EigenfacesSVM \
      --filename=./tests/generator-eigenfaces-svm/eigenfaces-svm-full.py \
      --import_class --enable_upload

$ cms openapi server start ./tests/generator-eigenfaces-svm/eigenfaces-svm-full.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;After running these commands, we opened a web user interface at &lt;a href=&#34;http://localhost:8080/cloudmesh/ui&#34;&gt;http://localhost:8080/cloudmesh/ui&lt;/a&gt;. In the user interface we run the download_data function with the default arguments. This downloads and extracts the labeled faces in the wild data set to the ~/scikit_learn_data/lfw_home directory.&lt;/p&gt;
&lt;p&gt;Next, we run the train function to train the model. The train function performs a 50/50 train/test split on the input data, and returns performance statistics of the trained model.&lt;/p&gt;
&lt;p&gt;Next, we use the upload function to upload an example image using &lt;code&gt;~./tests/generator-eigenfaces-svm/example_image.jpg&lt;/code&gt; as the function argument. This puts the example image in the ~/.cloudmesh/upload-file/ directory.&lt;/p&gt;
&lt;p&gt;Finally, we run the predict function with the uploaded file path as an argument, &lt;code&gt;~/.cloudmesh/upload-file/example_image.jpg&lt;/code&gt;, and receive  the classification as a response &lt;code&gt;[&#39;George W. Bush&#39;]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Last, we close the server:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ cms openapi server stop EigenfacesSVM
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Next, we benchmark these tests while wrapping them into pytests and run them on various cloud services.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Before continuing you must have successfully registered AWS, Azure, and Google clouds in your yaml file and be able to boot virtual machines on Google, AWS, and Azure. This example currently should work on Linux and macOS&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First, we must change to a git branch that includes Azure provider fixes, and setup our ~./cloudmesh/cloudmesh.yaml file to replicate the parameters set for the benchmark results above.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ cd ~/cm/cloudmesh-azure 
$ git checkout benchmark # required until changes merged to main

$ cd ~/cm/cloudmesh-openapi

$ cp ~/.cloudmesh/cloudmesh.yaml ~/.cloudmesh/cloudmesh.bak.1 # to revert reverse the cp

$ cms config set cloudmesh.cloud.azure.default.image=&amp;quot;Canonical:0001-com-ubuntu-server-focal:20_04-lts:20.04.202006100&amp;quot;
$ cms config set cloudmesh.cloud.azure.default.size=&amp;quot;Standard_D2s_v3&amp;quot;
$ cms config set cloudmesh.cloud.azure.credentials.AZURE_REGION=&amp;quot;eastus&amp;quot;

$ cms config set cloudmesh.cloud.aws.default.image=&amp;quot;ami-0dba2cb6798deb6d8&amp;quot;
$ cms config set cloudmesh.cloud.aws.default.size=&amp;quot;m4.large&amp;quot;
$ cms config set cloudmesh.cloud.aws.default.username=&amp;quot;ubuntu&amp;quot;
$ cms config set cloudmesh.cloud.aws.credentials.region=&amp;quot;us-east-1&amp;quot;

$ cms config set cloudmesh.cloud.google.default.image=&amp;quot;ubuntu-2004-lts&amp;quot;
$ cms config set cloudmesh.cloud.google.default.image_project=&amp;quot;ubuntu-os-cloud&amp;quot;
$ cms config set cloudmesh.cloud.google.default.zone=&amp;quot;us-east1-b&amp;quot;
$ cms config set cloudmesh.cloud.google.default.region=&amp;quot;us-east1&amp;quot;
$ cms config set cloudmesh.cloud.google.default.flavor=&amp;quot;n1-standard-2&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Next, we will modify the default security group to open the flask server port 8080 for OpenAPI service testing.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ cms sec rule add openapi 8080 8080 tcp 0.0.0.0/0
$ cms sec group add default openapi for_openapi_demo
# the above two command should allow aws and azure to work
# sec group load is broken for google and it does not use the default sec group, so you have to manually add the openapi rule to google cloud for now
# console.cloud.google.com &amp;gt; VPC network &amp;gt; firewall &amp;gt; create firewall rule
# name: openapi, targets:  all instances in network, Source IP ranges: 0.0.0.0 /0, specified protocols and ports: tcp 8080 &amp;gt; create
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Next, we will run the benchmarking script, ~./tests/generator-eigenfaces-svm/benchmark-eigenfaces.py. This script utilizes the Cloudmesh shell and the Bash script, ~/.tests/generator-eigenfaces-svm/eigenfaces-svm-full-script, to sequentially deploy a VM on each of the clouds, install Cloudmesh-openapi and the example dependencies, and then us the pytest, ./tests/test_030_generator_eigenfaces_svm.py, twice to benchmark the EigenfacesSVM service functions both locally from the server, and from the remote client running the benchmark script. Finally, it prints and plots performance statistics.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ ./tests/generator/eigenfaces-svm/benchmark-eigenfaces.py run
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If the command line argument &lt;code&gt;run&lt;/code&gt; is passed to the script, then it will start up the virtual machines on each cloud. Output and benchmark results from each of the virtual machines will be store in the ~/.cloudmesh/eigenfaces-svm/vm_script_output/ directory. The benchmark results are scraped from the script outputs and stored in the ~/.cloudmesh/eigenfaces-svm/benchmark_output directory. If the &lt;code&gt;run&lt;/code&gt; argument is &lt;strong&gt;not&lt;/strong&gt; provided, it will only print statistics from script output already stored in the vm_script_output directory.&lt;/p&gt;
&lt;p&gt;Statistics will be printed to the command line, and graphs will be displayed using plt.show() function calls as well as saved to the ~./tests/generator-eigenfaces-svm/ directory.&lt;/p&gt;
&lt;p&gt;Next, we will run the multi-cloud benchmarking script, ~/.tests/generator-eigenfaces-svm/bencmark-eigenfaces-multi-cloud.py. This script uses the Cloudmesh shell and the Bash script, ~/.tests/generator-eigenfaces-svm/eigenfaces-svm-full-multi-script, to sequentially deploy a VM on each of the clouds, install Cloudmesh-OpenAPI and the example dependencies, and start the AI service. Next, it conducts HTTP requests in parallel to interact with the services to measure the runtime for data download, training, uploading, and prediction.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ ./tests/generator/eigenfaces-svm/benchmark-eigenfaces-multi-cloud.py run
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;As above the command line argument run is used to conduct actual tests, and the absence of that argument simply computes statistics on existing output from the ~/.cloudmesh/eigenfaces-svm/vm_script_output_multi/ directory.&lt;/p&gt;
&lt;p&gt;Statistics will be printed to the command line, and graphs will be displayed using plt.show() function calls as well as saved to the ~./tests/generator-eigenfaces-svm/ directory.&lt;/p&gt;
&lt;h3 id=&#34;a4-using-unit-tests-for-benchmarking&#34;&gt;A.4. Using unit tests for Benchmarking&lt;/h3&gt;
&lt;p&gt;TODO: This section will be expanded upon&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Describe why we can unit tests&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Describe how we access multiple clouds&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms set cloud=aws
# run test
cms set cloud=azure
# run test
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Describe the Benchmark class from cloudmesh in one sentence and how
we use it&lt;/p&gt;
&lt;h3 id=&#34;a5-basic-auth-example&#34;&gt;A.5. Basic Auth Example&lt;/h3&gt;
&lt;p&gt;Basic Auth in cloudmesh openapi can be enabled with the following flag&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--basic_auth=&amp;lt;username&amp;gt;:&amp;lt;password&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;flag. As such, this example will be an extension of a previously existing example. To follow with this example, navigate to the &lt;code&gt;cloudmesh-openapi&lt;/code&gt; directory.&lt;/p&gt;
&lt;p&gt;We will use the &lt;code&gt;server-cpu&lt;/code&gt; example which tells the user the CPU of the machine running the API.&lt;/p&gt;
&lt;p&gt;For this example, let&amp;rsquo;s create a user with username &lt;code&gt;admin&lt;/code&gt; and password &lt;code&gt;secret&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi generate get_processor_name \
  --filename=./tests/server-cpu/cpu.py \
  --basic_auth=admin:secret
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can start the server as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi server start ./tests/server-cpu/cpu.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The user will now be required to authenticate as the registered user in order to access the API. This can be done by specifying the Basic Auth credentials in the header as done &lt;a href=&#34;https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Authorization&#34;&gt;here&lt;/a&gt;. Alternatively, the user can login via the &lt;a href=&#34;http://localhost:8080/cloudmesh/u&#34;&gt;swagger UI&lt;/a&gt; when the server is started.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;a6-switching-between-pickledb-and-mongodb&#34;&gt;A.6 Switching between PickleDB and MongoDB&lt;/h3&gt;
&lt;p&gt;The default &amp;ldquo;out-of-the-box&amp;rdquo; storage mechanism of cloudmesh-openapi is Pickle. This requires no setup of the DB on the user&amp;rsquo;s end.&lt;/p&gt;
&lt;p&gt;To switch to MongoDB, the user must first change their config option as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi register protocol mongo
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Note that by switching to mongo, certain mongo variables need to be filled out. Mongo may need to be installed as well. Refer to &lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/#installation&#34;&gt;this&lt;/a&gt; documentation to see how this process can be done.&lt;/p&gt;
&lt;p&gt;One may switch back to pickle with the same command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi register protocol pickle
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;appendix-b---code-location&#34;&gt;APPENDIX B. - Code Location&lt;/h2&gt;
&lt;p&gt;This is temporary and will in final be moved elsewhere. Its
conveniently for now placed on top so we can easier locate it&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;github: &lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi&#34;&gt;https://github.com/cloudmesh/cloudmesh-openapi&lt;/a&gt; &lt;sup id=&#34;fnref:23&#34;&gt;&lt;a href=&#34;#fn:23&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;23&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;branch: benchmark (not yet created)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;appendix-c---cloudmesh-links&#34;&gt;APPENDIX C. - Cloudmesh Links&lt;/h2&gt;
&lt;p&gt;We added this section so the Reader can easily find some cloudmesh
related information
Documentation for Cloudmesh can be found at:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cloudmesh.github.io/cloudmesh-manual/&#34;&gt;https://cloudmesh.github.io/cloudmesh-manual/&lt;/a&gt; &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Code for cloud mesh can be found at:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/cloudmesh/&#34;&gt;https://github.com/cloudmesh/&lt;/a&gt; &lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Examples in this paper came from the cloudmesh openapi manual which is located here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi&#34;&gt;https://github.com/cloudmesh/cloudmesh-openapi&lt;/a&gt; &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Information about cloudmesh can be found here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cloudmesh.github.io/cloudmesh-manual/preface/about.html&#34;&gt;https://cloudmesh.github.io/cloudmesh-manual/preface/about.html&lt;/a&gt; &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Various cloudmesh installations for various needs can be found here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cloudmesh.github.io/cloudmesh-manual/installation/install.html&#34;&gt;https://cloudmesh.github.io/cloudmesh-manual/installation/install.html&lt;/a&gt; &lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;appendix-d---plan&#34;&gt;APPENDIX D. - Plan&lt;/h2&gt;
&lt;p&gt;Thus far in the project we have familiarized ourselves with
Cloudmesh-Openapi by recreating example services on our local
machines, setup a git branch of the source project on which we will
collaborate, contributed to the paper’s background section, and
started looking for example AI analytics, like those provided at
SciKitLearn’s website. We obtained cloud service accounts from AWS,
Azure, GCP, and Chameleon Cloud, and verified Cloudmesh documentation
while applying for the cloud accounts. We registered our accounts with
the Cloudmesh shell and executed VM operations using Cloudmesh.&lt;/p&gt;
&lt;p&gt;Moving forward, we will develop benchmark tests in the
form of pytests that replicate the AI analytic examples.  We will each
use Cloudmesh to deploy these tests as an Openapi based REST service
and benchmark their performance on various cloud providers. Our
benchmarks will measure various components such as data transfer time,
model train time, model prediction time, etc. We will then consolidate
and report on our findings. Our final project will include a script
that utilizes the Cloudmesh shell to automate our benchmark tests so
others can replicate our work.&lt;/p&gt;
&lt;p&gt;For an AI analytic benchmark test, one intesresting example to
replicate may be the faces recognition example using eigenfaces and
SVMs
&lt;a href=&#34;https://scikit-learn.org/stable/auto_examples/applications/plot_face_recognition.html#sphx-glr-auto-examples-applications-plot-face-recognition-py&#34;&gt;https://scikit-learn.org/stable/auto_examples/applications/plot_face_recognition.html#sphx-glr-auto-examples-applications-plot-face-recognition-py&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Last week we created the first draft of the eigenfaces-svm example
that is found in the &amp;ldquo;benchmark&amp;rdquo; branch.  It outputs the example and
prints benchmark information. We are making progress on manually
running this example on a cloud VM using the Cloudmesh shell, which
will generate the requirements for our final script.&lt;/p&gt;
&lt;p&gt;This week we successfully ran the eigenfaces-svm example on Goolge
Cloud, Amazon Web Services, and Microsoft Azure. We created a script
eigenfaces-svm-script that can deploy the OpenAPI service on a fresh
VM on a cloud and run the eigenfaces-svm example. We also created the
eigenfaces-svm-full example which breaks the workflow into a functions
that download remote data, train and tests the model, provide a image
upload function, and a prediction function. We also created a pytest
that automatically run those four functions and print benchark
information.&lt;/p&gt;
&lt;p&gt;Next week we will create a script to run the eigenfaces-svm-full
example on each cloud multiple times, and them summarize and plot
benchmark information to compare the clouds. Additionally, we will
finish the report.&lt;/p&gt;
&lt;h2 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;We like to thank &lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-325/&#34;&gt;Vishwanadham Mandala&lt;/a&gt; to participate in helping write an earlier version of this document.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Cloudmesh Manual, &lt;a href=&#34;https://cloudmesh.github.io/cloudmesh-manual/&#34;&gt;https://cloudmesh.github.io/cloudmesh-manual/&lt;/a&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Cloudmesh Manaual Preface for cloudmesh,  &lt;a href=&#34;https://cloudmesh.github.io/cloudmesh-manual/preface/about.html&#34;&gt;https://cloudmesh.github.io/cloudmesh-manual/preface/about.html&lt;/a&gt; &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Cloudmesh OpenAPI Repository for automatically generated REST services from Python functions &lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi&#34;&gt;https://github.com/cloudmesh/cloudmesh-openapi&lt;/a&gt;. &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Cloudmesh Repositories, &lt;a href=&#34;https://github.com/cloudmesh/&#34;&gt;https://github.com/cloudmesh/&lt;/a&gt; &lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Cloudmesh Manual, Instalation instryctions for cloudmesh &lt;a href=&#34;https://cloudmesh.github.io/cloudmesh-manual/installation/install.html&#34;&gt;https://cloudmesh.github.io/cloudmesh-manual/installation/install.html&lt;/a&gt; &lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34; role=&#34;doc-endnote&#34;&gt;
 &lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:7&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;OpenAPI Initiative, &amp;ldquo;The openapi specification&amp;rdquo;, Web Page [Online]. Available: &amp;lt;https://github.com/OAI/OpenAPI- Specification/blob/main/versions/2.0.md&amp;gt; &lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:8&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;OpenAPI Initiative, &amp;ldquo;The openapi specification.&amp;rdquo; Web Page [Online]. Available: &lt;a href=&#34;https://github.com/OAI/OpenAPI-Specification&#34;&gt;https://github.com/OAI/OpenAPI-Specification&lt;/a&gt; &lt;a href=&#34;#fnref:8&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:9&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;RAML, &amp;ldquo;RAML version 1.0: RESTful api modeling language&amp;rdquo;, Web Page [Online]. Available: &lt;a href=&#34;https://github.com/raml-org/raml-spec/blob/main/versions/raml-10/raml-10.md&#34;&gt;https://github.com/raml-org/raml-spec/blob/main/versions/raml-10/raml-10.md&lt;/a&gt; &lt;a href=&#34;#fnref:9&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:10&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;R. H. Kevin Burke Kyle Conroy, &amp;ldquo;Flask-restful&amp;rdquo;, Web Page [Online]. Available: &lt;a href=&#34;https://flask-restful.readthedocs.io/en/latest/&#34;&gt;https://flask-restful.readthedocs.io/en/latest/&lt;/a&gt; &lt;a href=&#34;#fnref:10&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:11&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;E. O. Ltd, &amp;ldquo;Django rest framework&amp;rdquo;, Web Page [Online]. Available:
&lt;a href=&#34;https://www.django-rest-framework.org/&#34;&gt;https://www.django-rest-framework.org/&lt;/a&gt; &lt;a href=&#34;#fnref:11&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:12&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;S. Software, &amp;ldquo;API development for everyone&amp;rdquo;, Web Page [Online]. Available: &lt;a href=&#34;https://swagger.io&#34;&gt;https://swagger.io&lt;/a&gt; &lt;a href=&#34;#fnref:12&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:13&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;A. Y. W. Hate, &amp;ldquo;OpenAPI.Tools&amp;rdquo;, Web Page [Online]. Available:
&lt;a href=&#34;https://openapi.tools/&#34;&gt;https://openapi.tools/&lt;/a&gt; &lt;a href=&#34;#fnref:13&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:14&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;S. Software, &amp;ldquo;Swagger codegen documentation&amp;rdquo;, Web Page [Online]. Available: &lt;a href=&#34;https://swagger.io/docs/open-source-tools/swagger-codegen/&#34;&gt;https://swagger.io/docs/open-source-tools/swagger-codegen/&lt;/a&gt; &lt;a href=&#34;#fnref:14&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:15&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&amp;ldquo;NIST SP 800-145&amp;rdquo;, Web page [Online]. Available: &lt;a href=&#34;https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-145.pdf&#34;&gt;https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-145.pdf&lt;/a&gt; &lt;a href=&#34;#fnref:15&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:16&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&amp;ldquo;TensorFlow Enterprise&amp;rdquo;, Web page [Online]. Available: &lt;a href=&#34;https://cloud.google.com/tensorflow-enterprise&#34;&gt;https://cloud.google.com/tensorflow-enterprise&lt;/a&gt; &lt;a href=&#34;#fnref:16&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:17&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&amp;ldquo;Amazon Polly. Turn text into lifelike speech using deep learning.&amp;rdquo; Web page [Online]. Available: &lt;a href=&#34;https://aws.amazon.com/polly/?c=ml&amp;amp;sec=srv&#34;&gt;https://aws.amazon.com/polly/?c=ml&amp;amp;sec=srv&lt;/a&gt; &lt;a href=&#34;#fnref:17&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:18&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Scikit Learn, Pipeline Anova SVM, &lt;a href=&#34;https://scikit-learn.org/stable/auto_examples/feature_selection/plot_feature_selection_pipeline.html&#34;&gt;https://scikit-learn.org/stable/auto_examples/feature_selection/plot_feature_selection_pipeline.html&lt;/a&gt; &lt;a href=&#34;#fnref:18&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:19&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&amp;ldquo;Faces recognition example using eigenfaces and SVMs&amp;rdquo;, Web Page [Online]. Available: &lt;a href=&#34;https://scikit-learn.org/stable/auto_examples/applications/plot_face_recognition.html#sphx-glr-auto-examples-applications-plot-face-recognition-py&#34;&gt;https://scikit-learn.org/stable/auto_examples/applications/plot_face_recognition.html#sphx-glr-auto-examples-applications-plot-face-recognition-py&lt;/a&gt; &lt;a href=&#34;#fnref:19&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:20&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Huang, Gary &amp;amp; Jain, Vidit &amp;amp; Learned-Miller, Erik. (2007). Unsupervised Joint Alignment of Complex Images. ICCV. 1-8. 10.1109/ICCV.2007.4408858. Available: &lt;a href=&#34;http://vis-www.cs.umass.edu/papers/iccv07alignment.pdf&#34;&gt;http://vis-www.cs.umass.edu/papers/iccv07alignment.pdf&lt;/a&gt; &lt;a href=&#34;#fnref:20&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:21&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&amp;ldquo;Amazon EC2 Instance Types&amp;rdquo;, Web page [Online]. Available: &lt;a href=&#34;https://aws.amazon.com/ec2/instance-types/&#34;&gt;https://aws.amazon.com/ec2/instance-types/&lt;/a&gt; &lt;a href=&#34;#fnref:21&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:22&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&amp;ldquo;Machine Types&amp;rdquo;, Web page [Online]. Available: &lt;a href=&#34;https://cloud.google.com/compute/docs/machine-types&#34;&gt;https://cloud.google.com/compute/docs/machine-types&lt;/a&gt; &lt;a href=&#34;#fnref:22&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:23&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Cloudmesh Openapi Web page &lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi&#34;&gt;https://github.com/cloudmesh/cloudmesh-openapi&lt;/a&gt; &lt;a href=&#34;#fnref:23&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/cloudmesh-openapi/project_review/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/cloudmesh-openapi/project_review/</guid>
      <description>
        
        
        &lt;h1 id=&#34;project-review&#34;&gt;Project Review&lt;/h1&gt;
&lt;h2 id=&#34;team-members&#34;&gt;Team members:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Jonathan Beckford&lt;/li&gt;
&lt;li&gt;Brian Kegerreis&lt;/li&gt;
&lt;li&gt;Prateek Shaw&lt;/li&gt;
&lt;li&gt;Jagadeesh Kandimalla&lt;/li&gt;
&lt;li&gt;Ishan Mishra&lt;/li&gt;
&lt;li&gt;Andrew G&lt;/li&gt;
&lt;li&gt;Falconi&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;project-documentation&#34;&gt;Project Documentation:&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://cloudmesh.github.io/cloudmesh-openapi/index.html&#34;&gt;https://cloudmesh.github.io/cloudmesh-openapi/index.html&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;contributors-based-on-git-tracking&#34;&gt;Contributors based on Git tracking&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;NOTE:&lt;/strong&gt;&lt;/em&gt; This is not completely accurate because some did not have git config done correctly.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/graphs/contributors&#34;&gt;https://github.com/cloudmesh/cloudmesh-openapi/graphs/contributors&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-breakdown&#34;&gt;Code Breakdown&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;cms command:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/blob/main/cloudmesh/openapi/command/openapi.py&#34;&gt;https://github.com/cloudmesh/cloudmesh-openapi/blob/main/cloudmesh/openapi/command/openapi.py&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contributors:&lt;/strong&gt; all team&lt;/p&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;cms generate - to generate server yaml&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;executor&lt;/strong&gt; that parses parameters and calls generator:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/blob/main/cloudmesh/openapi/function/executor.py&#34;&gt;https://github.com/cloudmesh/cloudmesh-openapi/blob/main/cloudmesh/openapi/function/executor.py&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contributors:&lt;/strong&gt;  Brian, Professor&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;generator&lt;/strong&gt; that generates the server yaml:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/blob/main/cloudmesh/openapi/function/generator.py&#34;&gt;https://github.com/cloudmesh/cloudmesh-openapi/blob/main/cloudmesh/openapi/function/generator.py&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contributors:&lt;/strong&gt;  Brian, Jonathan, Prateek&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;cms server - to start and stop server&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/blob/main/cloudmesh/openapi/function/server.py&#34;&gt;https://github.com/cloudmesh/cloudmesh-openapi/blob/main/cloudmesh/openapi/function/server.py&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Contributors:&lt;/strong&gt;  Jonathan, Andrew, Prateek, Ishan&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;cms registry - register the server and cache model&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;registry&lt;/strong&gt; - registers server&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/blob/main/cloudmesh/openapi/registry/Registry.py&#34;&gt;https://github.com/cloudmesh/cloudmesh-openapi/blob/main/cloudmesh/openapi/registry/Registry.py&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contributors:&lt;/strong&gt; Falconi, Praful, Professor&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;cache&lt;/strong&gt; - cache serialized model locally&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/blob/main/cloudmesh/openapi/registry/cache.py&#34;&gt;https://github.com/cloudmesh/cloudmesh-openapi/blob/main/cloudmesh/openapi/registry/cache.py&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contributors:&lt;/strong&gt; Jonathan&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;fileoperation&lt;/strong&gt; - upload input files&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/blob/main/cloudmesh/openapi/registry/fileoperation.py&#34;&gt;https://github.com/cloudmesh/cloudmesh-openapi/blob/main/cloudmesh/openapi/registry/fileoperation.py&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contributors:&lt;/strong&gt; Prateek, Brian&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;cms scikitlearn - generate sklearn functions&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/blob/main/cloudmesh/openapi/scikitlearn/SklearnGeneratorFile.py&#34;&gt;https://github.com/cloudmesh/cloudmesh-openapi/blob/main/cloudmesh/openapi/scikitlearn/SklearnGeneratorFile.py&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contributors:&lt;/strong&gt; Jagadeesh&lt;/p&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;cms image processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contributors:&lt;/strong&gt; Falconi, Ishan&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;cms text analysis&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contirbutor:&lt;/strong&gt; Andrew Goldfarb&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/tree/main/tests/generator-natural-lang&#34;&gt;https://github.com/cloudmesh/cloudmesh-openapi/tree/main/tests/generator-natural-lang&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;deployment-steps&#34;&gt;Deployment steps&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://cloudmesh.github.io/cloudmesh-openapi/README.html#installation&#34;&gt;https://cloudmesh.github.io/cloudmesh-openapi/README.html#installation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;quick-start&#34;&gt;Quick Start&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://cloudmesh.github.io/cloudmesh-openapi/README.html#quick-steps-to-generate-start-and-stop-cpu-sample-example&#34;&gt;https://cloudmesh.github.io/cloudmesh-openapi/README.html#quick-steps-to-generate-start-and-stop-cpu-sample-example&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;pytests&#34;&gt;Pytests&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://cloudmesh.github.io/cloudmesh-openapi/README.html#pytests&#34;&gt;https://cloudmesh.github.io/cloudmesh-openapi/README.html#pytests&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;limitations&#34;&gt;Limitations&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Integration of openapi with cms allows for running locally only.  Cloud integration was not fully completed although team did create a way to setup openapi in a VM using a remote script for &lt;a href=&#34;https://github.com/cloudmesh/get/blob/main/openapi/ubuntu18.04/index.html&#34;&gt;openstack&lt;/a&gt; and &lt;a href=&#34;https://github.com/cloudmesh/get/blob/main/openapi/google/index.html&#34;&gt;google&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The generator only supports creating arrays of number data type.  This limitation is due to the bug documented below in &lt;em&gt;&lt;strong&gt;Bugs&lt;/strong&gt;&lt;/em&gt; section.  So manual changes are required to the output yaml to allow for other data types until another work around is found or the bug is resolved.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;bugs&#34;&gt;Bugs&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;reported a bug to Connexion and documented it in github for future reference:
&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/issues/60&#34;&gt;https://github.com/cloudmesh/cloudmesh-openapi/issues/60&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;additional-artifacts-produced&#34;&gt;Additional artifacts produced:&lt;/h2&gt;
&lt;h3 id=&#34;openstack-vm-set-up-script&#34;&gt;Openstack VM set up script&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cloudmesh/get/blob/main/openapi/ubuntu18.04/index.html&#34;&gt;OPENSTACK&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cloudmesh/get/blob/main/openapi/google/index.html&#34;&gt;GOOGLE&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Contributors:&lt;/strong&gt; Jonathan Beckford, Andrew Goldfarb&lt;/p&gt;
&lt;h3 id=&#34;openapi-project-readme-generator&#34;&gt;Openapi project readme generator&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/tree/main/sphinx&#34;&gt;https://github.com/cloudmesh/cloudmesh-openapi/tree/main/sphinx&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contributors:&lt;/strong&gt; Jonathan Beckford, Professor&lt;/p&gt;
&lt;h3 id=&#34;chapters&#34;&gt;Chapters&lt;/h3&gt;
&lt;h5 id=&#34;kubernetes&#34;&gt;Kubernetes&lt;/h5&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cloudmesh-community/sp20-516-231/blob/main/chapter/k8s-kubernetes-scheduler.md&#34;&gt;https://github.com/cloudmesh-community/sp20-516-231/blob/main/chapter/k8s-kubernetes-scheduler.md&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contributors:&lt;/strong&gt;  Jonathan Beckford, Brian Kegerreis, Ashok Singam&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/cloudmesh-openapi/readme-adam/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/cloudmesh-openapi/readme-adam/</guid>
      <description>
        
        
        &lt;p&gt;(ENV3) pi@red:~ $ sudo apt-get update
Get:1 &lt;a href=&#34;http://archive.raspberrypi.org/debian&#34;&gt;http://archive.raspberrypi.org/debian&lt;/a&gt; buster InRelease [32.6 kB]
Get:2 &lt;a href=&#34;http://raspbian.raspberrypi.org/raspbian&#34;&gt;http://raspbian.raspberrypi.org/raspbian&lt;/a&gt; buster InRelease [15.0 kB]
Get:3 &lt;a href=&#34;http://raspbian.raspberrypi.org/raspbian&#34;&gt;http://raspbian.raspberrypi.org/raspbian&lt;/a&gt; buster/main armhf Packages [13.0 MB]
Get:4 &lt;a href=&#34;http://archive.raspberrypi.org/debian&#34;&gt;http://archive.raspberrypi.org/debian&lt;/a&gt; buster/main armhf Packages [335 kB]
Fetched 13.4 MB in 30s (450 kB/s)                                                                                 &lt;br&gt;
Reading package lists&amp;hellip; Done&lt;/p&gt;
&lt;p&gt;(ENV3) pi@red:~ $ sudo apt-get upgrade&lt;/p&gt;
&lt;p&gt;(ENV3) pi@red:~ $ sudo cat /var/lib/rancher/k3s/server/node-token
K10126aaacd0a2d1e093900147d64ff9fd68a1663c779c17811fbca9d6f1b31de30::server:c980296a842c92143b6b97d5368d65a9&lt;/p&gt;
&lt;p&gt;(ENV3) pi@red:~ $ hostname -I
169.254.144.52 10.6.0.186&lt;/p&gt;
&lt;p&gt;(ENV3) pi@red:~ $ ssh &lt;a href=&#34;mailto:pi@red001.local&#34;&gt;pi@red001.local&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;pi@red001:~ $ curl -sfL &lt;a href=&#34;https://get.k3s.io&#34;&gt;https://get.k3s.io&lt;/a&gt; |&lt;br&gt;
K3S_URL=&amp;quot;https://169.254.144.52:6443&amp;rdquo;&lt;br&gt;
K3S_TOKEN=K10126aaacd0a2d1e093900147d64ff9fd68a1663c779c17811fbca9d6f1b31de30::server:c980296a842c92143b6b97d5368d65a9&lt;br&gt;
sh -&lt;/p&gt;
&lt;p&gt;pi@red001:~ $ exit&lt;/p&gt;
&lt;p&gt;(ENV3) pi@red:~ $ sudo kubectl get nodes&lt;/p&gt;
&lt;p&gt;NAME   STATUS   ROLES    AGE   VERSION
red    Ready    master   9d    v1.19.3+k3s3&lt;/p&gt;
&lt;p&gt;worker nodes are not showing up&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/cloudmesh-openapi/readme-scikitlearn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/cloudmesh-openapi/readme-scikitlearn/</guid>
      <description>
        
        
        &lt;h1 id=&#34;cloudmesh-openapi-scikit-learn&#34;&gt;Cloudmesh OpenAPI Scikit-learn&lt;/h1&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We use recommend Python 3.8.2 Python or newer.&lt;/li&gt;
&lt;li&gt;We recommend pip version 20.0.2 or newer&lt;/li&gt;
&lt;li&gt;We recommend that you use a venv (see developer install)&lt;/li&gt;
&lt;li&gt;MongoDB installed as regular program not as service&lt;/li&gt;
&lt;li&gt;Please run cim init command to start mongodb server&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We have not checked if it works on older versions.&lt;/p&gt;
&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;
&lt;p&gt;Make sure to follow the instruction for &lt;code&gt;cms openapi&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;When getting started using the &lt;code&gt;openapi&lt;/code&gt;, please first call &lt;code&gt;cms help openapi&lt;/code&gt; to see the available functions and options. For your
convenience we include the manual page later on in this document.&lt;/p&gt;
&lt;h2 id=&#34;scikit-learn-documentation&#34;&gt;Scikit-learn Documentation&lt;/h2&gt;
&lt;p&gt;Scikit-learn is a Machine learning library in Python.We can choose a
ML algorithm like LinearRegression and cloudmesh will be able to spin
up OPENAPI specification for the library we choose.  We can interact
with the Scikit-learn library using either CURL commands or through
GUI.&lt;/p&gt;
&lt;p&gt;This Version of Scikit-learn service accepts csv files in UTF-8 format
only.It is the user responsibility to make sure the files are in UTF-8
format.It is the user responsiblity to split the data in to train and
test datasets.  Split data functionality is not currently supported.&lt;/p&gt;
&lt;h3 id=&#34;setting-up-scikit-learn-service&#34;&gt;Setting up Scikit-learn service&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Please complete the basic installation of
&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi&#34;&gt;cloudmesh-openapi&lt;/a&gt;,
To make set up easy the same steps are even referenced at the
Developer Installation section in the document.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You can find Scikit-learn documentation in
&lt;a href=&#34;https://scikit-learn.org/dev/modules/classes.html&#34;&gt;Scikit-learn&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The following packages needs to be installed to access Scikit-learn&lt;/p&gt;
&lt;p&gt;.. code:: bash&lt;/p&gt;
&lt;p&gt;pip install pandas
pip install Scikit-learn&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Navigate to the &lt;code&gt;./cloudmesh-openapi&lt;/code&gt; directory on your machine&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Utilize the Scikit-learn generate command to create the python file
which will used to generate OpenAPI spec&lt;/p&gt;
&lt;p&gt;.. code:: bash&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi sklearnreadfile sklearn.linear_model.LinearRegression Linregpytest
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The sample generated file can be viewed at
&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/tree/main/tests/generator&#34;&gt;tests/generator&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Utilize the generate command to generate OpenAPI spec with upload functionality enabled&lt;/p&gt;
&lt;p&gt;.. code:: bash&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi generate --filename=./tests/generator/LinearRegression.py --all_functions --enable_upload
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Start the server after the yaml file is generated ot the same directory as the .py file&lt;/p&gt;
&lt;p&gt;.. code:: bash&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi server start ./tests/generator/LinearRegression.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Access the REST service using
&lt;a href=&#34;http://localhost:8080/cloudmesh/ui/&#34;&gt;http://localhost:8080/cloudmesh/ui/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run a curl command against the newly running server to upload the
testfiles.&lt;/p&gt;
&lt;p&gt;Place your test files in
&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/tree/main/tests/Scikitlearn-data&#34;&gt;Scikitlearn-data&lt;/a&gt;
We are testing with X_SAT.csv(SAT Scores of students),y_GPA(GPA of
students)&lt;/p&gt;
&lt;p&gt;.. code:: bash&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -X POST &amp;quot;http://localhost:8080/cloudmesh/upload&amp;quot; \
     -H &amp;quot;accept: text/plain&amp;quot; \
     -H &amp;quot;Content-Type: multipart/form-data&amp;quot; \
     -F &amp;quot;upload=@tests/Scikitlearn-data/X_SAT.csv;type=text/csv&amp;quot;


curl -X POST &amp;quot;http://localhost:8080/cloudmesh/upload&amp;quot; \
     -H &amp;quot;accept: text/plain&amp;quot; \
     -H &amp;quot;Content-Type: multipart/form-data&amp;quot; \
     -F &amp;quot;upload=@tests/Scikitlearn-data/y_GPA.csv;type=text/csv&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run a curl command against the newly running server to verify fit
method in Scikit-learn using the uploaded files&lt;/p&gt;
&lt;p&gt;.. code:: bash&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -X GET &amp;quot;http://localhost:8080/cloudmesh/LinearRegression_upload-enabled/fit?X=X_SAT&amp;amp;y=y_GPA&amp;quot; -H &amp;quot;accept: */*&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run a curl command against the newly running server to run the
Predict method.&lt;/p&gt;
&lt;p&gt;.. code:: bash&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -X GET &amp;quot;http://localhost:8080/cloudmesh/LinearRegression_upload-enabled/predict?X=X_SAT&amp;quot; -H &amp;quot;accept: text/plain&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run a curl command against the newly running server to run the Score method.&lt;/p&gt;
&lt;p&gt;.. code:: bash&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -X GET &amp;quot;http://localhost:8080/cloudmesh/LinearRegression_upload-enabled/score?X=X_SAT&amp;amp;y=y_GPA&amp;quot; -H &amp;quot;accept: text/plain&amp;quot;   
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stop the server&lt;/p&gt;
&lt;p&gt;.. code:: bash&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi server stop LinearRegression
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/cloudmesh-openapi/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/cloudmesh-openapi/readme/</guid>
      <description>
        
        
        &lt;h1 id=&#34;cloudmesh-openapi-service-generator&#34;&gt;Cloudmesh OpenAPI Service Generator&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The README.md page is outomatically generated, do not edit it.
To modify  change the content in
&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/blob/main/README-source.md&#34;&gt;https://github.com/cloudmesh/cloudmesh-openapi/blob/main/README-source.md&lt;/a&gt;
Curley brackets must use two in README-source.md&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/cloudmesh-openapi/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/cloudmesh-openapi.svg&#34; alt=&#34;image&#34;&gt;&lt;/a&gt;
&lt;a href=&#34;https://pypi.python.org/pypi/cloudmesh-openapi&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/cloudmesh-openapi.svg&#34; alt=&#34;Python&#34;&gt;&lt;/a&gt;
&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/blob/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-blue.svg&#34; alt=&#34;License&#34;&gt;&lt;/a&gt;
&lt;a href=&#34;https://pypi.python.org/pypi/cloudmesh-openapi&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/format/cloudmesh-openapi.svg&#34; alt=&#34;Format&#34;&gt;&lt;/a&gt;
&lt;a href=&#34;https://pypi.python.org/pypi/cloudmesh-openapi&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/status/cloudmesh-openapi.svg&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;
&lt;a href=&#34;https://travis-ci.com/cloudmesh/cloudmesh-openapi&#34;&gt;&lt;img src=&#34;https://travis-ci.com/cloudmesh/cloudmesh-openapi.svg?branch=main&#34; alt=&#34;Travis&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We recommend Python 3.8.2 Python or newer.&lt;/li&gt;
&lt;li&gt;We recommend pip version 20.0.2 or newer&lt;/li&gt;
&lt;li&gt;We recommend that you use a venv (see developer install)&lt;/li&gt;
&lt;li&gt;MongoDB installed as regular program not as service, which can
easily be done with cloudmesh on macOS, Linux, and Windows.&lt;/li&gt;
&lt;li&gt;Please run &lt;code&gt;cms gui quick&lt;/code&gt; to initialize the password for the mongodb
server&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Note: On windows you can use &lt;a href=&#34;https://gitforwindows.org/&#34;&gt;gitbash&lt;/a&gt;
so you can use bash and can use the same commands as on Linux or
macOS. Otherwise, please use the appropriate backslashes to access
the path.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;
&lt;p&gt;The installation is rather simple  and is documented next.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python -m venv ~/ENV3
source ~/ENV3/bin/activate 
mkdir cm
cd cm
pip install cloudmesh-installer
cloudmesh-installer get openapi 
cms help
cms gui quick
# fill out mongo variables
# make sure autinstall is True
cms config set cloudmesh.data.mongo.MONGO_AUTOINSTALL=True
cms admin mongo install --force
# Restart a new terminal to make sure mongod is in your path
cms init
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If you like to know more about the installation of cloudmesh, please
visit the &lt;a href=&#34;https://cloudmesh.github.io/cloudmesh-manual/installation/install.html&#34;&gt;Cloudmesh
Manual&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;command-overview&#34;&gt;Command Overview&lt;/h2&gt;
&lt;p&gt;When getting started using cloudmes &lt;code&gt;openapi&lt;/code&gt;, please first call to
get familiar with the options you have:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms help openapi
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We include the manual page later on in this document.&lt;/p&gt;
&lt;h2 id=&#34;quick-start&#34;&gt;Quick Start&lt;/h2&gt;
&lt;p&gt;Next we provide a very simple quickstart guide to steps to generate a
simple microservice that returns the CPU information of your computer.
We demonstrate how to generate, start, and stop the servive.&lt;/p&gt;
&lt;p&gt;Navigate to &lt;code&gt;~/cm/cloudmesh-openapi&lt;/code&gt; folder. In this folder you will
have a file called &lt;code&gt;cpu.py&lt;/code&gt; from which we will generate the server.&lt;/p&gt;
&lt;p&gt;First, generate an OpenAPI YAML file with the convenient command&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi generate get_processor_name \
    --filename=./tests/server-cpu/cpu.py
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This will create the file &lt;code&gt;cpu.yaml&lt;/code&gt; that contains the OpenAPI
specification. To start the service from this specification simply use
the command&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi server start ./tests/server-cpu/cpu.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now that the service is up and running, you can issue a request for
example via the commandline with&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -X GET &amp;quot;http://localhost:8080/cloudmesh/get_processor_name&amp;quot; \
     -H &amp;quot;accept: text/plain&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To view the automatically generated documentation, you can go to your
browser and open the link&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:8080/cloudmesh/ui&#34;&gt;http://localhost:8080/cloudmesh/ui&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;images/openapi-ui.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;You can also look at the status of the server with the command&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi server list
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;images/openapi-info.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Once yo no longer need the service, you can stop it with&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi server stop cpu
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;quickstart-to-creating-your-own-microservice&#34;&gt;Quickstart to Creating your own Microservice&lt;/h2&gt;
&lt;p&gt;Cloudmesh uses introspection to generate an OpenAPI compliant YAML
specification that will allow your Python code to run as a web
service. For this reason, any code you write must conform to a set of
guidelines.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The parameters and return values of any functions you write must use
python typing&lt;/li&gt;
&lt;li&gt;Your functions must include docstrings&lt;/li&gt;
&lt;li&gt;If a function uses or returns a class, that class must be defined as
a dataclass in the same file&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Next we demonstrate how to create your own microservice.
We provide two examples. One in which we return a float,
te other one in which the return value is wrapped in a
json object.&lt;/p&gt;
&lt;h3 id=&#34;returning-a-float&#34;&gt;Returning a Float&lt;/h3&gt;
&lt;p&gt;We define a function that adds tow values.  Note how x,
y, and the return value are all typed. In this case they are all
&lt;code&gt;float&lt;/code&gt;, but other types are supported. The description in the
docstring will be added to your YAML specification to help describe
what the function does.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;def add(x: float, y: float) -&amp;gt; float:
    &amp;quot;&amp;quot;&amp;quot;
    adding float and float.
    :param x: x value
    :type x: float
    :param y: y value
    :type y: float
    :return: result
    :return type: float
    &amp;quot;&amp;quot;&amp;quot;
    result = x + y

    return result
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To generate, start, retrieve a result, and stop the service you can use the
following command sequence:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi generate add --filename=./tests/add-float/add.py
cms openapi server start ./tests/add-float/add.yaml 
curl -X GET &amp;quot;http://localhost:8080/cloudmesh/add?x=1&amp;amp;y=2&amp;quot; -H  &amp;quot;accept: text/plain&amp;quot;
# This command returns
&amp;gt; 3.0
cms openapi server stop add
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;returning-a-json-object&#34;&gt;Returning a Json Object&lt;/h3&gt;
&lt;p&gt;Often we like to wrap the return value into a json string object, which can easily be
done by modifying the previous example as showcased next.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from flask import jsonify

def add(x: float, y: float) -&amp;gt; str:
    &amp;quot;&amp;quot;&amp;quot;
    adding float and float.
    :param x: x value
    :type x: float
    :param y: y value
    :type y: float
    :return: result
    :return type: float
    &amp;quot;&amp;quot;&amp;quot;
    result = {&amp;quot;result&amp;quot;: x + y}

    return jsonify(result)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To generate, start, retrieve a result, and stop the service you can use the
following command sequence:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi generate add --filename=./tests/add-json/add.py
cms openapi server start ./tests/add-json/add.yaml 
curl -X GET &amp;quot;http://localhost:8080/cloudmesh/add?x=1&amp;amp;y=2&amp;quot; -H  &amp;quot;accept: text/plain&amp;quot;
# This command returns
&amp;gt; {&amp;quot;result&amp;quot;:3.0}
cms openapi server stop add
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;As usual in both cases the web browser can be used to inspect the documentation as well as to test running the
example, by filling out the form.&lt;/p&gt;
&lt;h2 id=&#34;details-of-the-cms-openapi-command&#34;&gt;Details of the &lt;code&gt;cms openapi&lt;/code&gt; command&lt;/h2&gt;
&lt;p&gt;The gaol as stated earlier is to transform a simple python function as a service&lt;/p&gt;
&lt;h3 id=&#34;generating-openapi-specification&#34;&gt;Generating OpenAPI specification&lt;/h3&gt;
&lt;p&gt;Once you have a Python function you would like to deploy as a web
service, you can generate the OpenAPI specification. Navigate to your
.py file&amp;rsquo;s directory and generate the YAML. This will print
information to your console about the YAML file that was generated.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi generate [function_name] --filename=[filename.py]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If you would like to include more than one function in your web
service, like addition and subtraction, use the &lt;code&gt;--all_functions&lt;/code&gt;
flag. This will ignore functions whose names start with &amp;lsquo;_&amp;rsquo;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi generate --filename=[filename.py] --all_functions
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You can even write a class like Calculator that contains functions for
addition, subtraction, etc. You can generate a specification for an
entire class by using the &lt;code&gt;--import_class&lt;/code&gt; flag.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi generate [ClassName] --filename=[filename.py] --import_class
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;starting-a-server&#34;&gt;Starting a server&lt;/h3&gt;
&lt;p&gt;Once you have generated a specification, you can start the web service
on your localhost by providing the path to the YAML file. This will
print information to your console about the server&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi server start ./[filename.yaml]

  Starting: [server name]
  PID:      [PID]
  Spec:     ./[filename.py]
  URL:      http://localhost:8080/cloudmesh
  Cloudmesh UI:      http://localhost:8080/cloudmesh/ui
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;sending-requests-to-the-server&#34;&gt;Sending requests to the server&lt;/h3&gt;
&lt;p&gt;Now you have two options to interact with the web service. The first
is to navigate the the Cloudmesh UI and click on each endpoint to test
the functionality. The second is to use curl commands to submit
requests.&lt;/p&gt;
&lt;p&gt;We have already shown you earlier in our quickstart how to apply this to a
service such as our add service&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ curl -X GET &amp;quot;http://localhost:8080/cloudmesh/add?x=1.2&amp;amp;y=1.5&amp;quot; -H &amp;quot;accept: text/plain&amp;quot;
&amp;gt;   2.7
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;stopping-the-server&#34;&gt;Stopping the server&lt;/h3&gt;
&lt;p&gt;Now you can stop the server using the name of the server. If you
forgot the name, use &lt;code&gt;cms openapi server ps&lt;/code&gt; to get a list of server
processes.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ cms openapi server stop [server name]
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;basic-auth&#34;&gt;Basic Auth&lt;/h3&gt;
&lt;p&gt;To use basic http authentication with a user password for the
generated API, add the following flag at the end of a &lt;code&gt;cms openapi generate&lt;/code&gt; command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--basic_auth=&amp;lt;username&amp;gt;:&amp;lt;password&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We plan on supporting more security features in the future. Example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi generate get_processor_name \
    --filename=./tests/server-cpu/cpu.py \
    --basic_auth=admin:secret
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;manual-page&#34;&gt;Manual Page&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;openapi generate [FUNCTION] --filename=FILENAME
                         [--serverurl=SERVERURL]
                         [--yamlfile=YAML]
                         [--import_class]
                         [--all_functions]
                         [--enable_upload]
                         [--verbose]
                         [--basic_auth=CREDENTIALS]
openapi server start YAML [NAME]
              [--directory=DIRECTORY]
              [--port=PORT]
              [--server=SERVER]
              [--host=HOST]
              [--verbose]
              [--debug]
              [--fg]
              [--os]
openapi server stop NAME
openapi server list [NAME] [--output=OUTPUT]
openapi server ps [NAME] [--output=OUTPUT]
openapi register add NAME ENDPOINT
openapi register filename NAME
openapi register delete NAME
openapi register list [NAME] [--output=OUTPUT]
openapi TODO merge [SERVICES...] [--dir=DIR] [--verbose]
openapi TODO doc FILE --format=(txt|md)[--indent=INDENT]
openapi TODO doc [SERVICES...] [--dir=DIR]
openapi sklearn FUNCTION MODELTAG
openapi sklearnreadfile FUNCTION MODELTAG
openapi sklearn upload --filename=FILENAME

Arguments:
  FUNCTION  The name for the function or class
  MODELTAG  The arbirtary name choosen by the user to store the Sklearn trained model as Pickle object
  FILENAME  Path to python file containing the function or class
  SERVERURL OpenAPI server URL Default: https://localhost:8080/cloudmesh
  YAML      Path to yaml file that will contain OpenAPI spec. Default: FILENAME with .py replaced by .yaml
  DIR       The directory of the specifications
  FILE      The specification

Options:
  --import_class         FUNCTION is a required class name instead of an optional function name
  --all_functions        Generate OpenAPI spec for all functions in FILENAME
  --debug                Use the server in debug mode
  --verbose              Specifies to run in debug mode
                         [default: False]
  --port=PORT            The port for the server [default: 8080]
  --directory=DIRECTORY  The directory in which the server is run
  --server=SERVER        The server [default: flask]
  --output=OUTPUT        The outputformat, table, csv, yaml, json
                         [default: table]
  --srcdir=SRCDIR        The directory of the specifications
  --destdir=DESTDIR      The directory where the generated code
                         is placed

Description:
This command does some useful things.

openapi TODO doc FILE --format=(txt|md|rst) [--indent=INDENT]
    Sometimes it is useful to generate teh openaopi documentation
    in another format. We provide fucntionality to generate the
    documentation from the yaml file in a different formt.

openapi TODO doc --format=(txt|md|rst) [SERVICES...]
    Creates a short documentation from services registered in the
    registry.

openapi TODO merge [SERVICES...] [--dir=DIR] [--verbose]
    Merges tow service specifications into a single servoce
    TODO: do we have a prototype of this?


openapi sklearn sklearn.linear_model.LogisticRegression
    Generates the .py file for the Model given for the generator

openapi sklearnreadfile sklearn.linear_model.LogisticRegression
Generates the .py file for the Model given for the generator which supports reading files

openapi generate [FUNCTION] --filename=FILENAME
                             [--serverurl=SERVERURL]
                             [--yamlfile=YAML]
                             [--import_class]
                             [--all_functions]
                             [--enable_upload]
                             [--verbose]
                             [--basic_auth=CREDENTIALS]
    Generates an OpenAPI specification for FUNCTION in FILENAME and
    writes the result to YAML. Use --import_class to import a class
    with its associated class methods, or use --all_functions to 
    import all functions in FILENAME. These options ignore functions
    whose names start with &#39;_&#39;. Use --enable_upload to add file
    upload functionality to a copy of your python file and the
    resulting yaml file.

    For optional basic authorization, we support (temporarily) a single user
    credential. CREDENTIALS should be formatted as follows:

    user:password

    Example: --basic_auth=admin:secret

openapi server start YAML [NAME]
                  [--directory=DIRECTORY]
                  [--port=PORT]
                  [--server=SERVER]
                  [--host=HOST]
                  [--verbose]
                  [--debug]
                  [--fg]
                  [--os]
    starts an openapi web service using YAML as a specification
    TODO: directory is hard coded as None, and in server.py it
      defaults to the directory where the yaml file lives. Can
      we just remove this argument?

openapi server stop NAME
    stops the openapi service with the given name
    TODO: where does this command has to be started from

openapi server list [NAME] [--output=OUTPUT]
    Provides a list of all OpenAPI services in the registry

openapi server ps [NAME] [--output=OUTPUT]
    list the running openapi service

openapi register add NAME ENDPOINT
    Openapi comes with a service registry in which we can register
    openapi services.

openapi register filename NAME
    In case you have a yaml file the openapi service can also be
    registerd from a yaml file

openapi register delete NAME
    Deletes the names service from the registry

openapi register list [NAME] [--output=OUTPUT]
    Provides a list of all registerd OpenAPI services
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;basic-examples&#34;&gt;Basic Examples&lt;/h2&gt;
&lt;p&gt;Please follow &lt;a href=&#34;tests/README.md&#34;&gt;Pytest Information&lt;/a&gt; document for
pytests related information&lt;/p&gt;
&lt;p&gt;We have included a significant number of tests that aso serve as examples&lt;/p&gt;
&lt;h3 id=&#34;example-one-function-in-a-python-file&#34;&gt;Example: One function in a python file&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Please check &lt;a href=&#34;tests/server-cpu/cpu.py&#34;&gt;Python file&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run below command to generate yaml file and start server&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi generate get_processor_name --filename=./tests/server-cpu/cpu.py
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;example-multiple-functions-in-python-file&#34;&gt;Example: Multiple functions in python file&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Please check &lt;a href=&#34;tests/generator-calculator/calculator.py&#34;&gt;Python file&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run below command to generate yaml file and start server&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi generate --filename=./tests/generator-calculator/calculator.py --all_functions
cms openapi generate server start ./tests/generator-calculator/calculator.py
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;example-functions-in-python-class-file&#34;&gt;Example: Function(s) in python class file&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Please check &lt;a href=&#34;tests/generator-testclass/calculator.py&#34;&gt;Python file&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run below command to generate yaml file and start server&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi generate Calculator \
    --filename=./tests/generator-testclass/calculator.py \
    --import_class&amp;quot;
cms openapi server start ./tests/generator-testclass/calculator.yaml
curl -X GET &amp;quot;http://localhost:8080/cloudmesh/Calculator/multiplyint?x=1&amp;amp;y=5&amp;quot;
cms openapi server stop Calculator
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;example-uploading-data&#34;&gt;Example: Uploading data&lt;/h3&gt;
&lt;p&gt;Code to handle uploads is located in
&lt;code&gt;cloudmesh-openapi/tests/generator-upload&lt;/code&gt;. The code snippet in
uploadexample.py and the specification in uploadexample.yaml can be
added to existing projects by adding the &lt;code&gt;--enable_upload&lt;/code&gt; flag to the
&lt;code&gt;cms openapi generate&lt;/code&gt; command. The web service will be able to
retrieve the uploaded file from &lt;code&gt;~/.cloudmesh/upload-file/&lt;/code&gt;.&lt;/p&gt;
&lt;h4 id=&#34;upload-example&#34;&gt;Upload example&lt;/h4&gt;
&lt;p&gt;This example shows how to upload a CSV file and how the web service
can retrieve it.&lt;/p&gt;
&lt;p&gt;First, generate the OpenAPI specification and start the server&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi generate print_csv2np \
    --filename=./tests/generator-upload/csv_reader.py \
    --enable_upload
cms openapi server start ./tests/generator-upload/csv_reader.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Next, navigate to localhost:8080/cloudmesh/ui. Click to open
the /upload endpoint, then click &amp;lsquo;Try it out.&amp;rsquo; Click to choose a file
to upload, then upload &lt;code&gt;tests/generator-upload/np_test.csv&lt;/code&gt;. Click
&amp;lsquo;Execute&amp;rsquo; to complete the upload.&lt;/p&gt;
&lt;p&gt;The uploaded file will be located at
&lt;code&gt;~/.cloudmesh/upload-file/[filename]&lt;/code&gt;. The file
&lt;code&gt;tests/generator-upload/csv_reader.py&lt;/code&gt; contains some example code to
retrieve the array in the uploaded file. To see this in action, click
to open the /print_csv2np endpoint, then click &amp;lsquo;Try it out.&amp;rsquo; Enter
&amp;ldquo;np_test.csv&amp;rdquo; in the field that prompts for a filename, and then click
Execute to view the numpy array defined by the CSV file.&lt;/p&gt;
&lt;h3 id=&#34;example-pipeline-anova-svm-example&#34;&gt;Example: Pipeline Anova SVM Example&lt;/h3&gt;
&lt;p&gt;This example is based on the sklearn example
&lt;a href=&#34;https://scikit-learn.org/stable/auto_examples/feature_selection/plot_feature_selection_pipeline.html#sphx-glr-auto-examples-feature-selection-plot-feature-selection-pipeline-py&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In this example, we will upload a data set to the server, tell the
server to train the model, and utilize said model for predictions.&lt;/p&gt;
&lt;p&gt;Firstly, ensure we are in the correct directory.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ pwd
~/cm/cloudmesh-openapi
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Let us generate the yaml file from our python file to generate the proper specs for our service.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ cms openapi generate PipelineAnovaSVM \
      --filename=./tests/Scikitlearn-experimental/sklearn_svm.py \
      --import_class --enable_upload
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now let us start the server&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ cms openapi server start ./tests/Scikitlearn-experimental/sklearn_svm.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The server should now be active. Navigate to
&lt;code&gt;http://localhost:8080/cloudmesh/ui&lt;/code&gt;. We now have a nice user inteface
to interact with our newly generated API. Let us upload the data
set. We are going to use the iris data set in this example. We have
provided it for you to use. Simply navigate to the &lt;code&gt;/upload&lt;/code&gt; endpoint
by clicking on it, then click &lt;code&gt;Try it out&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We can now upload the file. Click on &lt;code&gt;Choose File&lt;/code&gt; and upload the data
set located at &lt;code&gt;~./tests/Scikitlearn-experimental/iris.data&lt;/code&gt;.  Simply
hit &lt;code&gt;Execute&lt;/code&gt; after the file is uploaded. We should then get a return
code of 200 (telling us that everything went ok).&lt;/p&gt;
&lt;p&gt;The server now has our dataset. Let us now navigate to the &lt;code&gt;/train&lt;/code&gt;
endpoint by, again, clicking on it. Similarly, click &lt;code&gt;Try it out&lt;/code&gt;.
The parameter being asked for is the filename. The filename we are
interested in is &lt;code&gt;iris.data&lt;/code&gt;. Then click &lt;code&gt;execute&lt;/code&gt;.  We should get
another 200 return code with a Classification Report in the Response
Body.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-CLASSIFICATION_REPORT:&#34; data-lang=&#34;CLASSIFICATION_REPORT:&#34;&gt;
           0       1.00      1.00      1.00         8
           1       0.85      1.00      0.92        11
           2       1.00      0.89      0.94        19

    accuracy                           0.95        38
   macro avg       0.95      0.96      0.95        38
weighted avg       0.96      0.95      0.95        38
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Our model is now trained and stored on the server. Let us make a
prediction now. As we have done, navigate to the &lt;code&gt;/make_prediction&lt;/code&gt;
endpoint.  The information we need to provide is the name of the model
we have trained as well as some test data. The name of the model will
be the same as the name of the data-file (ie. iris). So type in &lt;code&gt;iris&lt;/code&gt;
into the &lt;code&gt;model_name&lt;/code&gt; field. Finally for params, let us use the
example &lt;code&gt;5.1, 3.5, 1.4, 0.2&lt;/code&gt; as the model expects 4 values
(attributes). After clicking execute, we should received a response
with the classification the model has made given the parameters.&lt;/p&gt;
&lt;p&gt;The response received should be as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;quot;Classification: [&#39;Iris-setosa&#39;]&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can make as many predictions as we like. When finished, we can shut down the server.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ cms openapi server stop sklearn_svm
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;example-to-run-ai-services-in-the-cloud&#34;&gt;Example to Run AI Services in the Cloud&lt;/h2&gt;
&lt;h3 id=&#34;google&#34;&gt;Google&lt;/h3&gt;
&lt;p&gt;After you create your google cloud account, it is recommended to
download and install Google&amp;rsquo;s &lt;a href=&#34;https://cloud.google.com/sdk/docs/quickstarts&#34;&gt;Cloud
SDK&lt;/a&gt;.  This will
enable CLI. Make sure you enable all the required services.&lt;/p&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;gcloud services enable servicemanagement.googleapis.com
gcloud services enable endpoints.googleapis.com
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;and any other services you might be using for your specific Cloud API
function.&lt;/p&gt;
&lt;p&gt;To begin using the tests for any of the Google Cloud Platform AI
services you must first set up a Google account (set up a free tier
account): &lt;a href=&#34;https://cloud.google.com/billing/docs/how-to/manage-billing-account&#34;&gt;Google Account
Setup&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;After you create your google cloud account, it is recommended to
download and install Google&amp;rsquo;s &lt;a href=&#34;https://cloud.google.com/sdk/docs/quickstarts&#34;&gt;Cloud
SDK&lt;/a&gt;.  This will
enable CLI. Make sure you enable all the required services.&lt;/p&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;gcloud services enable servicemanagement.googleapis.com
gcloud services enable servicecontrol.googleapis.com
gcloud services enable endpoints.googleapis.com
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;and any other services you might be using for your specific Cloud API
function.&lt;/p&gt;
&lt;p&gt;It is also required to install the cloudmesh-cloud package, if not
already installed:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cloudmesh-installer get cloud
cloudmesh-installer install cloud
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This will allow you automatically fill out the cloudmesh yaml file
with your credentials once you generate the servcie account JSON file.&lt;/p&gt;
&lt;p&gt;After you have verified your account is created you must then give your account access to the proper APIs and create a
project in the Google Cloud Platform(GCP) console.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Go to the &lt;a href=&#34;console.cloud.google.com/projectselector2/home/&#34;&gt;project
selector&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Follow directions from Google to create a project linked to your
account&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;quickstart-google-python-api&#34;&gt;Quickstart Google Python API&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;For quickstart in using Google API for Python visit &lt;a href=&#34;https://developers.google.com/docs/api/quickstart/python&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;setting-up-your-google-account&#34;&gt;Setting up your Google account&lt;/h3&gt;
&lt;p&gt;Before you generate the service account JSON file for your account you
will want to enable a number of services in the GCP console.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Google Compute&lt;/li&gt;
&lt;li&gt;Billing&lt;/li&gt;
&lt;li&gt;Cloud Natural Language API&lt;/li&gt;
&lt;li&gt;Translate API&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;To do this you will need to click the menu icon in the Dashboard
navigation bar. Ensure you are in the correct porject.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Once that menu is open hover over the &amp;ldquo;APIs and Services&amp;rdquo; menu item
and click on &amp;ldquo;Dashboard&amp;rdquo; in the submenu.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;At the dashboard click on the &amp;ldquo;+ Enable APIs and Services&amp;rdquo; button
at the top of the dashboard&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Search for &lt;strong&gt;cloud natural language&lt;/strong&gt; to find the API in the search
results and click the result&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Once the page opens click &amp;ldquo;Enable&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Do the same for the &lt;strong&gt;translate&lt;/strong&gt; API to enable that as well&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Do the same for the &lt;strong&gt;compute engine API&lt;/strong&gt; to enable that as well&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You must now properly set up the account roles to ensure you will have
access to the API. Follow the directions from Google to &lt;a href=&#34;https://cloud.google.com/natural-language/docs/setup#auth&#34;&gt;set up proper
authentication&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Make you account an owner for each of the APIs in the IAM tool as
directed in the authentication steps for the natural language API.
This makes your service account have proper access to the required
APIs and once the private key is downloaded those will be stored
there.&lt;/p&gt;
&lt;p&gt;It is VERY important that you create a service account and download
the private key as described in the directions from Google.  If you do
not the cms google commands will not work properly.&lt;/p&gt;
&lt;p&gt;Once you have properly set up your permissions please make sure you
download your JSON private key for the service account that has
permissions set up for the required API services. These steps to
download are found
&lt;a href=&#34;https://cloud.google.com/natural-language/docs/setup#sa-create&#34;&gt;here&lt;/a&gt;.
Please take note of where you store the downloaded JSON and copy the
path string to a easily accessible location.&lt;/p&gt;
&lt;p&gt;The client libraries for each API are included in teh requirements.txt file for the openapi proejct and should be isntalled when the
package is installed. If not, follow directions outlined by google install each package:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;google-cloud-translate
google-cloud-language
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To pass the information from your service account private key file ot
the cloudmesh yaml file run the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms register update --kind=google --service=compute --filename=GOOGLE_JSON_FILE
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;running-the-google-natural-language-and-translate-rest-services&#34;&gt;Running the Google Natural Language and Translate REST Services&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Navigate to the &lt;code&gt;~/.cloudmesh&lt;/code&gt; repo and create a cache directory
for your text examples you would like to analyze.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mkdir text-cache
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add any plain text files your would like to analyze to this
directory with a name that has no special characters or spaces.
You can copy the files at this location,
&lt;code&gt;./cloudmesh-openapi/tests/textanaysis-example-text/reviews/&lt;/code&gt; into
the text-cache if you want to use provided examples.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Navigate to the &lt;code&gt;./cloudmesh-openapi&lt;/code&gt; directory on your machine&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Utilize the generate command to create the OpenAPI spec&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi generate TextAnalysis --filename=./tests/generator-natural-lang/natural-lang-analysis.py --all_functions
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Start the server after the yaml file is generated ot the same
directory as the .py file&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapie start server ./tests/generator-natural-lang/natural-lang-analysis.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run a curl command against the newly running server to verify it
returns a result as expected.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Sample text file name is only meant to be the name of the file
not the full path.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -X GET &amp;quot;http://127.0.0.1:8080/cloudmesh/analyze?filename=SAMPLE_TEXT_FILENAME&amp;amp;cloud=google&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;This is currently only ready to translate a single word through
the API.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -X GET &amp;quot;http://127.0.0.1:8080/cloudmesh/translate_text?cloud=google&amp;amp;text=WORD_TO_TRANSLATE&amp;amp;lang=LANG_CODE&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stop the server&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi server stop natural-lang-analysis
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;example-using-aws&#34;&gt;Example using AWS&lt;/h3&gt;
&lt;p&gt;Sign up for AWS&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Go to &lt;a href=&#34;https://portal.aws.amazon.com/billing/signup&#34;&gt;https://portal.aws.amazon.com/billing/signup&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Follow online instructions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Create an IAM User&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For instructions, see
&lt;a href=&#34;https://docs.aws.amazon.com/IAM/latest/UserGuide/getting-started_create-admin-group.html&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Set up AWS CLI and AWS SDKs&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To download and instructions to install AWS CLI, see
&lt;a href=&#34;https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Install Boto 3&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install boto3
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;For quickstart, vist &lt;a href=&#34;https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As long as you enable all the services you need for using AWS AI APIs you should be able to write your functions for OpenAPI&lt;/p&gt;
&lt;h3 id=&#34;example-using-azure&#34;&gt;Example using Azure&lt;/h3&gt;
&lt;h4 id=&#34;setting-up-azure-sentiment-analysis-and-translation-services&#34;&gt;Setting up Azure Sentiment Analysis and Translation Services&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Create an Azure subscription. If you do not have one, create a
&lt;a href=&#34;https://azure.microsoft.com/try/cognitive-services/&#34;&gt;free account&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create a &lt;a href=&#34;https://portal.azure.com/#create/Microsoft.CognitiveServicesTextAnalytics&#34;&gt;Text Analysis resource&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This link will require you to be logged in to the Azure portal&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create a &lt;a href=&#34;https://docs.microsoft.com/en-us/azure/cognitive-services/cognitive-services-apis-create-account?tabs=multiservice%2Cwindows&#34;&gt;Translation Resource&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The microsoft packages are included in the openapi package
requirements file so they should be installed. If they are not,
install the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install msrest 
pip install azure-ai-textanalytics
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Navigate to the &lt;code&gt;~/.cloudmesh&lt;/code&gt; repo and create a cache directory for your text examples you would like to analyze.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mkdir text-cache
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add any plain text files your would like to analyze to this
directory with a name that has no special characters or spaces.
You can copy the files at this location,
&lt;code&gt;./cloudmesh-openapi/tests/textanaysis-example-text/reviews/&lt;/code&gt; into
the text-cache if you want to use provided examples.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Navigate to the &lt;code&gt;./cloudmesh-openapi&lt;/code&gt; directory on your machine&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Utilize the generate command to create the OpenAPI spec&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi generate TextAnalysis --filename=./tests/generator-natural-lang/natural-lang-analysis.py --all_functions
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Start the server after the yaml file is generated ot the same
directory as the .py file&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapie start server ./tests/generator-natural-lang/natural-lang-analysis.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run a curl command against the newly running server to verify it
returns a result as expected.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Sample text file name is only meant to be the name of the file not the full path.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -X GET &amp;quot;http://127.0.0.1:8080/cloudmesh/analyze?filename=&amp;lt;&amp;lt;sample text file name&amp;gt;&amp;gt;&amp;amp;cloud=azure&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;This is currently only ready to translate a single word through the API.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Available language tags are described in the &lt;a href=&#34;https://docs.microsoft.com/en-us/azure/cognitive-services/translator/reference/v3-0-languages&#34;&gt;Azure docs&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -X GET &amp;quot;http://127.0.0.1:8080/cloudmesh/translate_text?cloud=azure&amp;amp;text=&amp;lt;&amp;lt;word to translate&amp;gt;&amp;gt;&amp;amp;lang=&amp;lt;&amp;lt;lang code&amp;gt;&amp;gt;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stop the server&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi server stop natural-lang-analysis
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The natural langauge analysis API can be improved by allowing for full
phrase translation via the API. If you contribute to this API there is
room for improvement to add custom translation models as well if
preferred to pre-trained APIs.&lt;/p&gt;
&lt;h4 id=&#34;setting-up-azure-computervision-ai-services&#34;&gt;Setting up Azure ComputerVision AI services&lt;/h4&gt;
&lt;h5 id=&#34;prerequisite&#34;&gt;Prerequisite&lt;/h5&gt;
&lt;p&gt;Using the Azure Computer Vision AI service, you can describe, analyze
and/ or get tags for a locally stored image or you can read the text
from an image or hand-written file.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Azure subscription. If you do not have one, create a &lt;a href=&#34;https://azure.microsoft.com/try/cognitive-services/&#34;&gt;free
account&lt;/a&gt; before
you continue further.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create a Computer Vision resource and get the
&lt;code&gt;COMPUTER_VISION_SUBSCRIPTION_KEY&lt;/code&gt; and
&lt;code&gt;COMPUTER_VISION_ENDPOINT&lt;/code&gt;. Follow
&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/cognitive-services/cognitive-services-apis-create-account?tabs=singleservice%2Cunix&#34;&gt;instructions&lt;/a&gt;
to get the same.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install following Python packages in your virtual environment:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install requests
pip install Pillow
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install Computer Vision client library&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install --upgrade azure-cognitiveservices-vision-computervision
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;steps-to-implement-and-use-azure-ai-image-and-text-rest-services&#34;&gt;Steps to implement and use Azure AI image and text &lt;em&gt;REST-services&lt;/em&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Go to &lt;code&gt;./cloudmesh-openapi&lt;/code&gt; directory&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run following command to generate the YAML files&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi generate AzureAiImage --filename=./tests/generator-azureai/azure-ai-image-function.py --all_functions --enable_upload
cms openapi generate AzureAiText --filename=./tests/generator-azureai/azure-ai-text-function.py --all_functions --enable_upload
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Verify the &lt;em&gt;YAML&lt;/em&gt; files created in &lt;code&gt;./tests/generator-azureai&lt;/code&gt; directory&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;azure-ai-image-function.yaml
azure-ai-text-function.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Start the REST service by running following command in &lt;code&gt;./cloudmesh-openapi&lt;/code&gt; directory&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi server start ./tests/generator-azureai/azure-ai-image-function.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The default port used for starting the service is 8080. In case you
want to start more than one REST service, use a different port in
following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi server start ./tests/generator-azureai/azure-ai-text-function.yaml --port=&amp;lt;**Use a different port than 8080**&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Access the REST service using &lt;a href=&#34;http://localhost:8080/cloudmesh/ui/&#34;&gt;http://localhost:8080/cloudmesh/ui/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;After you have started the azure-ai-image-function or azure-ai-text-function on default port 8080, run following command to upload the image or text_image file&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -X POST &amp;quot;http://localhost:8080/cloudmesh/upload&amp;quot; -H  &amp;quot;accept: text/plain&amp;quot; -H  &amp;quot;Content-Type: multipart/form-data&amp;quot; -F &amp;quot;upload=@tests/generator-azureai/&amp;lt;image_name_with_extension&amp;gt;;type=image/jpeg&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Keep your test image files at &lt;code&gt;./tests/generator-azureai/&lt;/code&gt; directory&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;With &lt;em&gt;azure-ai-text-function&lt;/em&gt; started on port=8080, in order to test the azure ai function for text detection in an image, run following command&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -X GET &amp;quot;http://localhost:8080/cloudmesh/azure-ai-text-function_upload-enabled/get_text_results?image_name=&amp;lt;image_name_with_extension_uploaded_earlier&amp;gt;&amp;quot; -H &amp;quot;accept: text/plain&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;With &lt;em&gt;azure-ai-image-function&lt;/em&gt; started on port=8080, in order to
test the azure ai function for describing an image, run following
command&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -X GET &amp;quot;http://localhost:8080/cloudmesh/azure-ai-image-function_upload-enabled/get_image_desc?image_name=&amp;lt;image_name_with_extension_uploaded_earlier&amp;gt;&amp;quot; -H &amp;quot;accept: text/plain&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;With &lt;em&gt;azure-ai-image-function&lt;/em&gt; started on port=8080, in order to
test the azure ai function for analyzing an image, run following
command&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -X GET &amp;quot;http://localhost:8080/cloudmesh/azure-ai-image-function_upload-enabled/get_image_analysis?image_name=&amp;lt;image_name_with_extension_uploaded_earlier&amp;gt;&amp;quot; -H &amp;quot;accept: text/plain&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;With &lt;em&gt;azure-ai-image-function&lt;/em&gt; started on port=8080, in order to
test the azure ai function for identifying tags in an image, run
following command&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -X GET &amp;quot;http://localhost:8080/cloudmesh/azure-ai-image-function_upload-enabled/get_image_tags?image_name=&amp;lt;image_name_with_extension_uploaded_earlier&amp;gt;&amp;quot; -H &amp;quot;accept: text/plain&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Check the running REST services using following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi server ps
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stop the REST service using following command(s):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi server stop azure-ai-image-function
cms openapi server stop azure-ai-text-function
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;list-of-tests&#34;&gt;List of Tests&lt;/h2&gt;
&lt;p&gt;The following table lists the different test we have, we provide additional
information for the tests in the test directory in a README file. Summaries
are provided below the table&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Test&lt;/th&gt;
&lt;th&gt;Short Description&lt;/th&gt;
&lt;th&gt;Link&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Generator-calculator&lt;/td&gt;
&lt;td&gt;Test to check if calculator api is generated correctly. This is to test multiple function in one python file&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/blob/main/tests/generator-calculator/test_01_generator.py&#34;&gt;test_01_generator.py&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Generator-testclass&lt;/td&gt;
&lt;td&gt;Test to check if calculator api is generated correctly. This is to test multiple function in one python class file&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/blob/main/tests/generator-testclass/test_02_generator.py&#34;&gt;test_02_generator.py&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Server-cpu&lt;/td&gt;
&lt;td&gt;Test to check if cpu api is generated correctly. This is to test single function in one python file and function name is different than file name&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/blob/main/tests/server-cpu/test_03_generator.py&#34;&gt;test_03_generator.py&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Server-cms&lt;/td&gt;
&lt;td&gt;Test to check if cms api is generated correctly. This is to test multiple function in one python file.&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/blob/main/tests/server-cms/test_04_generator.py&#34;&gt;test_04_generator.py&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Registry&lt;/td&gt;
&lt;td&gt;test_001_registry.py - Runs tests for registry. Description is in tests/README.md&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/blob/main/tests/README.md&#34;&gt;Link&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Image-Analysis&lt;/td&gt;
&lt;td&gt;image_test.py - Runs benchmark for text detection for Google Vision API and AWS Rekognition. Description in image-analysis/README.md&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/blob/main/tests/image-analysis/README.md&#34;&gt;image&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;For more information about test cases ,please check &lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/blob/main/tests/README.md&#34;&gt;tests info&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;tests/test_001_registry.py&#34;&gt;test_001_registry&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;tests/test_003_server_manage_cpu.py&#34;&gt;test_003_server_manage_cpu&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;tests/test_010_generator.py&#34;&gt;test_010_generator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;tests/test_011_generator_cpu.py&#34;&gt;test_011_generator_cpu&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;tests/test_012_generator_calculator.py&#34;&gt;test_012_generator_calculator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;tests/test_015_generator_azureai.py&#34;&gt;test_015_generator_azureai&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;tests/test_020_server_manage.py&#34;&gt;test_020_server_manage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;tests/test_server_cms_cpu.py&#34;&gt;test_server_cms_cpu&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that there a many more tests that you can explore.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/cloudmesh-openapi/tests/add-float/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/cloudmesh-openapi/tests/add-float/readme/</guid>
      <description>
        
        
        &lt;h1 id=&#34;readme&#34;&gt;README&lt;/h1&gt;
&lt;p&gt;please see the README in the root dir of this repository&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/cloudmesh-openapi/tests/add-json/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/cloudmesh-openapi/tests/add-json/readme/</guid>
      <description>
        
        
        &lt;h1 id=&#34;readme&#34;&gt;README&lt;/h1&gt;
&lt;p&gt;please see the README in the root dir of this repository&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/cloudmesh-openapi/tests/generator-natural-lang/googlecloudvmset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/cloudmesh-openapi/tests/generator-natural-lang/googlecloudvmset/</guid>
      <description>
        
        
        &lt;h1 id=&#34;steps&#34;&gt;Steps:&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Setup a google account with Google Cloud&lt;/li&gt;
&lt;li&gt;Create a project&lt;/li&gt;
&lt;li&gt;Set permission for create on compute engine in the project&lt;/li&gt;
&lt;li&gt;create a service account file and link to json in cloudmesh yaml file
&lt;a href=&#34;https://cloud.google.com/docs/authentication/production?hl=en_US&#34;&gt;https://cloud.google.com/docs/authentication/production?hl=en_US&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Create a storage location using google storage
&lt;a href=&#34;https://cloud.google.com/storage/docs/creating-buckets#storage-create-bucket-code_samples&#34;&gt;https://cloud.google.com/storage/docs/creating-buckets#storage-create-bucket-code_samples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Install the google cloud sdk
&lt;a href=&#34;https://cloud.google.com/compute/docs/tutorials/python-guide&#34;&gt;https://cloud.google.com/compute/docs/tutorials/python-guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Install the google cloud api client library
&lt;a href=&#34;https://cloud.google.com/apis/docs/client-libraries-explained&#34;&gt;https://cloud.google.com/apis/docs/client-libraries-explained&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Write a startup script for your vm&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;azure&#34;&gt;Azure&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal&#34;&gt;https://docs.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal&lt;/a&gt;
Credentials:
app-name: vm-creation-example
auth url:https://andyvmcreateexample.com/auth&lt;/p&gt;
&lt;p&gt;app (client) ID: 8db85342-7efd-433c-aeba-d175ae4d4404
directory (tenant) id: 398e5475-e850-4239-ba0d-62ddc3e644ff
object ID: 38224a7e-79e0-4642-b765-2bf731d296ad
client-secret: w[f7o=[dKKeSn?VxF3iNoZDW3ctMmd3G
subscription id:4513afc9-4159-49d0-aa1d-0a2a0ab9933c&lt;/p&gt;
&lt;p&gt;when creating a vm in the portal the network interface is set up for you
but if you do it programmatically you have to set it up.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/cloudmesh-openapi/tests/gregor/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/cloudmesh-openapi/tests/gregor/readme/</guid>
      <description>
        
        
        &lt;h1 id=&#34;test-it-yourself&#34;&gt;Test it yourself&lt;/h1&gt;
&lt;p&gt;cd to &lt;code&gt;cloudmesh-openapi&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Start the service&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapii server start ./tests/server-cpu.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Stop the service&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapii3 server stop cpu
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;urls&lt;/p&gt;
&lt;p&gt;cloudmesh/ui&lt;/p&gt;
&lt;p&gt;cloudmesh/cpu&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/cloudmesh-openapi/tests/image-analysis/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/cloudmesh-openapi/tests/image-analysis/readme/</guid>
      <description>
        
        
        &lt;h1 id=&#34;test-it-yourself&#34;&gt;Test it yourself&lt;/h1&gt;
&lt;h2 id=&#34;in-cloudmesh-openapi&#34;&gt;In cloudmesh-openapi&lt;/h2&gt;
&lt;p&gt;Start server&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cms openapi server start ./tests/image-analysis/image.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Get Response Google Vision&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;curl -sL http://127.0.0.1:8080/cloudmesh/image/detect_text_google
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Get Response AWS Rekognition&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;curl -sL http://127.0.0.1:8080/cloudmesh/image/detect_text_aws
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Stop server&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cms openapi server stop image
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;urls&lt;/p&gt;
&lt;p&gt;cloudmesh/image/detect_text_google&lt;/p&gt;
&lt;p&gt;cloudmesh/image/detect_text_aws&lt;/p&gt;
&lt;h2 id=&#34;image_testpy&#34;&gt;image_test.py&lt;/h2&gt;
&lt;p&gt;How to run test&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;pytest -v --capture&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;no tests/image-analysis/image_test.py 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;image_test.py has 7 tests&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Uses generate command to generate new yaml file&lt;/li&gt;
&lt;li&gt;Check yaml syntax&lt;/li&gt;
&lt;li&gt;Starts server&lt;/li&gt;
&lt;li&gt;Does a curl call for google vision api response&lt;/li&gt;
&lt;li&gt;Does a curl call for aws rekognition api response&lt;/li&gt;
&lt;li&gt;Stops the server&lt;/li&gt;
&lt;li&gt;Prints benchmark&lt;/li&gt;
&lt;/ol&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/cloudmesh-openapi/tests/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/cloudmesh-openapi/tests/readme/</guid>
      <description>
        
        
        &lt;h1 id=&#34;how-to-write-and-run-test-case-for-openapi&#34;&gt;How to write and run test case for OpenAPI&lt;/h1&gt;
&lt;h2 id=&#34;this-document-will-explain-how-to-validate-if-openapi-is-generated-correctly-and-server-start-and-stop-working-correctly&#34;&gt;This document will explain how to validate if openapi is generated correctly and server start and stop working correctly&lt;/h2&gt;
&lt;h3 id=&#34;we-have-create-a-framework-class-which-has-below-basic-test-case-functions&#34;&gt;We have create a framework class which has below basic test case functions&lt;/h3&gt;
&lt;p&gt;Framework file is present under tests/lib named as generator_test.py&lt;/p&gt;
&lt;h4 id=&#34;below-test-cases-are-related-to-generator-api&#34;&gt;Below test cases are related to generator API&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Create a build folder and copy py file into it. Build sub folder will created where test py file present.&lt;/li&gt;
&lt;li&gt;It will call generator generate function to generate Yaml file inside build folder&lt;/li&gt;
&lt;li&gt;It will check if generated YMAL file syntax is correct or not.&lt;/li&gt;
&lt;li&gt;It will check if number of function generated in YMAL is same as py file.&lt;/li&gt;
&lt;li&gt;Delete the build folder.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;two-test-cases-are-related-to-server-api&#34;&gt;Two test cases are related to server API&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;It will start server&lt;/li&gt;
&lt;li&gt;It will stop server&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;how-to-create-test-case&#34;&gt;How to create test case&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;If you creating new Open API , then inside tests folder you have to commit your working py and yaml files.&lt;/li&gt;
&lt;li&gt;Create new function for test case where py and yaml located. Example (test_01_generator)&lt;/li&gt;
&lt;li&gt;We have already created test cases function file for generator-calculator name as test_01_generator.py. Please check this file.&lt;/li&gt;
&lt;li&gt;Copy the contains of test_01_generator.py and paste inside your test py file.&lt;/li&gt;
&lt;li&gt;Change startservercommand and filename variables value accordingly to your use case.&lt;/li&gt;
&lt;li&gt;Change some of parameters of constructor of GeneratorBaseTest class.&lt;/li&gt;
&lt;li&gt;if your py file has a class then.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt; &lt;span style=&#34;color:#000&#34;&gt;gen&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; GeneratorBaseTest&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;(&lt;/span&gt;filename,False,True&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol&gt;
&lt;li&gt;if your py file has functions then&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt; &lt;span style=&#34;color:#000&#34;&gt;gen&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; GeneratorBaseTest&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;(&lt;/span&gt;filename,True,False&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol&gt;
&lt;li&gt;First boolean flag in GeneratorBaseTest for &amp;ndash;all_functions and second flag is for &amp;ndash;import_class&lt;/li&gt;
&lt;li&gt;If you need to write more test cases based on your requirement, check order of test case and write accordingly.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;how-to-run-test-case&#34;&gt;How to run test case&lt;/h3&gt;
&lt;p&gt;Below command can use to run your case. Make sure your current directory is cloudmesh-openapi.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ how &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;do&lt;/span&gt; you call this you can add -x to stop pytest when first &lt;span style=&#34;color:#204a87&#34;&gt;test&lt;/span&gt; failed
pytest -v  --capture&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;no tests/generator-calculator/test_01_generator.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;run-test-case-with-csv-command-enabled&#34;&gt;Run test case with CSV command enabled&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ how &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;do&lt;/span&gt; you call this , you can add -x to stop pytest when first &lt;span style=&#34;color:#204a87&#34;&gt;test&lt;/span&gt; failed
pytest -v  --capture&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;no tests/generator-calculator/test_01_generator.py  &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;|&lt;/span&gt; fgrep &lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#39;# cvs&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;below-are-test-case-files&#34;&gt;Below are test case files&lt;/h2&gt;
&lt;p&gt;Generator-calculator and file name is test_01_generator.py&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pytest -v  --capture=no tests/generator-calculator/test_01_generator.py
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Generator-testclass and file name is test_02_generator.py&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pytest -v --capture=no tests/generator-testclass/test_02_generator
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Server-cpu and file name is test_03_generator.py&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pytest -v  --capture=no tests/server-cpu/test_03_generator
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Server-cms and file name is test_04_generator.py&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pytest -v  --capture=no tests/server-cms/test_04_generator
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Generator and file name is test_05_generator.py&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pytest -v --capture=no tests/generator/test_05_generator
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Azure AI Image Function is test_06_generator.py&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;pytest -v --capture&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;no tests/generator_azureai/test_06_generator
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Azure AI Text Function is test_07_generator.py&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;pytest -v --capture&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;no tests/generator_azureai/test_07_generator
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Natural Language Analysis Generator Tests are run from test_generator_natural_language.py&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;pytest -v --capture&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;no  ./tests/test_generator_natural_language.py::TestGenerator
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This test will generate an OpenAPI spec for the natural-lang-analysis.py file located in the generator-natural-lang
directory. If the above command is copied and pasted to run in the terminal it will do the following.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Generate a yaml file&lt;/li&gt;
&lt;li&gt;Verify the spec has all the functions that are available in the natural-lang-analysis.py file&lt;/li&gt;
&lt;li&gt;Start a server hosting the openAPI spec&lt;/li&gt;
&lt;li&gt;Run a call against the sentiment analysis and translation endpoint for each available cloud service (Google/Azure) and verify it was successful.&lt;/li&gt;
&lt;li&gt;Stop the service&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Results for Natural Language Tests&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Attribute&lt;/th&gt;
&lt;th&gt;Value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;cpu_count&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mem.active&lt;/td&gt;
&lt;td&gt;2.0 GiB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mem.available&lt;/td&gt;
&lt;td&gt;2.1 GiB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mem.free&lt;/td&gt;
&lt;td&gt;148.8 MiB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mem.inactive&lt;/td&gt;
&lt;td&gt;2.0 GiB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mem.percent&lt;/td&gt;
&lt;td&gt;73.2 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mem.total&lt;/td&gt;
&lt;td&gt;8.0 GiB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mem.used&lt;/td&gt;
&lt;td&gt;4.8 GiB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mem.wired&lt;/td&gt;
&lt;td&gt;2.8 GiB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;platform.version&lt;/td&gt;
&lt;td&gt;10.14.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;python&lt;/td&gt;
&lt;td&gt;3.8.1 (v3.8.1:1b293b6006, Dec 18 2019, 14:08:53)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;[Clang 6.0 (clang-600.0.57)]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;python.pip&lt;/td&gt;
&lt;td&gt;20.0.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;python.version&lt;/td&gt;
&lt;td&gt;3.8.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;sys.platform&lt;/td&gt;
&lt;td&gt;darwin&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;uname.machine&lt;/td&gt;
&lt;td&gt;x86_64&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;uname.node&lt;/td&gt;
&lt;td&gt;Andrews-MacBook-Pro.local&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;uname.processor&lt;/td&gt;
&lt;td&gt;i386&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;uname.release&lt;/td&gt;
&lt;td&gt;18.2.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;uname.system&lt;/td&gt;
&lt;td&gt;Darwin&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;uname.version&lt;/td&gt;
&lt;td&gt;Darwin Kernel Version 18.2.0: Fri Oct  5 19:41:49 PDT 2018; root:xnu-4903.221.2~2/RELEASE_X86_64&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;user&lt;/td&gt;
&lt;td&gt;andrewgoldfarb&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;Status&lt;/th&gt;
&lt;th&gt;Time&lt;/th&gt;
&lt;th&gt;Start&lt;/th&gt;
&lt;th&gt;tag&lt;/th&gt;
&lt;th&gt;Node&lt;/th&gt;
&lt;th&gt;User&lt;/th&gt;
&lt;th&gt;OS&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;generator_test/copy_py_file&lt;/td&gt;
&lt;td&gt;ok&lt;/td&gt;
&lt;td&gt;0.003&lt;/td&gt;
&lt;td&gt;2020-05-09 06:33:47&lt;/td&gt;
&lt;td&gt;openapi&lt;/td&gt;
&lt;td&gt;Andrews-MacBook-Pro.local&lt;/td&gt;
&lt;td&gt;andrewgoldfarb&lt;/td&gt;
&lt;td&gt;Darwin&lt;/td&gt;
&lt;td&gt;10.14.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;generator_test/generate&lt;/td&gt;
&lt;td&gt;ok&lt;/td&gt;
&lt;td&gt;2.601&lt;/td&gt;
&lt;td&gt;2020-05-09 06:33:47&lt;/td&gt;
&lt;td&gt;openapi&lt;/td&gt;
&lt;td&gt;Andrews-MacBook-Pro.local&lt;/td&gt;
&lt;td&gt;andrewgoldfarb&lt;/td&gt;
&lt;td&gt;Darwin&lt;/td&gt;
&lt;td&gt;10.14.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;generator_test/read_spec&lt;/td&gt;
&lt;td&gt;ok&lt;/td&gt;
&lt;td&gt;0.012&lt;/td&gt;
&lt;td&gt;2020-05-09 06:33:49&lt;/td&gt;
&lt;td&gt;openapi&lt;/td&gt;
&lt;td&gt;Andrews-MacBook-Pro.local&lt;/td&gt;
&lt;td&gt;andrewgoldfarb&lt;/td&gt;
&lt;td&gt;Darwin&lt;/td&gt;
&lt;td&gt;10.14.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;generator_test/start_service&lt;/td&gt;
&lt;td&gt;ok&lt;/td&gt;
&lt;td&gt;1.864&lt;/td&gt;
&lt;td&gt;2020-05-09 06:33:49&lt;/td&gt;
&lt;td&gt;openapi&lt;/td&gt;
&lt;td&gt;Andrews-MacBook-Pro.local&lt;/td&gt;
&lt;td&gt;andrewgoldfarb&lt;/td&gt;
&lt;td&gt;Darwin&lt;/td&gt;
&lt;td&gt;10.14.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;test_generator_natural_language/test_run_analyze_google&lt;/td&gt;
&lt;td&gt;ok&lt;/td&gt;
&lt;td&gt;0.67&lt;/td&gt;
&lt;td&gt;2020-05-09 06:33:51&lt;/td&gt;
&lt;td&gt;openapi&lt;/td&gt;
&lt;td&gt;Andrews-MacBook-Pro.local&lt;/td&gt;
&lt;td&gt;andrewgoldfarb&lt;/td&gt;
&lt;td&gt;Darwin&lt;/td&gt;
&lt;td&gt;10.14.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;test_generator_natural_language/test_run_analyze_azure&lt;/td&gt;
&lt;td&gt;ok&lt;/td&gt;
&lt;td&gt;0.58&lt;/td&gt;
&lt;td&gt;2020-05-09 06:33:52&lt;/td&gt;
&lt;td&gt;openapi&lt;/td&gt;
&lt;td&gt;Andrews-MacBook-Pro.local&lt;/td&gt;
&lt;td&gt;andrewgoldfarb&lt;/td&gt;
&lt;td&gt;Darwin&lt;/td&gt;
&lt;td&gt;10.14.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;generator_test/stop_server&lt;/td&gt;
&lt;td&gt;ok&lt;/td&gt;
&lt;td&gt;2.095&lt;/td&gt;
&lt;td&gt;2020-05-09 06:33:52&lt;/td&gt;
&lt;td&gt;openapi&lt;/td&gt;
&lt;td&gt;Andrews-MacBook-Pro.local&lt;/td&gt;
&lt;td&gt;andrewgoldfarb&lt;/td&gt;
&lt;td&gt;Darwin&lt;/td&gt;
&lt;td&gt;10.14.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;generator_test/delete_file&lt;/td&gt;
&lt;td&gt;ok&lt;/td&gt;
&lt;td&gt;0.002&lt;/td&gt;
&lt;td&gt;2020-05-09 06:33:54&lt;/td&gt;
&lt;td&gt;openapi&lt;/td&gt;
&lt;td&gt;Andrews-MacBook-Pro.local&lt;/td&gt;
&lt;td&gt;andrewgoldfarb&lt;/td&gt;
&lt;td&gt;Darwin&lt;/td&gt;
&lt;td&gt;10.14.1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;todo-describe-what-they-do&#34;&gt;TODO DESCRIBE WHAT THEY DO&lt;/h2&gt;
&lt;p&gt;cache-scikitlearn
deprecated
examples
generator
generator-azureai
generator-calculator
generator-printerclass
generator-testclass
generator-upload
image-analysis
lib
Scikit-learntestfiles
Scikitlearn_tests
server-cms
server-cms-simple
server-cpu
server-sample
server-sampleFunction
test_mlperf
textanalysis-example-text
&lt;strong&gt;init&lt;/strong&gt;.py
README.md
test_001_registry.py
test_03_generator.py
test_010_generator.py
test_011_generator_cpu.py
test_012_generator_calculator.py
test_015_generator_azureai.py
test_020_server_manage.py
test_server_cms_cpu.py
util.py&lt;/p&gt;
&lt;p&gt;THIS WAS HERE BEFORE&lt;/p&gt;
&lt;h2 id=&#34;test_001_registrypy&#34;&gt;test_001_registry.py&lt;/h2&gt;
&lt;p&gt;This test has 5 test functions&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;test_registry_add&lt;/li&gt;
&lt;li&gt;test_registry_list_name&lt;/li&gt;
&lt;li&gt;test_registry_list&lt;/li&gt;
&lt;li&gt;test_registry_delete&lt;/li&gt;
&lt;li&gt;test_benchmark&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Test 1 calls registry and adds to the registry. If successful prints &amp;lsquo;PASSED&amp;rsquo;&lt;/p&gt;
&lt;p&gt;Test 2 calls registry and prints ONLY the server specified in filename.&lt;/p&gt;
&lt;p&gt;Test 3 calls registry and print list for ALL servers in registry.&lt;/p&gt;
&lt;p&gt;Test 4 calls registry and deletes entry for filename.&lt;/p&gt;
&lt;p&gt;Test 5 runs benchmark test on registry.&lt;/p&gt;
&lt;h3 id=&#34;how-to-call-this&#34;&gt;How to call this&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cms &lt;span style=&#34;color:#204a87&#34;&gt;set&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;filename&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#34;./tests/server-cpu/cpu.yaml&amp;#34;&lt;/span&gt;
pytest -v --capture&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;no tests/test_001_registry.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;deprecated
examples
generator
generator-calculator
generator-printerclass
generator-testclass
server-class
server-cms
server-cms-simple
server-cpu
server-sample
server-sampleFunction
textanalysis-example-text
&lt;strong&gt;init&lt;/strong&gt;.py
README.md
test_001_registry.py  Falconi
test_03_generator.py  jonthan
test_010_generator.py jonthan
test_011_generator_cpu.py prateek
test_012_generator_calculator.py prateek
test_020_server_manage.py ishan
test_server_cms_cpu.py andrew&amp;ndash;&amp;gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/cloudmesh-openapi/tests/server-cpu/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/cloudmesh-openapi/tests/server-cpu/readme/</guid>
      <description>
        
        
        &lt;h1 id=&#34;test-it-yourself&#34;&gt;Test it yourself&lt;/h1&gt;
&lt;p&gt;cd to &lt;code&gt;cloudmesh-openapi&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Start the service&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapii server start ./tests/server-cpu.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Stop the service&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapii3 server stop cpu
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;urls&lt;/p&gt;
&lt;p&gt;cloudmesh/ui&lt;/p&gt;
&lt;p&gt;cloudmesh/cpu&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/cloudmesh-openapi/tests/test_mlperf/readme-source/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/cloudmesh-openapi/tests/test_mlperf/readme-source/</guid>
      <description>
        
        
        &lt;h1 id=&#34;mlperf-tests&#34;&gt;MLPerf Tests&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;http://mlperf.org&#34;&gt;MLperf&lt;/a&gt; [@www-mlperf] provides &amp;ldquo;fair and useful benchmarks for measuring
training and inference performance of ML hardware, software, and
services&amp;rdquo;&lt;/p&gt;
&lt;p&gt;In this benchmark we will&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Deploy MLPerf on the system&lt;/li&gt;
&lt;li&gt;Use functions that run a number of tests as inout to the OpenAPI Gnerator&lt;/li&gt;
&lt;li&gt;From these functions we run our OpenAPI generator to create a service
that allows to run the MLperf examples through a Web service with
http calls&lt;/li&gt;
&lt;li&gt;Test out the created functions by running selected example invocations&lt;/li&gt;
&lt;li&gt;Report the time it takes to run these examples&lt;/li&gt;
&lt;li&gt;Provide a Makefile or python script that allows us to conveniently
cun these tests&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;deployment&#34;&gt;Deployment&lt;/h2&gt;
&lt;p&gt;Describe how to deploy&lt;/p&gt;
&lt;h3 id=&#34;reports-for-running-the-tests-on-machines&#34;&gt;Reports for running the tests on Machines&lt;/h3&gt;
&lt;p&gt;Provide summary information about teh runtime
Provide details do checked in results in the &lt;a href=&#34;results&#34;&gt;results&lt;/a&gt; directory&lt;/p&gt;
&lt;h3 id=&#34;local-output&#34;&gt;Local Output&lt;/h3&gt;
&lt;p&gt;All output is written into a &lt;code&gt;~/.cloudmesh/dest/benchmark/mlperf&lt;/code&gt; folder
which can be removed after the test is completed. In the results folder
we also find a copy of the OpenAPI YAML file that is generated with the
cenerator. This file can also be used to compare the generated output.&lt;/p&gt;
&lt;h2 id=&#34;selected-benchmarks&#34;&gt;Selected Benchmarks&lt;/h2&gt;
&lt;p&gt;Describe which benchmarks were selected&lt;/p&gt;
&lt;h2 id=&#34;functions&#34;&gt;Functions&lt;/h2&gt;
&lt;p&gt;Short description aboutthe functions that have been defined&lt;/p&gt;
&lt;h2 id=&#34;opeanapi&#34;&gt;OpeanAPI&lt;/h2&gt;
&lt;p&gt;Describe where to find the generated functions
Link th=o wher ethe open api is created in the&lt;/p&gt;
&lt;h2 id=&#34;how-to-run-individual-tests&#34;&gt;How to run individual tests&lt;/h2&gt;
&lt;p&gt;Describe how to run indific=dual Tests&lt;/p&gt;
&lt;h2 id=&#34;benchmarks&#34;&gt;Benchmarks&lt;/h2&gt;
&lt;p&gt;Links to benchmarks that are listed in the &lt;a href=&#34;results&#34;&gt;results&lt;/a&gt; directory&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/cloudmesh-openapi/tests/test_mlperf/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/cloudmesh-openapi/tests/test_mlperf/readme/</guid>
      <description>
        
        
        &lt;h1 id=&#34;mlperf-tests&#34;&gt;MLPerf Tests&lt;/h1&gt;
&lt;p&gt;According to &lt;a href=&#34;https://mlperf.org/&#34;&gt;https://mlperf.org/&lt;/a&gt; MLPerf provides &amp;quot; Fair and useful
benchmarks for measuring training and inference performance of ML
hardware, software, and services&amp;rdquo; [@www-mlperf]&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/cloudmesh-openapi/tests/test_mlperf/results/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/cloudmesh-openapi/tests/test_mlperf/results/readme/</guid>
      <description>
        
        
        &lt;p&gt;put your result files here&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/cloudmesh-openapi/tests/timeseries-example/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/cloudmesh-openapi/tests/timeseries-example/readme/</guid>
      <description>
        
        
        &lt;h1 id=&#34;time-series-forecast-using-multi-cloud-ai-services&#34;&gt;Time Series Forecast using Multi Cloud AI Services&lt;/h1&gt;
&lt;p&gt;Prafull Porwal, &lt;a href=&#34;https://github.com/cloudmesh-community/sp20-516-255/blob/main/Cloudmesh-OpenAPI/Readme.md&#34;&gt;sp20-516-255&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/cloudmesh-community/sp20-516-255/graphs/contributors&#34;&gt;Contributors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/cloudmesh-community/fa19-516-147/pulse&#34;&gt;Insights&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/cloudmesh-community/sp20-516-255/tree/main/Cloudmesh-OpenAPI/AWSForecast&#34;&gt;Project Code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;objective&#34;&gt;Objective&lt;/h2&gt;
&lt;p&gt;Develop Open API for time series forecasting on multiple clouds&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Many cloud providers have introduced machine learning capabilities on their infrastructure. The project aims to provide an open API for timeseries forecasting for AWS using Forecast Services and S3&lt;/p&gt;
&lt;h3 id=&#34;aws-ai-service--forecast-open-api-service-features&#34;&gt;AWS AI Service : Forecast Open API Service Features&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Upload the data file to ./cloudmesh/upload-file location&lt;/li&gt;
&lt;li&gt;Upload the json schema file to ./cloudmesh/upload-file location&lt;/li&gt;
&lt;li&gt;Validate the data for missing and less than 0 values&lt;/li&gt;
&lt;li&gt;Split the dataset into Train and test by specifying split percentge.&lt;/li&gt;
&lt;li&gt;Provide list of Multi Cloud supported for Timeseries Forecasting&lt;/li&gt;
&lt;li&gt;Initialize the cloud service&lt;/li&gt;
&lt;li&gt;Create a Dataset Group&lt;/li&gt;
&lt;li&gt;Create a Target Time Series Dataset&lt;/li&gt;
&lt;li&gt;Import data into Forecast from AWS Storage S3&lt;/li&gt;
&lt;li&gt;Create a Predictor&lt;/li&gt;
&lt;li&gt;Generate Forecast&lt;/li&gt;
&lt;li&gt;Query the Forecast&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;additional-features&#34;&gt;Additional Features&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Multiple instance of the process supported&lt;/li&gt;
&lt;li&gt;Data Validation and missing values checks&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;environment-configuration&#34;&gt;Environment Configuration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Python 3.8.2 Python or newer.&lt;/li&gt;
&lt;li&gt;Use a venv (see developer install)&lt;/li&gt;
&lt;li&gt;MongoDB installed as regular program not as service&lt;/li&gt;
&lt;li&gt;AWS boto3 library&lt;/li&gt;
&lt;li&gt;Open API package installed&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Make sure that cloudmesh is properly installed on your machine and you have mongodb setup to work with cloudmesh.
More details can be found in the &lt;a href=&#34;https://cloudmesh.github.io/cloudmesh-manual/installation/install.html&#34;&gt;Cloudmesh Manual&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;openapi-package-installation&#34;&gt;OpenAPI package installation&lt;/h3&gt;
&lt;p&gt;Make sure you use a python venv before installing. Users can install the code with&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ pip install cloudmesh-openapi
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;pre-requisites-&#34;&gt;Pre Requisites :&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;add below parameter to cloudmesh.yaml for forecast service to work&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;bucket_name : awsforecastassignnment&lt;/li&gt;
&lt;li&gt;region_name : us-east-1&lt;/li&gt;
&lt;li&gt;forecast_srv : forecast&lt;/li&gt;
&lt;li&gt;forecastquery_srv : forecastquery&lt;/li&gt;
&lt;li&gt;s3_srv : s3&lt;/li&gt;
&lt;li&gt;iam_role_arn: XXXXXX&lt;/li&gt;
&lt;li&gt;algorithmArn: arn:aws:forecast:::algorithm/Deep_AR_Plus&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Data Format : The data should be in csv file format and must have&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;item_id : reference column for which time series forecast is required&lt;/li&gt;
&lt;li&gt;target_value : the column which need to be predicted, data type integer&lt;/li&gt;
&lt;li&gt;timestamp : timestamp of data samples&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.aws.amazon.com/forecast/latest/dg/API_CreateDataset.html&#34;&gt;AWS Time Series Forecast&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Json Schema : Json Schema file with name schema.json&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;quick-forecast-api-reference-commands&#34;&gt;Quick Forecast API reference Commands&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Start the open API server for the forecast service&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cms openapi server start .//forecast.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Check for supported AI services&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;curl http://localhost:8080/cloudmesh/forecast
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;e.g. output:
&amp;ldquo;model&amp;rdquo;: &amp;ldquo;Supported Time Series Forecast Services AWS : Forecast &amp;quot;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Upload file to the server from location (
File path should be the location on server where file is located.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;curl &lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#34;http://localhost:8080/cloudmesh/forecast/upload&amp;#34;&lt;/span&gt; -F &lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#34;upload=@&amp;lt;file_path&amp;gt;\countries-aggregated.csv&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;e.g. output:
countries-aggregated.csv uploaded successfully&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Validate data file&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;curl &lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#34;http://localhost:8080/cloudmesh/forecast/validate_data&amp;#34;&lt;/span&gt; -F &lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#34;upload=@&amp;lt;file_path&amp;gt;\countries-aggregated.csv&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;e.g. output:
countries-aggregated.csv validated successfully&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Split the data into test and train. Data should be validated first before splitting&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;curl http://localhost:8080/cloudmesh/forecast/split_data?split_pct&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;20&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;output: &amp;ldquo;Please validate the data first&amp;rdquo;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;curl http://localhost:8080/cloudmesh/forecast/split_data?split_pct&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;20&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;output: &amp;ldquo;Data split successfully&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Initialize aws parameters&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;curl &lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#34;http://localhost:8080/cloudmesh/forecast/aws&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;e.g. output:
{&amp;ldquo;model&amp;rdquo;:&amp;ldquo;AWS AI Service initialized successfully&amp;rdquo;}&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create Forecast, this is a multistep process, it cretes datasetgroup, dataset, import job, predictor and forecast&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;curl http://localhost:8080/cloudmesh/forecast/create_forecast?country&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;Austrailia
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This api expects cloud services to be already initialized if not it will request to initialize
output:&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Please initialize cloud service&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;output: &amp;ldquo;Forecast generated successfully&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lookup a Forecast&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;curl http://localhost:8080/cloudmesh/forecast/lookupForecast?countryName&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;Austrailia
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;output :
shows &lt;a href=&#34;https://github.com/cloudmesh-community/sp20-516-255/blob/main/Cloudmesh-OpenAPI/AWSForecast/sampleOutput&#34;&gt;ouput&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Delete Data Stack for the current project
This API should be executed at the end of the session to delete all the resources created for the analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;curl &lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#34;http://localhost:8080/cloudmesh/forecast/deletestack&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;algorithm-details&#34;&gt;Algorithm details&lt;/h2&gt;
&lt;p&gt;The AWS Forecast service supports following pre-defined algortithms&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Autoregressive Integrated Moving Average (ARIMA) Algorithm - arn:aws:forecast:::algorithm/ARIMA&lt;/li&gt;
&lt;li&gt;DeepAR+ Algorithm - arn:aws:forecast:::algorithm/Deep_AR_Plus&lt;/li&gt;
&lt;li&gt;Exponential Smoothing (ETS) - arn:aws:forecast:::algorithm/ETS&lt;/li&gt;
&lt;li&gt;Non-Parametric Time Series (NPTS) Algorithm - arn:aws:forecast:::algorithm/NPTS&lt;/li&gt;
&lt;li&gt;Prophet Algorithm - arn:aws:forecast:::algorithm/Prophet&lt;/li&gt;
&lt;li&gt;Supports hyperparameter optimization (HPO)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.aws.amazon.com/forecast/latest/dg/forecast.dg.pdf&#34;&gt;AWS Time Series Forecast&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;limitations&#34;&gt;Limitations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Requires data file with mandatory colums item_id, target_value and timestamp&lt;/li&gt;
&lt;li&gt;Requires a schema file schema.json to be provided by the user&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.aws.amazon.com/forecast/latest/dg/forecast.dg.pdf&#34;&gt;https://docs.aws.amazon.com/forecast/latest/dg/forecast.dg.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/aws-samples/amazon-forecast-samples&#34;&gt;https://github.com/aws-samples/amazon-forecast-samples&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-301/assignment6/assignment6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-301/assignment6/assignment6/</guid>
      <description>
        
        
        &lt;h1 id=&#34;assignment-6&#34;&gt;Assignment 6&lt;/h1&gt;
&lt;h1 id=&#34;health-and-medicine--artificial-intelligence-influence-on-ischemic-stroke-imaging&#34;&gt;Health and Medicine – Artificial Intelligence Influence on Ischemic Stroke Imaging&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Gavin Hemmerlein, fa20-523-301&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-301/blob/master/assignment6/assignment6.md&#34;&gt;Edit&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Keywords:&lt;/strong&gt; convolutional neural network, random forest learning, Computer Tomography Scan, CT Scan, stroke, artificial intelligence, deep learning, machine learning, large vessel occlusions&lt;/p&gt;
&lt;h2 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;The Computer Tomography Scan (CT Scan) is a medical procedure that involves multiple x-rays analyzed using computer aided techniques. The CT Scan’s creation was credited to Allan M. Cormack and Godfrey N. Hounsfield for which both individuals were awarded the 1979 Nobel Prize in Physiology or Medicine &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. The OECD estimates that there are a total of 42.64 million scanners located in the United States; the fourth most of any country &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. This prevalence is extremely important when discussing a diagnosis for stroke victims. In the 1980s, the identification techniques were generally done through a process called computer-aided diagnosis (CAD). “CAD usually relies on a combination of interpretation of medical images through computational algorithms and the physicians’ evaluation of the medical images &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;”.&lt;/p&gt;
&lt;p&gt;The goal of researchers and medical practitioners is to improve upon detection rates to ensure that more lives are saved by early detection. According to Johns Hopkins Medical Department the faster medical precautions can be given to a victim, the better the prognosis is for the individual &lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;. The brain requires a constant supply of blood and oxygen. When it is starved of these nutrients, the brain tissue begins to die.&lt;/p&gt;
&lt;h2 id=&#34;2-assisting-researchers-with-artificial-intelligence&#34;&gt;2. Assisting Researchers with Artificial Intelligence&lt;/h2&gt;
&lt;p&gt;According to an article in Radiology Business, automated detection of stroke anomalies is improving. As stated in a review in the article, “the team found convolutional neural networks beat random forest learning (RFL) on sensitivity, 85% to 68% &lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;.” This improvement is an excellent improvement by switching the algorithm that is used to train the model. A convolutional neural network (CNN) is a deep learning technique while a random forest is a modified decision tree. By modifying approaches from a decision tree to a deep learning technique, there is a very high likelihood that more lives could be saved. Strokes account for nearly 140,000 deaths a year and are one of the leading causes of permanent disability in the United States &lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;A RFL algorithm is a form of decision tree supervised learning. Decision trees are unique because they can also be used to solve regression and classification problems; which is unique to supervised learning methods. The RFL uses many decision trees that build upon one another. Where the CNN algorithm differs is that it is a form of deep learning that performs unsupervised learning. Each layer in the CNN understands its inputs and outputs while passing the output on to the next layer. A CNN can pass this information forward through a number of layers, but there is also a diminishing return given the amount of processing needed for each layer.&lt;/p&gt;
&lt;p&gt;After reviewing the literature from the Radiology Business article, the most common avenue for early detection appears to be the RFL as stated above. A meta analysis reviewing PubMed articles from January 2014 to February 2019 found that the RFL was the highest performer for predictive measures &lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;. For large vessel occlusions (LVO), the best approach was to use a CNN. CNN’s use little pre-processing and rely moreso on the filters with the data. This results in a more dynamic approach to the data as opposed to the harder developed structure of a decision tree.&lt;/p&gt;
&lt;h2 id=&#34;3-future-work&#34;&gt;3. Future Work&lt;/h2&gt;
&lt;p&gt;Upon examining the cited sources, there are some future areas to look research. To improve on current understanding, a standardization of metrics for to evaluate the fidelity of the models &lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;, continued development of automative image analysis software &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;, and leveraging emerging techniques to develop even more effective algorithms to detect large vessel occlusion &lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;. As of 2019, the advantage of CNN’s over conventional detection methods was only 7.6% &lt;sup id=&#34;fnref:9&#34;&gt;&lt;a href=&#34;#fn:9&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;. The percentage may seem marginal, but when expanded out to the 140,000 strokes per year the amount of strokes identified could be as much as 10,000 individuals.&lt;/p&gt;
&lt;p&gt;These areas are only a few of the many improvements that could be made in the world of stroke detection. It is not a far stretch to imagine detecting vessels that are becoming clogged or brittle. If detection of these medical issues could become prevalent, even more lives could be saved by predicting strokes before they even occur.&lt;/p&gt;
&lt;h2 id=&#34;4-references&#34;&gt;4. References&lt;/h2&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Nobel Prizes &amp;amp; Laureates, &amp;ldquo;The Nobel Prize in Physiology or Medicine 1979,&amp;rdquo; &lt;em&gt;The Nobel Prize,&lt;/em&gt; [Online]. Available:
&lt;a href=&#34;https://www.nobelprize.org/prizes/medicine/1979/summary/&#34;&gt;https://www.nobelprize.org/prizes/medicine/1979/summary/&lt;/a&gt; [Accessed Oct. 16, 2020]. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;OECD, &amp;ldquo;Computed tomography (CT) scanners,&amp;rdquo; &lt;em&gt;OECD Data,&lt;/em&gt; [Online]. Available:
&lt;a href=&#34;https://data.oecd.org/healtheqt/computed-tomography-ct-scanners.htm&#34;&gt;https://data.oecd.org/healtheqt/computed-tomography-ct-scanners.htm&lt;/a&gt; [Accessed Oct. 16, 2020]. &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Y. Mokli, J. Pfaff, D. Pinto dos Santos, C. Herweh, and S. Nagel &amp;ldquo;Computer-aided imaging analysis in acute ischemic stroke – background and clinical applications&amp;rdquo;, &lt;em&gt;Neurological Research and Practice&lt;/em&gt;, p. 1-13. 2020 [Online serial]. Available:  &lt;a href=&#34;https://neurolrespract.biomedcentral.com/track/pdf/10.1186/s42466-019-0028-y&#34;&gt;https://neurolrespract.biomedcentral.com/track/pdf/10.1186/s42466-019-0028-y&lt;/a&gt; [Accessed Oct. 13, 2020]. &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;A. Pruski, “Stroke Recovery Timeline,” &lt;em&gt;John Hopkins Medical,&lt;/em&gt; [Online]. Available: &lt;a href=&#34;https://www.hopkinsmedicine.org/health/conditions-and-diseases/stroke/stroke-recovery-timeline&#34;&gt;https://www.hopkinsmedicine.org/health/conditions-and-diseases/stroke/stroke-recovery-timeline&lt;/a&gt; [Accessed Oct. 16, 2020]. &lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;D. Pearson, &amp;ldquo;AI helps bust stroke, identify occlusions,&amp;rdquo; &lt;em&gt;Radiology Business,&lt;/em&gt; [Online]. Available:
&lt;a href=&#34;https://www.radiologybusiness.com/topics/artificial-intelligence/ai-helps-bust-stroke-identify-occlusions&#34;&gt;https://www.radiologybusiness.com/topics/artificial-intelligence/ai-helps-bust-stroke-identify-occlusions&lt;/a&gt; [Accessed Oct. 13, 2020]. &lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;The Internet Stroke Center, &amp;ldquo;About Strokes,&amp;rdquo; &lt;em&gt;Stroke Statistics,&lt;/em&gt; [Online]. Available:
&lt;a href=&#34;http://www.strokecenter.org/patients/about-stroke/stroke-statistics/#:~:text=More%20than%20140%2C000%20people%20die,and%20185%2C000%20are%20recurrent%20attacks&#34;&gt;http://www.strokecenter.org/patients/about-stroke/stroke-statistics/#:~:text=More%20than%20140%2C000%20people%20die,and%20185%2C000%20are%20recurrent%20attacks&lt;/a&gt; [Accessed Oct. 16, 2020]. &lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;N. Murray, &amp;ldquo;Artificial intelligence to diagnose ischemic stroke and identify large vessel occlusions: a systematic review,&amp;rdquo; &lt;em&gt;Journal of NeuroInterventional Surgery&lt;/em&gt;, vol. 12, no. 2, p. 156-164. 2020 [Online serial]. Available: &lt;a href=&#34;https://jnis.bmj.com/content/12/2/156&#34;&gt;https://jnis.bmj.com/content/12/2/156&lt;/a&gt; [Accessed Oct. 13, 2020]. &lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:8&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;M. Stib, J. Vasquez, M. Dong, Y. Kim, S. Subzwari, H. Triedman, A. Wang, H. Wang, A. Yao, M. Jayaraman, J. Boxerman, C. Eickhoff, U. Cetintemel, G. Baird, and R. McTaggart, &amp;ldquo;Detecting Large Vessel Occlusion at Multiphase CT Angiography by Using a Deep Convolutional Neural Network&amp;rdquo;, &lt;em&gt;Original Research Neuroradiology&lt;/em&gt;, Sep 29, 2020. [Online serial]. Available: &lt;a href=&#34;https://pubs.rsna.org/doi/full/10.1148/radiol.2020200334&#34;&gt;https://pubs.rsna.org/doi/full/10.1148/radiol.2020200334&lt;/a&gt; [Accessed Oct. 13, 2020]. &lt;a href=&#34;#fnref:8&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:9&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;J. Tuan, &amp;ldquo;How AI is able to Predict and Detect a Stroke&amp;rdquo;, &lt;em&gt;Referral MD&lt;/em&gt;. [Online]. Available: &lt;a href=&#34;https://getreferralmd.com/2019/10/how-ai-is-able-to-predict-and-detect-a-stroke/&#34;&gt;https://getreferralmd.com/2019/10/how-ai-is-able-to-predict-and-detect-a-stroke/&lt;/a&gt; [Accessed Oct. 13, 2020]. &lt;a href=&#34;#fnref:9&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-301/project/misc_files/blank/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-301/project/misc_files/blank/</guid>
      <description>
        
        
        &lt;h1 id=&#34;blank&#34;&gt;Blank&lt;/h1&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-301/project/plan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-301/project/plan/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;EE 534: BIG DATA&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CHELSEA GORIUS&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;GAVIN HEMMERLEIN&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CLASS 11530&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;FALL 2020&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NBA PERFORMANCE AND INJURY&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;1-team&#34;&gt;1. Team&lt;/h2&gt;
&lt;p&gt;Our team will consist of Chelsea Gorius (cgorius - fa20-523-344) and Gavin Hemmerlein (ghemmer - fa20-523-301).  Both members are students in the ENGR E534 course.  Chelsea and Gavin are also IU Masters students pursuing a degree in Data Science.&lt;/p&gt;
&lt;h2 id=&#34;2-topic&#34;&gt;2. Topic&lt;/h2&gt;
&lt;p&gt;The topic to be investigated is basketball player performance as it relates to injury. The topic of injury and recovery is a multi-billion dollar industry.  The Sports Medicine field is expected to reach $7.2 billion dollars by 2025 &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.  The scope of this effort is to explore National Basketball Association(NBA) teams, but the additional uses of a topic such as this could expand into other realms such as the National Football League, Major League Baseball, the Olympic Committees, and many other avenues.  For leagues with salaries, projecting an expected return on the investment can assist in contract negotiations and cater expectations.&lt;/p&gt;
&lt;h2 id=&#34;3-dataset&#34;&gt;3. Dataset&lt;/h2&gt;
&lt;p&gt;To compare performance and injury, a minimum of two datasets will be needed. The first is a dataset of injuries for players &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. This dataset will create the samples necessary for review.&lt;/p&gt;
&lt;p&gt;Once the controls for injuries are established, the next requirement will be to establish  pre-injury performance parameters and post-injury parameters.  These areas will be where the feature engineering will take place.  The datasets needed must dive into appropriate basketball performance stats to establish a metric to encompass a player’s performance. One example that ESPN has tried in the past is the Player Efficiency Rating (PER).  To accomplish this, it will be important to review player performance within games such as in the “NBA games data” &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; dataset.  There is a potential to pull more data from other datasets such as the “NBA Enhanced Box Score and Standings (2012 - 2018)” &lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;.  It is important to use the in depth data from the “NBA games data” &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; dataset because of how it will allow us to see how the player was performing throughout the season, and not just their average stats across the year.  With in depth information about each game of the season, and not just the teams and players aggregated stats, added to the data provided from the injury dataset &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; we will be able to compose new metrics to understand how these injuries are actually affecting the players performance.&lt;/p&gt;
&lt;p&gt;Along the way we look forward to discovering if there is also a causal relationship to the severity of some of the injuries, based on how the player was performing just before the injury.  The term “load management” has become popular in recent years to describe players taking rest periodically throughout the season in order to prevent injury from overplaying.  This new practice has received both support for the player safety it provides and also criticism around players taking too much time off.  Of course not all injuries are entirely based on the recent strain under the players body, but a better understanding about how that affects the injury as a whole could give better insight into avoiding more injuries.  It is important to remember though that any pattern identification would not lead to an elimination of all injuries, any contact sport will continue to have injuries, especially one as high impact as the NBA.  There is value to learn from why some players are able to return from certain injuries more quickly and why some return to almost equivalent or better playing performance than before the injury.  This comparison of performance will be made by deriving metrics based on varying ranges of games immediately leading up to injury and then immediately after returning from injury.  In addition to that we will perform comparisons to the players known peak performance to better understand how the injury affected them.  Another factor it will be important to include is the length of time recovering from the injury. Different players take differing amounts of time off, sometimes even with similar injuries.  Something will be said about the player’s dedication to recovery and determination to remain at peak performance, even through injury, when looking at how severe their injury was, how much time was taken for recovery, and how they performed upon returning.&lt;/p&gt;
&lt;p&gt;These datasets were chosen because they allow for a review of individual game performance, for each team, throughout each season in the recent decade.  Aggregate statistics such as points per game (ppg) can be deceptive because duration of the metric is such a large period of time.  The large sample of 82 games can lead to a perception issue when reviewing the data.  These datasets include more variables to help us determine effects to player injury, such as minutes per game (mpg) to understand how strenuous the pre-injury performance or how fatigue may have played a factor in the injury.  Understanding more of the variables such as fouls given or drawn can help determine if the player or other team seemed to be the primary aggressor before any injury.&lt;/p&gt;
&lt;h2 id=&#34;4-objective&#34;&gt;4. Objective&lt;/h2&gt;
&lt;p&gt;The objective of this project is to develop performance indicators for injured players returning to basketball in the NBA.  It is unreasonable to expect a player to return to the same level of play post injury immediately upon starting back up after recovery.  It often takes a player months if not years to return to the same level of play as pre-injury, especially considering the severity of the injuries.  In order to successfully analyse this information from the datasets, a predictive model will need to be created using a large set of the data to train.&lt;/p&gt;
&lt;p&gt;From this point, a test run will be used to gauge the validity and accuracy of the model compared to some of the data set aside.  The model created will be able to provide feature importance to give a better understanding of which specific features are the most crucial when it comes to determining how bad the effects of an injury may or may not be on player performance.  Feature engineering will be performed prior to training the model in order to improve the chances of higher accuracy from the predictions.  This model could be used to keep an eye out for how a player&amp;rsquo;s performance intensity and the engineered features could affect how long a player takes to recover from injury, if there are any warning signs prior to an injury, and even how well they perform when returning.&lt;/p&gt;
&lt;h2 id=&#34;sources&#34;&gt;Sources&lt;/h2&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;A. Mehra, &lt;em&gt;Sports Medicine Market worth $7.2 billion by 2025&lt;/em&gt;, Markets and Markets.
&lt;a href=&#34;https://www.marketsandmarkets.com/PressReleases/sports-medicine-devices.asp&#34;&gt;https://www.marketsandmarkets.com/PressReleases/sports-medicine-devices.asp&lt;/a&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;R. Hopkins, &lt;em&gt;NBA Injuries from 2010-2020&lt;/em&gt;, Kaggle. &lt;a href=&#34;https://www.kaggle.com/ghopkins/nba-injuries-2010-2018&#34;&gt;https://www.kaggle.com/ghopkins/nba-injuries-2010-2018&lt;/a&gt; &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;N. Lauga, &lt;em&gt;NBA games data&lt;/em&gt;, Kaggle.  &lt;a href=&#34;https://www.kaggle.com/nathanlauga/nba-games?select=games_details.csv&#34;&gt;https://www.kaggle.com/nathanlauga/nba-games?select=games_details.csv&lt;/a&gt; &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;P. Rossotti, &lt;em&gt;NBA Enhanced Box Score and Standings (2012 - 2018)&lt;/em&gt;, Kaggle. &lt;a href=&#34;https://www.kaggle.com/pablote/nba-enhanced-stats&#34;&gt;https://www.kaggle.com/pablote/nba-enhanced-stats&lt;/a&gt; &lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-301/project/project/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-301/project/project/</guid>
      <description>
        
        
        &lt;h1 id=&#34;nba-performance-and-injury&#34;&gt;NBA Performance and Injury&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-301/actions&#34;&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-301/workflows/Check%20Report/badge.svg&#34; alt=&#34;Check Report&#34;&gt;&lt;/a&gt;
&lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-301/actions&#34;&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-301/workflows/Status/badge.svg&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;
Status: in progress&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gavin Hemmerlein, fa20-523-301&lt;/li&gt;
&lt;li&gt;Chelsea Gorius, fa20-523-344&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-301/blob/main/project/project.md&#34;&gt;Edit&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;div class=&#34;pageinfo pageinfo-primary&#34;&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Sports Medicine will be a $7.2 billion dollar industry by 2025. The NBA has a vested interest in predicting performance of players as they return from injury. The authors evaluated datasets available to the public within the 2010 decade to build machine and deep learning models to expect results. The team utilized Gradient Based Regressor, Light GBM, and Keras Deep Learning models. The results showed that the coefficient of determination for the deep learning model was approximately 98.5%. The team recommends future work to predicting individual player performance utilizing the Keras model.&lt;/p&gt;
&lt;p&gt;Contents&lt;/p&gt;
&lt;div class=&#34;toc&#34;&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#1-introduction&#34;&gt;1. Introduction&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#2-background-research-and-previous-work&#34;&gt;2. Background Research and Previous Work&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#3-dataset&#34;&gt;3. Dataset&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#31-data-transformations-and-calculations&#34;&gt;3.1 Data Transformations and Calculations&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#4-methodology&#34;&gt;4. Methodology&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#41-development-of-models&#34;&gt;4.1 Development of Models&lt;/a&gt;
          &lt;ul&gt;
            &lt;li&gt;&lt;a href=&#34;#411-evaluation-metrics&#34;&gt;4.1.1 Evaluation Metrics&lt;/a&gt;&lt;/li&gt;
            &lt;li&gt;&lt;a href=&#34;#412-gradient-boost-regression&#34;&gt;4.1.2 Gradient Boost Regression&lt;/a&gt;&lt;/li&gt;
            &lt;li&gt;&lt;a href=&#34;#412-lightgbm-regression&#34;&gt;4.1.2 LightGBM Regression&lt;/a&gt;&lt;/li&gt;
            &lt;li&gt;&lt;a href=&#34;#413-keras-deep-learning-models&#34;&gt;4.1.3 Keras Deep Learning Models&lt;/a&gt;&lt;/li&gt;
          &lt;/ul&gt;
        &lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#5-inference&#34;&gt;5. Inference&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#6-conclusion&#34;&gt;6. Conclusion&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#61-limitations&#34;&gt;6.1 Limitations&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#7-acknowledgements&#34;&gt;7. Acknowledgements&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#71-work-breakdown&#34;&gt;7.1 Work Breakdown&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#8-references&#34;&gt;8. References&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Keywords:&lt;/strong&gt; basketball, NBA, injury, performance, salary, rehabilitation, artificial intelligence, convolutional neural network, lightGBM, deep learning, gradient based regressor.&lt;/p&gt;
&lt;h2 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;The topic to be investigated is basketball player performance as it relates to injury. The topic of injury and recovery is a multi-billion dollar industry.  The Sports Medicine field is expected to reach $7.2 billion dollars by 2025 &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.  The scope of this effort is to explore National Basketball Association(NBA) teams, but the additional uses of a topic such as this could expand into other realms such as the National Football League, Major League Baseball, the Olympic Committees, and many other avenues.  For leagues with salaries, projecting an expected return on the investment can assist in contract negotiations and cater expectations.  Competing at such a high level of intensity puts these players at a greater risk to injury than the average athlete because of the intense and constant strain on their bodies.  The overall valuation of the NBA in recent years is over 2 billion dollars, meaning each team is spending millions of dollars in the pursuit of a championship every season.  Injuries to players can cost teams not only wins but also significant profits.  Ticket sales alone for a single NBA finals game have reported greater than 10 million dollars in profit for the home team, if a team&amp;rsquo;s star player gets injured just before the playoffs and the team does not succeed, that is a lot of money lost.  These injuries can have an effect no matter the time of year, regular season ticket sales have been known to fluctuate with injuries from the team&amp;rsquo;s top performers.  Besides ticket sales these injuries can also influence viewership, TV or streaming, and potentially lead to a greater loss in profits.  With the health of the players and so much money at stake NBA team organizations as a whole do their best to take care of their players and keep them injury free.&lt;/p&gt;
&lt;h2 id=&#34;2-background-research-and-previous-work&#34;&gt;2. Background Research and Previous Work&lt;/h2&gt;
&lt;p&gt;The assumptions were made based on current literature as well. The injury return and limitations upon return of Anterior Cruciate Ligament (ACL) rupture (ACLR) are well documented and known. Interesting enough, forty percent of the players in the study occurred during the fourth quarter &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. This leads some credence to the idea that fatigue is a major factor in the occurrence of these injuries.&lt;/p&gt;
&lt;p&gt;The current literature also shows that a second or third injury can occur more frequently due to minor injuries. &lt;em&gt;&amp;ldquo;When an athlete is recovering from an injury or surgery, tissue is already compromised and thus requires far more attention despite the recovery of joint motion and strength. Moreover, injuries and surgical procedures can create detraining issues that increase the likelihood of further injury&amp;rdquo;&lt;/em&gt; &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h2 id=&#34;3-dataset&#34;&gt;3. Dataset&lt;/h2&gt;
&lt;p&gt;To compare performance and injury, a minimum of two datasets will be needed. The first is a dataset of injuries for players &lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;. This dataset created the samples necessary for review.&lt;/p&gt;
&lt;p&gt;Once the controls for injuries were established, the next requirement was to establish pre-injury performance parameters and post-injury parameters. These areas were where the feature engineering took place. The datasets needed had to include appropriate basketball performance stats to establish a metric to encompass a player&amp;rsquo;s performance. One example that ESPN has tried in the past is the Player Efficiency Rating (PER). To accomplish this, it was important to review player performance within games such as in the &lt;em&gt;NBA games data&lt;/em&gt; &lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt; dataset because of how it allowed the team to evaluate the player performance throughout the season, and not just the average stats across the year. In addition to that the data from the &lt;em&gt;NBA games data&lt;/em&gt; &lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt; dataset was valuable in order to compare the calculated performance metrics just before an injury or after recovery to the player&amp;rsquo;s overall performance that season or in seasons prior. That comparison provided a solid baseline to understand how injuries can effect a player&amp;rsquo;s performance. With in depth information about each game of the season, and not just the teams and players aggregated stats, added to the data provided from the injury dataset &lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; the team was be able to compose new metrics to understand how these injuries are actually affecting the players performance.&lt;/p&gt;
&lt;p&gt;Along the way attempted to discover if there is also a causal relationship to the severity of some of the injuries, based on how the player was performing just before the injury. The term &lt;em&gt;load management&lt;/em&gt; has become popular in recent years to describe players taking rest periodically throughout the season in order to prevent injury from overplaying. This new practice has received both support for the player safety it provides and also criticism around players taking too much time off. Of course not all injuries are entirely based on the recent strain under the players body, but a better understanding about how that affects the injury as a whole could give better insight into avoiding more injuries. It is important to remember though that any pattern identification would not lead to an elimination of all injuries, any contact sport will continue to have injuries, especially one as high impact as the NBA. There is value to learn from why some players are able to return from certain injuries more quickly and why some return to almost equivalent or better playing performance than before the injury. This comparison of performance was attempted by deriving metrics based on varying ranges of games immediately leading up to injury and then immediately after returning from injury. In addition to that performed comparisons to the players known peak performance to better understand how the injury affected them. Another factor that was important to include is the length of time recovering from the injury. Different players take differing amounts of time off, sometimes even with similar injuries. Something will be said about the player’s dedication to recovery and determination to remain at peak performance, even through injury, when looking at how severe their injury was, how much time was taken for recovery, and how they performed upon returning.&lt;/p&gt;
&lt;p&gt;These datasets were chosen because they allow for a review of individual game performance, for each team, throughout each season in the recent decade. Aggregate statistics such as points per game (ppg) can be deceptive because duration of the metric is such a large period of time. The large sample of 82 games can lead to a perception issue when reviewing the data. These datasets include more variables to help the team determine effects to player injury, such as minutes per game (mpg) to understand how strenuous the pre-injury performance or how fatigue may have played a factor in the injury. Understanding more of the variables such as fouls given or drawn can help determine if the player or other team seemed to be the primary aggressor before any injury.&lt;/p&gt;
&lt;h3 id=&#34;31-data-transformations-and-calculations&#34;&gt;3.1 Data Transformations and Calculations&lt;/h3&gt;
&lt;p&gt;Using the Kaggle package the datasets were downloaded direct from the website and unzipped to a directory accessible by the ‘project_dateEngineering.ipynb’ notebook. The 7 unzipped datasets are then loaded into the notebook as pandas data frames using the ‘.read_csv()’ function. The data engineering performed in the notebook includes removal of excess data and data type transformations across almost all the data frames loaded. This data transformation includes transforming the games details column ‘MIN’, meaning minutes played, from a timestamp format to a numerical format that could have calculations like summation or average performed on it. This was a crucial transformation since minutes played have a direct correlation to player fatigue, which can increase a player’s chance of injury.&lt;/p&gt;
&lt;p&gt;One of the more difficult tasks was transforming the Injury dataset into something that would provide more information through machine learning and analysis. The dataset is loaded as one data set where 2 columns ‘Relinquished’ and ‘Acquired’ defined if the row in questions was a player leaving the roster due to injury or returning from injury, respectively.  In this case for each for one of those two columns contained a players name and the other was blank. Besides that the data frame contained information like the date, notes, and the team name. In order to appropriately understand each injury as whole the data frame needs to be transformed into one where each row contains the player, the start date of the injury, and the end date of the injury. In order to do this first the original Injury dataset was separated into rows marking the start of an injury and those marking the end of an injury. Data frames from the &lt;em&gt;NBA games data&lt;/em&gt; &lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt; data set were used to join TeamID and PlayerID columns to the Injury datasets. An ‘iterrows():’ loop was then used on the data frame marking the start of an injury to specifically locate the corresponding row in the Injury End data frame with the same PlayerID and where the return date was the closest date after the injury date. As this new data frame was being transformed, it was noted that sometimes a Player would have multiple rows with the same Injury ending date but different injury start dates, this can happen if an injury worsens or the player did not play due to last minute decision. In order to solve this the table was grouped by the PlayerID and InjuryEnd Date while keeping the oldest Injury Start date, since the model will want to see the full length of the injury. From there it was simple to calculate the difference in days for each row between the Injury start and end dates. This data frame is called ‘df_Injury_length’ in the notebook and is much easier to use for improved understanding of NBA injuries than the original format of the Injury data set.&lt;/p&gt;
&lt;p&gt;Once created, the ‘df_Injury_length’ data frame was copied and built upon. Using ‘iterrows():’ loop again to filter down the games details data frame rows with the same PlayerId, over 60 calculated columns are created to produce the ‘df_Injury_stats’ data frame. The data frame includes performance statistics specifically from the game the player was injured and the game the player returned from that injury. In addition to this aggregate performance metrics were calculated based on the 5 games prior to the injury and the 5 games post returning from injury. At this time the season of when the injury occurred and when the player returned is also stored in the dataframe. This will allow comparisons between the ‘df_Injury_stats’ data frame and the ‘df_Season_stats’ data frame which contains the players average performance metrics for entire seasons.&lt;/p&gt;
&lt;p&gt;A few interesting figures were generated within the Exploratory Data Analysis (EDA) stage. &lt;strong&gt;Figure 1&lt;/strong&gt; gave a view of the load of the player returning from injury. The load to the player will show how recovered the player is upon completion of rehab. Many teams decide to slowly work a returning player in. Additionally, the amount of time for an injury can be seen on this graph. The longer the injury, the more unlikely the player will return to action.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/avg_min_played_post5.png&#34; alt=&#34;Average Minutes Played in First Five Games Upon Return over Injury Length in Days&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Average Minutes Played in First Five Games Upon Return over Injury Length in Days*&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 2&lt;/strong&gt; shows the frequency in which a player is injured. The idea behind this graph is to see a relationship between the time leading up to the injury. Interesting enough, there is no key indication of where injury is more likely to occur. It can be assumed that there is a rarity of players who see playing time greater than 30 minutes. The histogram only shows a near flat relationship; which was surprising.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/frequencies_by_average_minutes.png&#34; alt=&#34;Frequency of Injuries by Average Minutes Played in Prior Five Games&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 2:&lt;/strong&gt; Frequency of Injuries by Average Minutes Played in Prior Five Games*&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 3&lt;/strong&gt; shows the length of injury over number of injuries. By reviewing this data, it can be seen that most injuries occur fewer rather than more often. A player that is deemed injury prone will be a lot more likely to be cut from the team. This data makes sense.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/injury_length.png&#34; alt=&#34;Injury Length in Days over Number of Injuries&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 3:&lt;/strong&gt; Injury Length in Days over Number of Injuries&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 4&lt;/strong&gt; shows the injury length over average minutes played in the five games before injury. This graph attempts to show all of the previous games and the impacts to the players injury. The data looks evenly distributed, but the majority of plaers do not play close to 40 minutes per game. By looking at this data, it shows that minutes played does likely contribute to the injury severity.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/injury_length_over_avg_min.png&#34; alt=&#34;Injury Length in Days over Avg Minutes Played in Prior 5 Games&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 4:&lt;/strong&gt; Injury Length in Days over Avg Minutes Played in Prior 5 Games&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 5&lt;/strong&gt; shows that in general the number of games played does not have a significant relationship to the length of the injury.  There is a darker cluster between 500-1000 days injured that exists over the 40-82 games played, this could suggest that as more games are played there is likeliness for more severe injury.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/injurylength_gamesplayed.png&#34; alt=&#34;Injury Length in Days over Player Games Played that Season&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 5:&lt;/strong&gt; Injury Length in Days over Player Games Played that Season&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figures 6&lt;/strong&gt;, &lt;strong&gt;Figure 7&lt;/strong&gt;, and &lt;strong&gt;Figure 8&lt;/strong&gt; attempt to demonstrate if any relationship exists visually between a player&amp;rsquo;s injury length and their age, weight, or height.  For the most part &lt;strong&gt;Figure 6&lt;/strong&gt; shows most severe injuries occurring to younger players, which could make sense considering they can perform more difficult moves or have more stamina than older players.  Some severe injuries still exist among the older players, this also makes sense considering their bodies have been under stress for many years and are more prone to injury. It should be noted that there are more players in the league that fall into the younger age bucket than the older ages. It is difficult to identify any pattern on &lt;strong&gt;Figure 7&lt;/strong&gt;.  If anything the graph is somewhat normally shaped similar to the heights of players across the league. Suprisingly the injuries on &lt;strong&gt;Figure 8&lt;/strong&gt; are clustered a bit towards the left, being the lighter players.  This could be explained through the fact that the lighter players are often more athletic and perform more strenuous moves than heavier players.  It is also somewhat surprising since the argument that heavier players are putting more strain on their bodies could be used as a reason why heavier players would have worse injuries. One possible explanation could be the musculature adding more of the dense body mass could add protection to weakened joints. More investigation would be needed to identify an exact reason.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/injurylength_playerage.png&#34; alt=&#34;Injury Length in Days over Player Age that Season&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 6:&lt;/strong&gt; Injury Length in Days over Player Age that Season&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/injurylength_playerHeight.png&#34; alt=&#34;Injury Length in Days over Player Height in Inches&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 7:&lt;/strong&gt; Injury Length in Days over Player Height in Inches&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/injurylength_playerWeight.png&#34; alt=&#34;Injury Length in Days over Player Weight in Kilograms&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 8:&lt;/strong&gt; Injury Length in Days over Player Weight in Kilograms&lt;/p&gt;
&lt;p&gt;Finally, the team decided to use the z-score to normalize all of the data. By using the Z-score from the individual data in a column of df_Injury_stats, the team was able to limit variability of multiple metrics across the dataframe. A player&amp;rsquo;s blocks and steals should be a miniscule amount compared to minutes or points of some players. The same can be said of assists, technical fouls, or any other statistic in the course of an NBA game. The Z-score, by nature of the metric from the mean, allows for much less variability across the columns.&lt;/p&gt;
&lt;h2 id=&#34;4-methodology&#34;&gt;4. Methodology&lt;/h2&gt;
&lt;p&gt;The objective of this project was to develop performance indicators for injured players returning to basketball in the NBA. It is unreasonable to expect a player to return to the same level of play post injury immediately upon starting back up after recovery. It often takes a player months if not years to return to the same level of play as pre-injury, especially considering the severity of the injuries. In order to successfully analyze this information from the datasets, a predictive model will need to be created using a large set of the data to train.&lt;/p&gt;
&lt;p&gt;From this point, a test run was used to gauge the validity and accuracy of the model compared to some of the data set aside. The model created was able to provide feature importance to give a better understanding of which specific features are the most crucial when it comes to determining how bad the effects of an injury may or may not be on player performance. Feature engineering was performed prior to training the model in order to improve the chances of higher accuracy from the predictions. This model could be used to keep an eye out for how a player&amp;rsquo;s performance intensity and the engineered features could affect how long a player takes to recover from injury, if there are any warning signs prior to an injury, and even how well they perform when returning.&lt;/p&gt;
&lt;h3 id=&#34;41-development-of-models&#34;&gt;4.1 Development of Models&lt;/h3&gt;
&lt;p&gt;To help with review of the data, conditioned data was used to save resources on Google Colab. By conditioning the data and saving the files as a .CSV, the team was able to create a streamlined process. Additionally, the team found benefit by uploading these files to Google Drive to quickly import data near real time. After operating in this fashion for some time, the team was able to load the datasets into Github and utilize that feature. By loading the datasets up to Github, a url could be used to link the files directly to the files saved on Github without using a token like with Kaggle or Google Drive. The files saved were the following:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Table 1:&lt;/strong&gt; Datasets Imported&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;&lt;strong&gt;Dataframe&lt;/strong&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Title&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;1.&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;df_Injury_stats&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;2.&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;df_Injury_length&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;3.&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;df_Season_stats&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;4.&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;games&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;5.&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;df_Games_gamesDetails&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;6.&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;injuries_2010-2018&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;7.&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;players&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;8.&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;ranking&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;9.&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;teams&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Every time Google Colab loads data, it takes time and resources. The team was able to utilize the cross platform connectivity of the Google utilities. The team could then focus on building models as opposed to conditioning data every time the code was ran.&lt;/p&gt;
&lt;h4 id=&#34;411-evaluation-metrics&#34;&gt;4.1.1 Evaluation Metrics&lt;/h4&gt;
&lt;p&gt;The metrics chosen were designed to give results on  Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and the Explained Variance (EV) Score. MAE is a measure of errors between paired observations experiencing the same expression. RMSE is the standard deviation of the prediction errors for our dataset. EV is the relationship between the train data and the test data. By using these metrics, the team is capable of reviewing the data in a statistical manner.&lt;/p&gt;
&lt;h4 id=&#34;412-gradient-boost-regression&#34;&gt;4.1.2 Gradient Boost Regression&lt;/h4&gt;
&lt;p&gt;The initial model that was used was a Gradient Boosting Regressor (GBR) model. This model produced the results shown in Table 2. The GBR model builds in a stage-wise fashion; similarly to other boosting methods. GBR also generalizes the data and attempts to optimize the results utilizing a loss function. An example of the algorithm can be seen in &lt;strong&gt;Figure 5&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/gbr.png&#34; alt=&#34;Gradient Boosting Regressor&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 5:&lt;/strong&gt; Gradient Boosting Regressor &lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;The team saw a relationship given the data. &lt;strong&gt;Table 2&lt;/strong&gt; shows the results of that model. The results were promising given the speed and utility of a GBR model. The team reviewed the data multiple times after multiple stages of conditioning the data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Table 2:&lt;/strong&gt; GBR Results&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;&lt;strong&gt;Category&lt;/strong&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Value&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;MAE Mean&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;-10.787&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;MAE STD&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.687&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;RMSE Mean&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;-115.929&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;RMSE STD&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;96.64&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;EV Mean&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;EV STD&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;After running a GBR model, the decision was made to try multiple models to see what gives the best results. The team settled on LightGBM and a Deep Learning model utilizing Keras built on the TensorFlow platform. These results will be seen in &lt;em&gt;4.1.2&lt;/em&gt; and &lt;em&gt;4.1.3&lt;/em&gt;.&lt;/p&gt;
&lt;h4 id=&#34;412-lightgbm-regression&#34;&gt;4.1.2 LightGBM Regression&lt;/h4&gt;
&lt;p&gt;Another algorithm chosen was a Light Gradient Boost Machine (LightGBM) model. LightGBM is known for its lightweight and resource sparse abilities. The model is built from decision tree algorithms and used for ranking, classification, and other machine learning tasks. By choosing LightGBM data scientists are able to analyze larger data a faster approach. LightGBM can often over fit a model if the data is too small, but fortunately for the purpose of this assignment the data available for NBA injuries and stats is extremely large. Availability of data allowed for smooth operation of the LightGBM model. Mandot explains the model really well in The Medium. Mandot said, &lt;em&gt;&amp;ldquo;Light GBM can handle the large size of data and takes lower memory to run. Another reason of why Light GBM is popular is because it focuses on accuracy of results. LGBM also supports GPU learning and thus data scientists are widely using LGBM for data science application development&amp;rdquo;&lt;/em&gt; &lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;. There are a lot of benefits available to this algorithm.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/lightGBM_regressor.png&#34; alt=&#34;LightGBM Algorithm: Leafwise searching&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 6:&lt;/strong&gt; LightGBM Algorithm: Leafwise searching &lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;When running the model &lt;strong&gt;Table 3&lt;/strong&gt; was generated. This table uses the same metrics as the GBR Results Table (&lt;strong&gt;Table 2&lt;/strong&gt;). After reviewing the results, the GBR model still appeared to be a viable avenue. The Keras model will be evaluated next to see most optimal model to use for repeatable fresults.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Table 3:&lt;/strong&gt; LightGBM Results&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;&lt;strong&gt;Category&lt;/strong&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Value&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;MAE Mean&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;-0.011&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;MAE STD&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.001&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;RMSE Mean&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;-0.128&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;RMSE STD&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.046&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;EV Mean&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.982&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;EV STD&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.013&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;413-keras-deep-learning-models&#34;&gt;4.1.3 Keras Deep Learning Models&lt;/h4&gt;
&lt;p&gt;The final model attempted was a Deep Learning model. A few runs of different layers and epochs were chosen. They can be seen in &lt;strong&gt;Table 4&lt;/strong&gt; (shown later). The model was sequentially ran through the test layers to refine the model. When this is done, each predecessor layer acts as an input to the next layer&amp;rsquo;s input for the model. The results can produce accurate results while using unsupervised learning. The visualization for this model can be seen in the following figure:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/simple_neural_network_vs_deep_learning.jpg&#34; alt=&#34;Neural Network&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 7:&lt;/strong&gt; Neural Network &lt;sup id=&#34;fnref:9&#34;&gt;&lt;a href=&#34;#fn:9&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;When the team ran the Neural Networks, the data went through three layers. Each layer was built upon the previous similarly to the figure. This allowed for the team to capture information from the processing. &lt;strong&gt;Table 4&lt;/strong&gt; shows the results for the deep learning model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Table 4:&lt;/strong&gt; Epochs and Batch Sizes Chosen&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;&lt;strong&gt;Number&lt;/strong&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Regressor Epoch&lt;/strong&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Regressor Batch Sizes&lt;/strong&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;&lt;strong&gt;KFolds&lt;/strong&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Model Epochs&lt;/strong&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;&lt;strong&gt;R2&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;em&gt;1.&lt;/em&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;em&gt;25&lt;/em&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;em&gt;25&lt;/em&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;em&gt;10&lt;/em&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;em&gt;10&lt;/em&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;em&gt;0.985&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;2.&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;40&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;25&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;20&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.894&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;3.&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;20&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;25&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;20&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.966&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;4.&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;20&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;20&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.707&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;5.&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;25&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;25&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.611&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;6.&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;25&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;25&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;20&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.982&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The team has decided that the results for the Deep Learning are the most desirable. This model would be the one that the team would recommend based on the results from the metrics available. The parameters the team recommends are italicized in &lt;em&gt;Line 1&lt;/em&gt; of &lt;strong&gt;Table 4&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;5-inference&#34;&gt;5. Inference&lt;/h2&gt;
&lt;p&gt;With the data available, some conclusions can be made. Not all injuries are of the same severity. By treating an ACL tear in the same manner as a bruise, the team doctors would take terrible approaches to rehab. The severity of the injury is a part of the approach to therapy. This detail is nearly impossible to capture in the model.&lt;/p&gt;
&lt;p&gt;Another aspect to come to a conclusion is that not every player recovers in the same timetable as another. Genetics, diet, effort, and mental health can all harm or reinforce the efforts from the medical staff. These areas are hard to capture in the data and cannot be appropriately reviewed with this model.&lt;/p&gt;
&lt;p&gt;It is also difficult to indicate where a previous injury may have contributed to a current injury. The kinetic chain is a structure of the musculoskeletal system that moves the body using the muscles and bones. If one portion of the chain is compromised, the entire chain will need to be modified to continue movement. This modification can result in more injuries. The data cannot provide this information.  It is important to remember these possible confounding variables when interpreting the results of the model.&lt;/p&gt;
&lt;h2 id=&#34;6-conclusion&#34;&gt;6. Conclusion&lt;/h2&gt;
&lt;p&gt;After reviewing the results, the team created a robust model to predict the performance of a player after an injury. The coefficient of determination for the deep learning model shows a strong relationship between the training and test sets. After conditioning the data, the results can be seen in &lt;strong&gt;Table 2&lt;/strong&gt;, &lt;strong&gt;Table 3&lt;/strong&gt;, and &lt;strong&gt;Table 5&lt;/strong&gt;. The team had an objective to find this correlation and build it to the point where injury and performance can be modeled. The team was able to accomplish this goal.&lt;/p&gt;
&lt;p&gt;Additionally, these results are consistent with the current scientific literature &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;. The biological community has been able to record these results for decades. By leveraging this effort, the scientific community could move to a more proactive approach as opposed to reactive with respect to injury controls. This data will also allow for proper contract negotiations to take place in the NBA, considering potential decisions to avoid injury may include less playing time. The negotiations are pivotal to ensuring that expectations are met in the future seasons; especially when injury occurs in the final year of a player&amp;rsquo;s contract. Teams with an improved understanding of how players can or will return from injury have an opportunity to make the best of scenarios where other teams may be hesitant to sign an injured player.  These different opportunities for a team&amp;rsquo;s front office could be the difference between a championship ring and missing the playoffs entirely.&lt;/p&gt;
&lt;h2 id=&#34;61-limitations&#34;&gt;6.1 Limitations&lt;/h2&gt;
&lt;p&gt;With respect to the current work, the models could be continued to be refined. Currently the results are to the original intentions of the team, but improvements can be made. Feature Engineering is always an area where the models can improve. Some valuable features to be created in the future are the calculations for the player&amp;rsquo;s efficiency overall, as well as offensinve and defensive efficiencies in each game. The team would also like to develop a model to use the stats of a player in pre-injury and apply that to the post-injury set of metrics. Also, the team would like to move to where the same could be applied given the length of the injury to the player while considering the severity of the injury. Longer and more severe injury will lead to different future results than say a long not severe injury, or a short injury that was somewhat severe.  The number of varaibles that could provide more valuable information to the model are endless.&lt;/p&gt;
&lt;h2 id=&#34;7-acknowledgements&#34;&gt;7. Acknowledgements&lt;/h2&gt;
&lt;p&gt;The authors would like to thank Dr. Gregor Von Laszewski, Dr. Geoffrey Fox, and the associate instructors in the &lt;em&gt;FA20-BL-ENGR-E534-11530: Big Data Applications&lt;/em&gt; course (offered in the Fall 2020 semester at Indiana University, Bloomington) for their continued assistance and suggestions with regard to exploring this idea and also for their aid with preparing the various drafts of this article. In addition to that the community of students from the &lt;em&gt;FA20-BL-ENGR-E534-11530: Big Data Applications&lt;/em&gt; course also deserve a thanks from the author for the support, continued engagement, and valuable discussions through Piazza.&lt;/p&gt;
&lt;h3 id=&#34;71-work-breakdown&#34;&gt;7.1 Work Breakdown&lt;/h3&gt;
&lt;p&gt;For the effort developed, the team split tasks between each other to cover more ground. The requirements for the investigation required a more extensive effort for the teams in the ENGR-E 534 class. To accomplish the requirements, the task was expanded by addressing multiple datasets within the semester and building in multiple models to display the results. The team members were responsible for committing in Github multiple times throughout the semester. The tasks were divided as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Chelsea Gorius
&lt;ul&gt;
&lt;li&gt;Exploratory Data Analysis&lt;/li&gt;
&lt;li&gt;Feature Engineering&lt;/li&gt;
&lt;li&gt;Keras Deep Learning Model&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Gavin Hemmerlein
&lt;ul&gt;
&lt;li&gt;Organization of Items&lt;/li&gt;
&lt;li&gt;Model Development&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Both
&lt;ul&gt;
&lt;li&gt;Report&lt;/li&gt;
&lt;li&gt;All Outstanding Items&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;8-references&#34;&gt;8. References&lt;/h2&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;A. Mehra, &lt;em&gt;Sports Medicine Market worth $7.2 billion by 2025&lt;/em&gt;, [online] Markets and Markets.
&lt;a href=&#34;https://www.marketsandmarkets.com/PressReleases/sports-medicine-devices.asp&#34;&gt;https://www.marketsandmarkets.com/PressReleases/sports-medicine-devices.asp&lt;/a&gt; [Accessed Oct. 15, 2020]. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;J. Harris, B. Erickson, B. Bach Jr, G. Abrams, G. Cvetanovich, B. Forsythe, F. McCormick, A. Gupta, B. Cole,
&lt;em&gt;Return-to-Sport and Performance After Anterior Cruciate Ligament Reconstruction in National Basketball Association Players&lt;/em&gt;, Sports Health. 2013 Nov;5(6):562-8. doi: 10.1177/1941738113495788. [Online serial]. Available: &lt;a href=&#34;https://pubmed.ncbi.nlm.nih.gov/24427434&#34;&gt;https://pubmed.ncbi.nlm.nih.gov/24427434&lt;/a&gt; [Accessed Oct. 24, 2020]. &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;W. Kraemer, C. Denegar, and S. Flanagan, &lt;em&gt;Recovery From Injury in Sport: Considerations in the Transition From Medical Care to Performance Care&lt;/em&gt;, Sports Health. 
2009 Sep; 1(5): 392–395.[Online serial]. Available: &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3445177&#34;&gt;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3445177&lt;/a&gt;  [Accessed Oct. 24, 2020]. &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;R. Hopkins, &lt;em&gt;NBA Injuries from 2010-2020&lt;/em&gt;, [online] Kaggle. &lt;a href=&#34;https://www.kaggle.com/ghopkins/nba-injuries-2010-2018&#34;&gt;https://www.kaggle.com/ghopkins/nba-injuries-2010-2018&lt;/a&gt; [Accessed Oct. 9, 2020]. &lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;N. Lauga, &lt;em&gt;NBA games data&lt;/em&gt;, [online] Kaggle. &lt;a href=&#34;https://www.kaggle.com/nathanlauga/nba-games?select=games_details.csv&#34;&gt;https://www.kaggle.com/nathanlauga/nba-games?select=games_details.csv&lt;/a&gt; [Accessed Oct. 9, 2020]. &lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;J. Cirtautas, &lt;em&gt;NBA Players&lt;/em&gt;, [online] Kaggle. &lt;a href=&#34;https://www.kaggle.com/justinas/nba-players-data&#34;&gt;https://www.kaggle.com/justinas/nba-players-data&lt;/a&gt; [Accessed Oct. 9, 2020]. &lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;V. Aliyev, &lt;em&gt;A hands-on explanation of Gradient Boosting Regression&lt;/em&gt;, [online] Medium. &lt;a href=&#34;https://medium.com/@vagifaliyev/a-hands-on-explanation-of-gradient-boosting-regression-4cfe7cfdf9e&#34;&gt;https://medium.com/@vagifaliyev/a-hands-on-explanation-of-gradient-boosting-regression-4cfe7cfdf9e&lt;/a&gt; [Accessed Nov., 9 2020]. &lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:8&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;P. Mandon, &lt;em&gt;What is LightGBM, How to implement it? How to fine tune the parameters?&lt;/em&gt;, [online] Medium. &lt;a href=&#34;https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc&#34;&gt;https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc&lt;/a&gt; [Accessed Nov., 9 2020]. &lt;a href=&#34;#fnref:8&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:9&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;The Data Scientist, &lt;em&gt;What deep learning is and isn’t&lt;/em&gt;, [online] The Data Scientist.  &lt;a href=&#34;https://thedatascientist.com/what-deep-learning-is-and-isnt&#34;&gt;https://thedatascientist.com/what-deep-learning-is-and-isnt&lt;/a&gt; [Accessed Nov., 9 2020]. &lt;a href=&#34;#fnref:9&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-301/test/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-301/test/</guid>
      <description>
        
        
        &lt;h2 id=&#34;gavin-hemmerlein&#34;&gt;Gavin Hemmerlein&lt;/h2&gt;
&lt;h2 id=&#34;ghemmer&#34;&gt;ghemmer&lt;/h2&gt;
&lt;h2 id=&#34;engr-e-534&#34;&gt;ENGR-E 534&lt;/h2&gt;
&lt;p&gt;This is a test MarkDown file to ensure I have write privileges.&lt;/p&gt;
&lt;h1 id=&#34;test-typing&#34;&gt;Test Typing&lt;/h1&gt;
&lt;p&gt;This appears to be &lt;em&gt;working.&lt;/em&gt;&lt;/p&gt;
&lt;h1 id=&#34;table&#34;&gt;Table&lt;/h1&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Col1&lt;/th&gt;
&lt;th&gt;Col2&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Row 1&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Row 2&lt;/td&gt;
&lt;td&gt;21&lt;/td&gt;
&lt;td&gt;22&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id=&#34;images&#34;&gt;Images&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://assets.iu.edu/brand/3.2.x/trident-large.png&#34; alt=&#34;Image of IU Logo&#34;&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-302/assignment6/wearables_and_ai/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-302/assignment6/wearables_and_ai/</guid>
      <description>
        
        
        &lt;p&gt;Wearables and Personalized Medicine&lt;br&gt;
Adam Martin&lt;/p&gt;
&lt;p&gt;Wearables have been on the market for years now, gradually improving and providing increasingly insightful data on user health metrics.  Most wearables contain an array of sensors allowing the user to track aspects of their physical health.  This includes heart rate, motion, calories burned, and some devices now support ECG and BMI measurements.  This vast trove of data is valuable to consumers, as it allows for the measurement and gamification of key health metrics.  But can this data also be useful for health professionals in determining a patient’s activity levels and tracing important events in their health history?&lt;/p&gt;
&lt;p&gt;Many wearable devices, predominantly smartwatches, provide high-granularity data to the various apps that consume it.  The Apple Watch Core Motion API provides accelerometer, gyroscope, pedometer, magnetometer, altitude, and other measurements at a rate of 50hz.  This is in addition to the heart rate data that is sampled throughout the day.  Apple also provides a Movement Disorder Manager interface for the analysis of Parkinson’s disease symptoms.  FitBit and Pebble devices provide similar tracking capabilities.  Beyond existing consumer smartwatches, there is hope for smart tattoos, VR displays, footwear, and fabrics.  These wearables could measure a user’s electrolyte and metabolite levels in their perspiration.  They could measure abnormal gaits or detect bacteria (Yetisen, 2018).&lt;/p&gt;
&lt;p&gt;This high-fidelity data describing a wide variety of user activities could be invaluable to a healthcare professional hoping to find some insight in a patient’s condition.  However, the process for extraction, transformation, and transfer of this data is unclear.  With different device protocols and APIs providing information of varying quality and quantity, there is a need for a centralized, structured database for collection and analysis.  Along with this, there is a potential for the application of AI on the analysis of wearable data.  Raw sensor values will likely be incomprehensible to most analysts, so clustering of movement types and fuzzy logic on various parameters can allow a healthcare professional to better understand the meaning behind the data.  Furthermore, this data can be used to feed into a system of “predictive preventative diagnosis”.  Patients suffering from a variety of psychological or physical ailments can provide valuable data that highlights periods of symptom expression and also predicts prognosis (Piwek, 2016). When something is measured, it is easier to begin to act towards fixing it.&lt;/p&gt;
&lt;p&gt;The artificial intelligence algorithms employed in the processing of collected data can be as diverse and complex as the systems they attempt to understand.  Time series analysis for oscillating signals involving Fourier transforms.  Feature extraction analysis through PCA.  Noise reduction and motion clustering.  These applications ignore the extra layer of abstraction, which involves the diagnosis and prediction aspects of wearable data.  The field of wearables devices is growing, along with the promise of better digital representations, or ‘digital twins’, of patients.  While there are still are matters to consider, including patient well-being and data privacy, the prognosis of wearables changing the healthcare industry looks good.&lt;/p&gt;
&lt;p&gt; 
Works Cited&lt;br&gt;
Piwek, L. (2016). The Rise of Consumer Health Wearables: Promises and Barriers. PLOS MEDICINE.&lt;br&gt;
Yetisen, A. K. (2018). Wearables in Medicine. Wiley Online Library.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-302/project/plan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-302/project/plan/</guid>
      <description>
        
        
        &lt;p&gt;Wearables and Personalized Medicine
Adam Martin&lt;/p&gt;
&lt;p&gt;Wearables have been on the market for years now, gradually improving and providing increasingly insightful data on user health metrics. Most wearables contain an array of sensors allowing the user to track aspects of their physical health. This includes heart rate, motion, calories burned, and some devices now support ECG and BMI measurements. This vast trove of data is valuable to consumers, as it allows for the measurement and gamification of key health metrics. But can this data also be useful for health professionals in determining a patient’s activity levels and tracing important events in their health history?&lt;/p&gt;
&lt;p&gt;Many wearable devices, predominantly smartwatches, provide high-granularity data to the various apps that consume it. The Apple Watch Core Motion API provides accelerometer, gyroscope, pedometer, magnetometer, altitude, and other measurements at a rate of 50hz. This is in addition to the heart rate data that is sampled throughout the day. Apple also provides a Movement Disorder Manager interface for the analysis of Parkinson’s disease symptoms. FitBit and Pebble devices provide similar tracking capabilities. Beyond existing consumer smartwatches, there is hope for smart tattoos, VR displays, footwear, and fabrics. These wearables could measure a user’s electrolyte and metabolite levels in their perspiration. They could measure abnormal gaits or detect bacteria (Yetisen, 2018).&lt;/p&gt;
&lt;p&gt;This high-fidelity data describing a wide variety of user activities could be invaluable to a healthcare professional hoping to find some insight in a patient’s condition. However, the process for extraction, transformation, and transfer of this data is unclear. With different device protocols and APIs providing information of varying quality and quantity, there is a need for a centralized, structured database for collection and analysis. Along with this, there is a potential for the application of AI on the analysis of wearable data. Raw sensor values will likely be incomprehensible to most analysts, so clustering of movement types and fuzzy logic on various parameters can allow a healthcare professional to better understand the meaning behind the data. Furthermore, this data can be used to feed into a system of “predictive preventative diagnosis”. Patients suffering from a variety of psychological or physical ailments can provide valuable data that highlights periods of symptom expression and also predicts prognosis (Piwek, 2016). When something is measured, it is easier to begin to act towards fixing it.&lt;/p&gt;
&lt;p&gt;The artificial intelligence algorithms employed in the processing of collected data can be as diverse and complex as the systems they attempt to understand. Time series analysis for oscillating signals involving Fourier transforms. Feature extraction analysis through PCA. Noise reduction and motion clustering. These applications ignore the extra layer of abstraction, which involves the diagnosis and prediction aspects of wearable data. The field of wearables devices is growing, along with the promise of better digital representations, or ‘digital twins’, of patients. While there are still are matters to consider, including patient well-being and data privacy, the prognosis of wearables changing the healthcare industry looks good.&lt;/p&gt;
&lt;p&gt;  Works Cited
Piwek, L. (2016). The Rise of Consumer Health Wearables: Promises and Barriers. PLOS MEDICINE.
Yetisen, A. K. (2018). Wearables in Medicine. Wiley Online Library.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-302/project/project/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-302/project/project/</guid>
      <description>
        
        
        &lt;h1 id=&#34;review-of-the-use-of-wearables-in-personalized-medicine&#34;&gt;Review of the Use of Wearables in Personalized Medicine&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-302/actions&#34;&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-302/workflows/Check%20Report/badge.svg&#34; alt=&#34;Check Report&#34;&gt;&lt;/a&gt;
&lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-302/actions&#34;&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-302/workflows/Status/badge.svg&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;
Status: in progress&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Figure 1 does not provide useful information to the report. Please remove and use menaingful description of the data.&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Please do not use &amp;ldquo;Below is an&amp;rdquo; and use numbered figure references instead as provided in the template.&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Please provide for each type you classify an example so we can see the differences.&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; The analysis does not tell how accurate it is&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Why are not resting periods such as sleep, light sleep or napping included&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Is it possible to distinguish between sitting and eating and sitting and working?&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; The analysis does not include a benchmark&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Explain in more detail what you can do with the data after classification&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Adam Martin, &lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-302&#34;&gt;fa20-523-302&lt;/a&gt;, &lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-302/blob/main/project/project.md&#34;&gt;Edit&lt;/a&gt;&lt;/p&gt;


&lt;div class=&#34;pageinfo pageinfo-primary&#34;&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;todo&lt;/p&gt;
&lt;p&gt;Contents&lt;/p&gt;
&lt;div class=&#34;toc&#34;&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#1-introduction&#34;&gt;1. Introduction&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#2-background-research-and-previous-work&#34;&gt;2. Background Research and Previous Work&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#3-choice-of-data-sets&#34;&gt;3. Choice of Data-sets&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#4-methodology&#34;&gt;4. Methodology&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#5-discussion&#34;&gt;5. Discussion&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#6-conclusion&#34;&gt;6. Conclusion&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#7-acknowledgements&#34;&gt;7. Acknowledgements&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#8-references&#34;&gt;8. References&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Keywords:&lt;/strong&gt; Wearables, Classification, Descriptive Analysis, Healthcare, Movement Tracking, Precision Health&lt;/p&gt;
&lt;h2 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;Wearables have been on the market for years now, gradually improving and providing increasingly insightful data on user health metrics. Most wearables contain an array of sensors allowing the user to track aspects of their physical health. This includes heart rate, motion, calories burned, and some devices now support ECG and BMI measurements. This vast trove of data is valuable to consumers, as it allows for the measurement and gamification of key health metrics. But can this data also be useful for health professionals in determining a patient’s activity levels and tracing important events in their health history?&lt;/p&gt;
&lt;h2 id=&#34;2-background-research-and-previous-work&#34;&gt;2. Background Research and Previous Work&lt;/h2&gt;
&lt;p&gt;Previous work exists on the use of sensors and wearables in assisted living environments.  Consumer wearables are commonplace and have been used primarily for tracking individual activity metrics.  This research attempts to establish the efficacy of these devices in providing useful data for user activity, and how this information could be useful for healthcare workers.  This paper examines the roadblocks in making this information available to healthcare professionals and examines what wearable information is currently being used in healthcare.&lt;/p&gt;
&lt;p&gt;Existing research focuses on a wide variety of inputs &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;.  Sensors including electrodes, chemical probes, microphones, optical detectors, and blood glucose sensors are referenced as devices used for gathering healthcare information.  This research will focus on data that can be gathered with a modern smartphone or smartwatch.  Most of the sensors described are not as ubiquitous as consumer items like FitBits or Apple Watches.&lt;/p&gt;
&lt;p&gt;Previous studies have indicated the significance of precision health and the need for patient-specific data from wearables to be integrated into a patient&amp;rsquo;s care strategy &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.  Wearable data outlining a patient&amp;rsquo;s sleep, motion habits, heart rate, and other metrics can be invaluable in diagnosing or predicting conditions.  Increased sedentary activity could indicate depression, and could predict future heart problems.  A patient&amp;rsquo;s health could be graphed and historical trends could be useful to determine potential causes for a diagnosis.
It is often asserted that a person&amp;rsquo;s environmental factors are better predictors for their health than their genetic makeup &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.  Linking behavioral and social determinants with biomedical data would allow professionals to better target certain conditions.&lt;/p&gt;
&lt;h2 id=&#34;3-choice-of-data-sets&#34;&gt;3. Choice of Data-sets&lt;/h2&gt;
&lt;p&gt;The dataset used for this project contains labeled movement data from wearable devices.  The goal is to establish the potential for wearable devices to provide high-quality data to users and healthcare professionals.&lt;/p&gt;
&lt;p&gt;A dataset gathered from 24 individuals with Apple devices measuring attitude, gravity, acceleration, and rotation rate, will be used to determine user states.  The dataset is labeled with six states (walking downstairs, walking upstairs, sitting, standing, walking and jogging) and each gyroscopic sensor has several attributes describing its motion.&lt;/p&gt;
&lt;p&gt;If successful, this will establish that wearables have a high potential for providing relevant information beyond exercise metrics.&lt;/p&gt;
&lt;h2 id=&#34;4-methodology&#34;&gt;4. Methodology&lt;/h2&gt;
&lt;p&gt;The analysis of relevant wearable data is undertaken to determine the accuracy of activity information.  This analysis will consist of a brief descriptive analysis of the motion tracking data, and will proceed with attempts to classify the labeled data using various classification methods (K-nearest neighbors, random forest).&lt;/p&gt;
&lt;p&gt;First, the data has to be downloaded from the MotionSense project on GitHub.  A basic descriptive analysis will be performed, visualizing the sensor values for each movement class over time.&lt;/p&gt;
&lt;p&gt;A SciKit pipeline is set up for each classifier.  After obtaining a train/test split the pipeline applies a standard scaler to normalize the data, and then fits the classifier on the training data.  Each classifier is then scored using the testing dataset.&lt;/p&gt;
&lt;p&gt;If a classification strategy of sufficient accuracy is possible, it will be determined that wearable data can potentially serve as a useful supplementary source of information to aid in establishing a patient&amp;rsquo;s medical history.&lt;/p&gt;
&lt;p&gt;Reviewing relevant literature is important to determine the current state of wearables research regarding usefulness to healthcare workers and user well-being.  Much of this research will be focused on determining the state of wearables in the healthcare industry and determining if there is a need for streamlined data transfer to healthcare professionals.&lt;/p&gt;
&lt;h2 id=&#34;5-discussion&#34;&gt;5. Discussion&lt;/h2&gt;
&lt;p&gt;The dataset is comprised of six discrete classes of movement.  There are 12 parameters describing the readouts of the sensors over time.  The six movement classes are one-hot encoded.&lt;/p&gt;
&lt;p&gt;There is an imbalance in the number of datapoints for each class, which could lead to classification errors.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-302/main/project/images/occurence.png&#34; alt=&#34;Figure 2&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 2:&lt;/strong&gt; Data distribution per movement class.&lt;/p&gt;
&lt;p&gt;Figure 3 is an example of the data&amp;rsquo;s representation of a class of movement.  In this instance it&amp;rsquo;s that of a male jogging.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-302/main/project/images/timeseries_run.png&#34; alt=&#34;Figure 3&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 2:&lt;/strong&gt; 10 second sensor readout of a jogging male.&lt;/p&gt;
&lt;h2 id=&#34;6-conclusion&#34;&gt;6. Conclusion&lt;/h2&gt;
&lt;p&gt;todo&lt;/p&gt;
&lt;h2 id=&#34;7-acknowledgements&#34;&gt;7. Acknowledgements&lt;/h2&gt;
&lt;p&gt;todo&lt;/p&gt;
&lt;h2 id=&#34;8-references&#34;&gt;8. References&lt;/h2&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Yetisen, Ali K. (2018, August 16).  I Retrieved November 15, 2020 from &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6541866/&#34;&gt;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6541866/&lt;/a&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Piwek L, Ellis DA, Andrews S, Joinson A. (2016, February 02).  I Retrieved November 11, 2020 from &lt;a href=&#34;https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1001953&#34;&gt;https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1001953&lt;/a&gt; &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Glasgow, Russell E. Realizing the full potential of precision health: The need to include patient-reported health behavior, mental health, social determinants, and patient preferences data (2018, September 13). I Retrieved November 15, 2020 from &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6202010/&#34;&gt;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6202010/&lt;/a&gt; &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-304/project/project/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-304/project/project/</guid>
      <description>
        
        
        &lt;h1 id=&#34;using-big-data-to-eliminate-racial-bias-in-healthcare&#34;&gt;Using Big Data to Eliminate Racial Bias in Healthcare&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; does not follow template&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Robert Neubauer, &lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-304/&#34;&gt;fa20-523-304&lt;/a&gt;, &lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-304/blob/main/project/project.md&#34;&gt;Edit&lt;/a&gt;&lt;/p&gt;


&lt;div class=&#34;pageinfo pageinfo-primary&#34;&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Missing&lt;/p&gt;
&lt;p&gt;Contents&lt;/p&gt;
&lt;div class=&#34;toc&#34;&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#1-introduction&#34;&gt;1. Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Keywords:&lt;/strong&gt; Missing&lt;/p&gt;
&lt;h2 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;Missing&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-304/test/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-304/test/</guid>
      <description>
        
        
        &lt;h1 id=&#34;header&#34;&gt;Header&lt;/h1&gt;
&lt;h2 id=&#34;sub-header-with&#34;&gt;Sub Header with&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Bulleted&lt;/li&gt;
&lt;li&gt;lists&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sub-header-with-1&#34;&gt;Sub Header with&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Numbered&lt;/li&gt;
&lt;li&gt;Lists&lt;/li&gt;
&lt;/ol&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-305/homework3/cody_harris_hw3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-305/homework3/cody_harris_hw3/</guid>
      <description>
        
        
        &lt;h1 id=&#34;square-kilometer-array-ska-use-case&#34;&gt;Square Kilometer Array (SKA) Use Case&lt;/h1&gt;
&lt;p&gt;The SKA is an unprecedented, international, engineering endeavor to create the largest radio telescope in the world. Completion of this project requires the use of state-of-the-art technologies to facilitate the massive amount of data that will be captured [1]. Once this data is captured, it will require advanced high-performance computing centers to make sense of the data and gain valuable insight. While there are many innovative ideas involved with the SKA, this use case will only examine the technologies and processes involved with the solutions directly related to the SKA’s big data needs.&lt;/p&gt;
&lt;h1 id=&#34;what-is-a-radio-telescope&#34;&gt;What is a radio telescope?&lt;/h1&gt;
&lt;p&gt;Before understanding the data needs of the SKA, it is important to understand what a radio telescope is. Many people are familiar with a regular telescope that uses a series of lenses to amplify light waves from distant places to create an image. A radio telescope is similar in the fact that it collects weak electromagnetic radiation from far distances, and then amplifies it so that it can be analyzed. Another application could be to send radio waves towards a direction and then record the reflection off celestial bodies. In any case, the signal’s that astronomers are interested in are extremely weak. Many earthly sources of electro-magnetic radiation are many times greater in strength. There are multiple ways to combat this noise from earth-based radiation, and some of it could be done using hardware, or software, but there are also other ways to combat this that the SKA is utilizing.
Modern radio telescopes accept a wide range of radio frequencies, and then computationally split the frequencies into up to many thousands of channels. To further complicate things, while increasing the efficacy of the radio telescopes, generally more than one telescope is used. This allows multiple positions on the ground to receive the same radio signal, but at slightly different times and slightly different phases of the waveform. This variation allows for more complex analysis of the radio signal. Obviously, this adds another step in the computational work, but having a large array of radio telescopes is imperative to accomplish most modern astronomical research goals [2].&lt;/p&gt;
&lt;h1 id=&#34;science-goals&#34;&gt;Science Goals&lt;/h1&gt;
&lt;p&gt;The vast size of the SKA project allows the exploration of a variety of burning questions that not only intrigue astrophysicists, but nearly everyone on the planet. One overreaching design goal of the SKA is to have a design flexible enough that it can be used as a “discovery machine” for the “exploration of the unknown”. With that said, there are five broad research goals of the SKA [3].&lt;/p&gt;
&lt;h2 id=&#34;galaxy-evolution-and-dark-energy&#34;&gt;Galaxy Evolution and Dark Energy&lt;/h2&gt;
&lt;p&gt;As a central goal of the SKA, this is quite a broad question that requires a great deal of study to fully understand. With the data gathered, researchers how to understand fundamental questions about how galaxies change over the course of their lifetimes. One problem with studying this, is that most galaxies nearest to us are so far along in their evolution that it is hard to know what happens in the early years of the galaxy. We can overcome this challenge with SKA, due to its “sensitivity and resolution”. The SKA will be able to focus on younger galaxies that are much earlier in their evolution to study what our galaxy was like shortly after the big bang.
To gain an understanding of the creation and evolution of galaxies, a study of dark energy must be done. While this mysterious energy has made headlines in the past decade, it is still the subject of a lot of speculation. As gravity is a main driving factor in the evolution of cosmic objects, understanding dark energy is needed to gain a full picture of what is happening in galactical evolution. Currently our fundamental physical theories, derived by Einstein, suggest that universal expansion should be slowing, but it is not. This is where dark energy plays a part in the formation of our universe [4].&lt;/p&gt;
&lt;h2 id=&#34;was-einsteins-theory-of-relativity-correct&#34;&gt;Was Einstein’s theory of relativity, correct?&lt;/h2&gt;
&lt;p&gt;It is a tall order to question the most influential physicist in history. Technology is catching up with our theoretical understanding of physics so that we can test fundamental theories that we have held true for many years. The SKA hopes to use its incredible sensitivity to investigate gravitational waves from extremely powerful sources of gravity such as black holes. While Einstein’s theories are very likely to be mostly true, they might not be fully complete and that is what SKA hopes to find out [1].&lt;/p&gt;
&lt;h2 id=&#34;what-are-the-sources-of-large-magnetic-fields-in-space&#34;&gt;What are the sources of large magnetic fields in space?&lt;/h2&gt;
&lt;p&gt;We know that our earth creates a magnetic field that is imperative for life to exist. For the most part we understand that this is due to the composition and actions of the core of the planet. When it comes to the origin of magnetic fields in space, we are not completely sure what creates all the fields. The study of these magnetic fields will allow further study of the evolution of galaxies and our universe [5].&lt;/p&gt;
&lt;h2 id=&#34;what-are-the-origins-of-our-universe&#34;&gt;What are the origins of our universe?&lt;/h2&gt;
&lt;p&gt;This is a burning question that we have some theories about, but still have a great deal of exploration to do on the topic. The prevailing theory relies on the big bang, but the SKA hopes to further study the eras shortly after the big bang to gain insight into the origins of our universe. The SKA hopes to do this by once again using its sensitivity to give the most accurate measurements of the initial light sources in our universe [6]. As long this question remains unsolved, humans will always want to understand where we all came from.&lt;/p&gt;
&lt;h2 id=&#34;as-living-beings-are-we-alone-in-the-universe&#34;&gt;As living beings, are we alone in the universe?&lt;/h2&gt;
&lt;p&gt;Using Drake’s equation, and new exoplanet information, scientists are extremely optimistic that life exists somewhere in our universe. In some estimates, what has happened on our planet, could have happened about “10 billion other times over in cosmic history!” [7].  One way that SKA can look for extraterrestrial life is by searching for radio signals sent out by advanced civilizations such as ours. Another way that SKA could look for extraterrestrial life is by looking for signs of the building blocks of life. One of these building blocks are amino acids, which can be identified by the SKA.&lt;/p&gt;
&lt;h1 id=&#34;current-progress&#34;&gt;Current Progress&lt;/h1&gt;
&lt;p&gt;The SKA telescopes reside in two separate locations. One location is in Western Australia and will be focused on low frequencies. The second location is in South Africa and will have two arrays, one for mid frequencies, and one for mid to high frequency [8].&lt;/p&gt;
&lt;h2 id=&#34;south-africa&#34;&gt;South Africa&lt;/h2&gt;
&lt;p&gt;Design and preparations for the final SKA implementation are still on-going. Currently there are two arrays named KAT7 and MeerKAT that are installed and functioning and will be the precursor to the SKA arrays in South Africa.&lt;/p&gt;
&lt;h2 id=&#34;australia&#34;&gt;Australia&lt;/h2&gt;
&lt;p&gt;This site also has a precursor to SKA already operating named ASKAP. It is currently located in the same location that the SKA’s major components will eventually occupy, so this will give insights into the performance of this location for radio telescopes. Also, in Australia, as recent as in the past year, prototype antennas are being setup in smaller arrays to capture data and run tests before the design is used in the final array [10].&lt;/p&gt;
&lt;h1 id=&#34;big-data-challenges-and-solutions&#34;&gt;Big Data Challenges and Solutions&lt;/h1&gt;
&lt;p&gt;The SKA presents many big data challenges, from preprocessing to long-term storage of data. The estimated output of all the telescopes is around 700 PB per year [12].&lt;/p&gt;
&lt;h2 id=&#34;raw-data-and-preprocessing&#34;&gt;Raw Data and Preprocessing&lt;/h2&gt;
&lt;p&gt;The data comes in the form of an analog radio signals that are collected over a vast geographical area. At some point, to do analytics on the data, the data needs to be converted from analog to digital. While this is usually done via hardware, and is not on computational machines, this is still a data processing step that must be done at scale.
There is also some preprocessing of the data, that must happen constantly as data is collected. While this could be done once reaching the supercomputer, it is a repetitive task that could be done using FPGAs. The benefit of using a FGPA is that it can parallel process in many more threads and do repetitive algorithms faster and with less power as normal CPUs [12].&lt;/p&gt;
&lt;h2 id=&#34;storage-and-access&#34;&gt;Storage and Access&lt;/h2&gt;
&lt;p&gt;As mentioned previously, the estimated data output of the telescope at peak is 700 PB. The initiative also hopes to save all data for the lifetime of the project which is around 50 years. This ends up being in the realm of needing to eventually store 35 EB of data. For more immediate storage, the SKA team plans to use a buffer system. The way this works is by having a large array of fast read and write storage devices such as SSDs and NVMe (a specialized SSD). This buffer will immediately take in the data as it is coming in at rates that require write speeds that are not as prevalent with traditional spinning disks. After being written to this buffer, they will slowly move the data onto more affordable solutions, that have slower read/write speeds.
While the team could use SSDs for the entire storage, the cost would be enormous. It is much more cost effective to have most of the data stored on hard disk. When it comes to long-term storage of data, even cheaper sources of data such as tape drives could be utilized. After a certain time from data collection, the data will be opened up to the public, this means that the data will likely not end up in a cold storage system [12].&lt;/p&gt;
&lt;h2 id=&#34;processing-of-data&#34;&gt;Processing of data&lt;/h2&gt;
&lt;p&gt;Currently, the processing of data will be done at a large network of sites that will be made up of a variety of technologies. Mostly, no new high-performance computing centers will be created. Existing infrastructures, including public clouds will be used for the processing of data. Along with using FPGAs for pre-processing and possibly more processing afterwards, the SKA team plans to use GPU accelerators to allow for efficient processing.
Each team of researchers will have various goals that they will want from the data. This means that they will have a variety of processing needs, which will be carried out in SKA Regional Centers (SRCs). This might mean machine learning programs to get insights from the data, all the way to other mathematical operations to make the data ready for study. In any case, it is the expectation that this additional data is preserved as well, leading to even more data needing to be managed [12].&lt;/p&gt;
&lt;h2 id=&#34;other-challenges&#34;&gt;Other Challenges&lt;/h2&gt;
&lt;p&gt;While this data is not the most sensitive data on the planet, it is important that security is considered. The SKA team is planning on creating a sort of firewall between users and the actual HPC centers by using an AAAI (authorization, access, authentication, and identification) system. Security of proprietary data will be a concern that will have to be addressed. As there is a large team working on the project, as well as many external actors, security becomes extremely complex, especially the more access points there are to the data [12].
A project this large and versatile requires the use of many software tools. These software tools generally need some level or automatic communication if they are used together in a project. With a large number of tools, there becomes a complex IT infrastructure that needs to be managed, and constantly monitored. It is possible for one tool to receive a critical update, and then cause issues with integration of other software systems.&lt;/p&gt;
&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;
&lt;p&gt;[1] &amp;ldquo;Square Kilometre Array - ICRAR&amp;rdquo;, ICRAR, 2020. [Online]. Available: &lt;a href=&#34;https://www.icrar.org/our-research/ska/&#34;&gt;https://www.icrar.org/our-research/ska/&lt;/a&gt;. [Accessed: 23- Sep- 2020].&lt;br&gt;
[2] &amp;ldquo;What are Radio Telescopes? - National Radio Astronomy Observatory&amp;rdquo;, National Radio Astronomy Observatory, 2020. [Online]. Available:                                              &lt;a href=&#34;https://public.nrao.edu/telescopes/radio-telescopes/&#34;&gt;https://public.nrao.edu/telescopes/radio-telescopes/&lt;/a&gt;. [Accessed: 23- Sep- 2020].&lt;br&gt;
[3] &amp;ldquo;SKA Science - Public Website&amp;rdquo;, SQUARE KILOMETRE ARRAY, 2020. [Online]. Available: &lt;a href=&#34;https://www.skatelescope.org/science/&#34;&gt;https://www.skatelescope.org/science/&lt;/a&gt;. [Accessed: 24-      Sep- 2020].&lt;br&gt;
[4] &amp;ldquo;Galaxy Evolution, Cosmology and Dark Energy - Public Website&amp;rdquo;, SQUARE KILOMETRE ARRAY, 2020. [Online]. Available:      &lt;a href=&#34;https://www.skatelescope.org/galaxyevolution/&#34;&gt;https://www.skatelescope.org/galaxyevolution/&lt;/a&gt;. [Accessed:      24- Sep- 2020].&lt;br&gt;
[5] &amp;ldquo;Cosmic Magnetism - Public Website&amp;rdquo;, SQUARE KILOMETRE ARRAY, 2020. [Online]. Available: &lt;a href=&#34;https://www.skatelescope.org/magnetism/&#34;&gt;https://www.skatelescope.org/magnetism/&lt;/a&gt;. [Accessed: 24- Sep- 2020].&lt;br&gt;
[6] &amp;ldquo;Probing the Cosmic Dawn - Public Website&amp;rdquo;, SQUARE KILOMETRE ARRAY, 2020. [Online]. Available: &lt;a href=&#34;https://www.skatelescope.org/cosmicdawn/&#34;&gt;https://www.skatelescope.org/cosmicdawn/&lt;/a&gt;. [Accessed: 24- Sep- 2020].&lt;br&gt;
[7] L. Sierra, &amp;ldquo;Are we alone in the universe? Revisiting the Drake equation&amp;rdquo;, Exoplanet Exploration: Planets Beyond our Solar System, 2020. [Online]. Available: &lt;a href=&#34;https://exoplanets.nasa.gov/news/1350/are-we-alone-in-the-universe-revisiting-the-drake-equation/&#34;&gt;https://exoplanets.nasa.gov/news/1350/are-we-alone-in-the-universe-revisiting-the-drake-equation/&lt;/a&gt;. [Accessed: 24- Sep- 2020].&lt;br&gt;
[8] &amp;ldquo;Design - ICRAR&amp;rdquo;, ICRAR, 2020. [Online]. Available: &lt;a href=&#34;https://www.icrar.org/our-research/ska/design/&#34;&gt;https://www.icrar.org/our-research/ska/design/&lt;/a&gt;. [Accessed: 24- Sep- 2020].&lt;br&gt;
[9] &amp;ldquo;Africa - Public Website&amp;rdquo;, SQUARE KILOMETRE ARRAY, 2020. [Online]. Available: &lt;a href=&#34;https://www.skatelescope.org/africa/&#34;&gt;https://www.skatelescope.org/africa/&lt;/a&gt;. [Accessed: 24- Sep- 2020].&lt;br&gt;
[10] Square Kilometre Array, Building a giant telescope in the outback - part 2. 2020.&lt;br&gt;
[11] &amp;ldquo;Australia - Public Website&amp;rdquo;, SQUARE KILOMETRE ARRAY, 2020. [Online]. Available: &lt;a href=&#34;https://www.skatelescope.org/australia/&#34;&gt;https://www.skatelescope.org/australia/&lt;/a&gt;. [Accessed: 24- Sep- 2020].&lt;br&gt;
[12] Filled in Use Case Survey for SKA&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-305/homework6/cody_harris_hw6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-305/homework6/cody_harris_hw6/</guid>
      <description>
        
        
        &lt;h1 id=&#34;applying-computer-vision-to-medical-imaging&#34;&gt;Applying Computer Vision to Medical Imaging&lt;/h1&gt;
&lt;p&gt;Computer vision technology has made great strides in the past decade. The most obvious proof of this statement comes from looking at early consumer image classification programs. Early programs from the early 2010s struggled to find the difference between a cat and a person. Now, consumer image classification programs can accurately tell the difference between two cats. With these improvements, there are great applications for computer vision to aid with radiology.&lt;/p&gt;
&lt;h2 id=&#34;specific-application-areas&#34;&gt;Specific Application Areas&lt;/h2&gt;
&lt;p&gt;The five most common modalities of medical imaging are: X-Rays, CT scans, MRI, ultrasound, and PET scan [1]. Within these major modalities, there are various special types of each imaging technique such as an fMRI, which is called a functional MRI. Then there are the more niche imaging techniques that are used. For example, Diffusion Tensor Imaging (DTI) is a technique that allows visualization of the white matter in the brain. One application of this imaging technique is coming up with a way to diagnose certain mental illness from imaging [2]. Seeing as mental health issues are typically tougher to diagnose, this would be a major breakthrough.&lt;/p&gt;
&lt;p&gt;Oncology seems to be a major area of study for computer vision in medical imaging. Logically this makes sense as cancers seems to create anomalies that can be seen in medical imaging. Thoracic imaging focuses on looking at the lungs, and computer vision could aid with finding anomalies that could lead to the early detection of cancer which in turn creates a better prognosis. An application that is only based on analyzing normal images or video, is analyzing a colonoscopy. Certain structures in the colon can create colorectal cancer if not correctly identified and classified as benign or malignant. Another imaging technique that can be rather difficult to analyze correctly are mammograms, and correctly identifying the various anomalies that are present as either malignant or benign [3].&lt;/p&gt;
&lt;h2 id=&#34;how-computer-vision-can-be-used-in-medical-imaging&#34;&gt;How Computer Vision can be used in Medical Imaging&lt;/h2&gt;
&lt;p&gt;In a perfect world, a sufficiently advanced AI could be the only entity to ever examine a certain medical image before providing a prognosis. In reality our technology is far from achieving this lofty goal. In the meantime, AI can still be used to improve radiologist workflows. In some studies, it was found that a radiologist would have to look at one image every three to four seconds to stay caught up with their workload in an 8-hour day. It is obvious why this could cause issues with accuracy. Now think about having the same time to look at an image, but instead of a raw image, the image comes with suggestions of diagnosis, and points to specific areas for the radiologist to focus on. This would improve the effectiveness of radiologists without relying completely on the AI model to be 100% accurate [3].&lt;/p&gt;
&lt;h2 id=&#34;modeling-techniques&#34;&gt;Modeling Techniques&lt;/h2&gt;
&lt;p&gt;There are two main techniques that are currently being employed to work with computer vision and medical imaging. The first technique is extra certain features from the image based on qualifications that are input to the system. For example, the user of the system might put in to extract the texture and shape of anomalies in the lower left lobe of the lungs as one of the features. Once all of these features are collected, they are fed into an expert system that selects the most promising features that could help with diagnosis. These selected features are fed into a machine learning classifier system that then sends its insights along with the image to the radiologist [3]. This system has it’s draw backs that are typical of expert systems. First off, setting the system up and giving it the parameters for the expert system is extremely complicated, and incorrect parameters in the system could heavily affect the output.&lt;/p&gt;
&lt;p&gt;The second technique employs deep learning. Over the years, deep learning has become a widely used method to gain insights from data, and computer vision is no exception. The deep learning models have the benefit of not requiring any setup or expert systems. Really the biggest challenge is getting a good enough training data such that the deep learning model accurately predicts in the same way that a radiologist would. Some studies have been done on testing the accuracy of such methods and they found that “deep learning technologies are on par with radiologists’ performance for both detection and segmentation tasks in ultrasonography and MRI, respectively” [3].&lt;/p&gt;
&lt;h2 id=&#34;special-considerations&#34;&gt;Special Considerations&lt;/h2&gt;
&lt;p&gt;While deep learning seems to be the best method to create these systems, experts still need to be involved with the creation of these systems. One example of this is having expert radiologists evaluate training data. Just because there might be 30 years worth of data, that doesn’t mean it all can be used. The medical field is constantly evolving and making sure that the data you train your model is relevant is an important part of creating any model. Also using radiologists to shape the software that is used by radiologists would always improve the end product [4]. Too often software is built by software engineers and data scientists and doesn’t use enough advice from experts in the field, and this almost always is a detriment to the software.&lt;/p&gt;
&lt;p&gt;Radiologists using these deep learning tools, will require a great deal of training with these tools. They know that the AI model is not always going to be correct, and it is important that the radiologists understand how the software works so that they can make a determination of whether their opinion should be trusted over the output from the software, especially early on with newer technology [4].&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] &amp;ldquo;Different Imaging Tests Explained | UVA Radiology&amp;rdquo;, UVA Radiology and Medical Imaging Blog for Patients, 2019. [Online]. Available: &lt;a href=&#34;https://blog.radiology.virginia.edu/different-imaging-tests-explained/&#34;&gt;https://blog.radiology.virginia.edu/different-imaging-tests-explained/&lt;/a&gt;. [Accessed: 20- Oct- 2020].&lt;/p&gt;
&lt;p&gt;[2] &amp;ldquo;Diffusion Tensor Imaging (DTI) | Psychiatry Neuroimaging Laboratory&amp;rdquo;, Pnl.bwh.harvard.edu, 2020. [Online]. Available: &lt;a href=&#34;http://pnl.bwh.harvard.edu/portfolio-item/diffusion-tensor-imaging-dti/&#34;&gt;http://pnl.bwh.harvard.edu/portfolio-item/diffusion-tensor-imaging-dti/&lt;/a&gt;. [Accessed: 20- Oct- 2020].&lt;/p&gt;
&lt;p&gt;[3] A. Hosny, C. Parmar, J. Quackenbush, L. Schwartz and H. Aerts, &amp;ldquo;Artificial intelligence in radiology&amp;rdquo;, Nature Reviews Cancer, vol. 18, no. 8, pp. 500-510, 2018. Available: 10.1038/s41568-018-0016-5 [Accessed 20 October 2020].&lt;/p&gt;
&lt;p&gt;[4] &amp;ldquo;AI and the Future of Radiology&amp;rdquo;, Diagnostic Imaging, 2020. [Online]. Available: &lt;a href=&#34;https://www.diagnosticimaging.com/view/ai-and-future-radiology&#34;&gt;https://www.diagnosticimaging.com/view/ai-and-future-radiology&lt;/a&gt;. [Accessed: 20- Oct- 2020].&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-305/project/project/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-305/project/project/</guid>
      <description>
        
        
        &lt;h1 id=&#34;estimating-soil-moisture-content-using-weather-data&#34;&gt;Estimating Soil Moisture Content Using Weather Data&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-305/actions&#34;&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-305/workflows/Check%20Report/badge.svg&#34; alt=&#34;Check Report&#34;&gt;&lt;/a&gt;
&lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-305/actions&#34;&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-305/workflows/Status/badge.svg&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;
Status: final&lt;/p&gt;
&lt;p&gt;Cody Harris, &lt;a href=&#34;mailto:harrcody@iu.edu&#34;&gt;harrcody@iu.edu&lt;/a&gt;, &lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-305&#34;&gt;fa20-523-305&lt;/a&gt;, &lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-305/edit/main/project/project.md&#34;&gt;Edit&lt;/a&gt;&lt;/p&gt;


&lt;div class=&#34;pageinfo pageinfo-primary&#34;&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;As the world is gripped with finding solutions to problems such as food and water shortages, the study of agriculture could improve where we stand with both of these problems. By integrating weather and sensor data, a model could be created to estimate soil moisture based on weather data that is easily accessible. While some farmers could afford to have many moisture sensors and monitor them, many would not have the funds or resources to keep track of the soil moisture long term. A solution would be to allow farmers to contract out a limited study of their land using sensors and then this model would be able to predict soil moistures from weather data. This collection of data, and predictions could be used on their own or as a part of a larger agricultural solution.&lt;/p&gt;
&lt;p&gt;Contents&lt;/p&gt;
&lt;div class=&#34;toc&#34;&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#1-introduction&#34;&gt;1. Introduction&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#2-background&#34;&gt;2. Background&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#3-datasets&#34;&gt;3. Datasets&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#4-data-cleaning-and-aggregation&#34;&gt;4. Data Cleaning and Aggregation&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#5-pipeline-for-preprocessing&#34;&gt;5. Pipeline for Preprocessing&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#51-loading-and-joining-data&#34;&gt;5.1 Loading and Joining Data&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#52-feature-engineering&#34;&gt;5.2 Feature Engineering&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#53-generic-pipeline&#34;&gt;5.3 Generic Pipeline&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#6-multiple-models-for-multiple-soil-depths&#34;&gt;6. Multiple Models for Multiple Soil Depths&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#7-splitting-data-into-train-and-test&#34;&gt;7. Splitting Data into Train and Test&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#8-preliminary-analysis-and-eda&#34;&gt;8. Preliminary Analysis and EDA&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#9-initial-model-testing-regressor&#34;&gt;9. Initial Model Testing (Regressor)&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#10-classifier-vs-regressor&#34;&gt;10. Classifier vs. Regressor&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#11-various-other-linear-regression-model-experiments&#34;&gt;11. Various Other Linear Regression Model Experiments&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#12-other-models&#34;&gt;12. Other Models&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#13-conclusion&#34;&gt;13. Conclusion&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#14-acknowledgements&#34;&gt;14. Acknowledgements&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#15-references&#34;&gt;15. References&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Keywords:&lt;/strong&gt; agriculture, soil moisture, IoT, machine learning, regression, sklearn&lt;/p&gt;
&lt;h2 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;Maintaining correct soil moisture throughout the plant growing process can result in better yields, and less overall problems with the crop. Water deficiencies or surplus at various stages of growth have different effects, or even negligible effects &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. It is important to have an idea of how your land consumes and stores water, which could be very different based on the plants being used, and variation of elevation and geography.&lt;/p&gt;
&lt;p&gt;For hundreds of years, farmers have done something similar to this model. The difference is the precision that we can gain by using real data. For the past few hundred years, farmers had to rely on mostly experience and touch to know the moisture of their soil. While many farmers were successful, in the sense that they produced crops, there were ways they could have better optimized their crops to produce better. The water available to the plants is not the only variable that effects yields, but this project seeks to create an accessible model to which farmers can have predicted values of soil moisture without needing to buy and deploy expensive sensors.&lt;/p&gt;
&lt;p&gt;The model created could be used in various ways. The first main use is to be able to monitor what is currently happening in the soil so that changes can be made to correct the issue if there is one. Secondly, a farmer could evaluate historical data and compare it to yields or other results of the harvest and use this analytical information to inform future decisions. For example, a corn farmer might only care about the predicted conditions to make sure that they are within reasonable ranges. A grape farmer in a wine vineyard might use this data, along with other data, to predict the quality of wine or even the recipe of wine that would best used grapes farmed under these conditions. Again, this model is just the starting point of a theoretical complex agricultural data analysis suite.&lt;/p&gt;
&lt;p&gt;This project specifically seeks to see the effect of weather on a particular piece of land in Washington state. This process could be done all over the world to obtain benchmarks. These benchmarks could be a cheap option for a farmer that does not have the funds to support a full study of water usage on their land to use as training data. Instead, they could look for a model that has land that has similar soil and or geographical features, and then use their own weather data to estimate their soil moisture content. A major goal of this project is to create the best tool that is cheap enough for widespread adoption.&lt;/p&gt;
&lt;h2 id=&#34;2-background&#34;&gt;2. Background&lt;/h2&gt;
&lt;p&gt;Understanding how weather impacts soil moisture is something that has been studied in various ways, all because it is a driving factor in crop success. Multiple studies have sought to apply a deterministic approach to calculating soil moisture based on observational weather data.&lt;/p&gt;
&lt;p&gt;One such study, was motivated by trying to predict dust storms in China, in which soil moisture plays a large role in. This prediction used multiple-linear regression, and focused on predictions that dealt with the top 10 cm of soil. Two key takeaways can be derived from this work that are beneficial for carrying out this project.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;The influence of precipitation on surface soil moisture content does not last more than 16 days.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;The compound effect of the ratio of precipitation to evaporation, which is nonlinearly summed, can be used to calculate the surface soil moisture content in China&amp;rdquo; &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Moving forward, this project will assume that precipitation from the prior 16 days is relevant. In the case that for the specific data being fit, less days are relevant, then their coefficients in the model will likely become small enough to not affect the model. Secondly, soil moisture is influenced by a ratio or precipitation to evaporation. While this project might not seek to evaluate this relationship directly, it will seek to include data that would influence these ratios such as temperature, time of year, and wind speeds.&lt;/p&gt;
&lt;p&gt;Multiple publications have sought to come up with complete hydrological models to determine soil moisture from a variety of factors. These models are generally stochastic in nature and are reliable predictors when many parameters of the model are available. One such cited model requires a minimum or 19 variables or measured coefficients &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;. The authors of another study note the aforementioned study, as well as other similar studies, and make a point that these methods might not be the best models when it comes to practical applications. Their solution was to create a generalize model that relied mostly on soil moisture as &amp;ldquo;a function of the time-weighted average of previous cumulative rainfall over a period&amp;rdquo; &lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;. Such a model is closer in terms to simplicity and generalization to what is hoped to be accomplished in this project.&lt;/p&gt;
&lt;p&gt;The relationship between soil moisture and weather patterns is one with a rich history of study. Both of these measures affect each other in various ways. Most studies that sought to quantify this relationship were conducted at a time in which large scale sensor arrays could not have been implemented in the field. With the prevalence of IoT and improved sensing technologies, it seems as though there might not be a need to use predictive models for soil moisture, but instead just use sensor data. While this could be true in some applications, a wide array of challenges occur when trying to maintain these sensor arrays. Problems such as charging or replacing batteries, sensor and relay equipment not working if completely buried, but are in the way of farming if mounted above ground, sensors failing, etc. These were real challenges faced by the farm in which the soil moisture data was collected &lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;. The objective of this project is to create predictive models based on limited training data so that farmers would not need to deal with sensor arrays indefinitely.&lt;/p&gt;
&lt;h2 id=&#34;3-datasets&#34;&gt;3. Datasets&lt;/h2&gt;
&lt;p&gt;The first data set comes from NOAA and contains daily summary data in regards to various measurements such as temperature, precipitation, wind speed, etc. For this project, only data that came from the closest station to the field will be used &lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;. In this case, that is the Pullman station at the Pullman-Moscow airport. Below is an image showing the weather data collection location, and the red pin is at the longitude and latitude of one of the sensors in the field. This data is in csv format (see Figure 1).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-305/main/project/images/distance_map.png&#34; alt=&#34;Figure 1&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Estimated distance from weather reports to the crop fields. Distance is calculated using Google Maps&lt;/p&gt;
&lt;p&gt;The second dataset comes from the USDA. This dataset consists of &amp;ldquo;hourly and daily measurements of volumetric water content, soil temperature, and bulk electrical conductivity, collected at 42 monitoring locations and 5 depths (30, 60, 90, 120, and 150 cm)&amp;rdquo; at a farm in Washington state &lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;. Mainly, the daily temperature and water content are the measurements of interest. There are multiple files that have data that corresponds to what plants are being grown in specific places, and the makeup of the soil at each sensor cite. This auxilary information could be used in later models once the base model has been completed. This data is in tab delimited files.&lt;/p&gt;
&lt;p&gt;Within the data, there are GIS file types that can be imported into Google Maps desktop to visualize the locations of the sensors and other geographical information. Below is an example of the sensor locations plotted on the satellite image (see Figure 2).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-305/main/project/images/sensor_locations.png&#34; alt=&#34;Figure 2&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 2:&lt;/strong&gt; Location of sensors within the test field&lt;/p&gt;
&lt;h2 id=&#34;4-data-cleaning-and-aggregation&#34;&gt;4. Data Cleaning and Aggregation&lt;/h2&gt;
&lt;p&gt;The first step is to get the soil moisture data into a combined format, currently it is in one file per sensor, and there are 42 sensors. See the &lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-305/blob/main/project/code/ml_pipeline.ipynb&#34;&gt;ml_pipeline.ipynb&lt;/a&gt; file to see how this was done, specifically the section titled &amp;ldquo;Data Processing&amp;rdquo;. After aggregation, some basic information can be checked about the data. For instance, there is quite a bit of NAs in the data. These NAs are just instances where there was no measurement on that day. There is about 45% NAs in the measurement columns. To further clean the data, any row that has only NAs for the measurements will be removed.&lt;/p&gt;
&lt;p&gt;Next, the weather data needs some small adjustments. This is mostly in the form of removing columns that either are empty or have redundant data such as elevation, which is the same for every row.&lt;/p&gt;
&lt;p&gt;Once the data is sufficiently clean, some choices have to be made on joining the data. The simplest route would be to join the weather measurements directly with the same day the soil measurement, however, the previous days weather is likely to also have an impact on the moisture. As evaluated in section 2 above, it is believed that the prior 16 days weather data is what is needed for a good prediction.&lt;/p&gt;
&lt;h2 id=&#34;5-pipeline-for-preprocessing&#34;&gt;5. Pipeline for Preprocessing&lt;/h2&gt;
&lt;p&gt;Before feeding the data through a machine learning algorithm, the data needs to be manipulated in such a way that it is ready to be directly fed into an algorithm. This includes joining the two data sets, feature engineering, and other tasks that prepare the data. This will need to be done every time a new dataset is being used, so this must be built in a repeatable way. The machine learning library scikit-learn incorporates something called &amp;ldquo;pipelines&amp;rdquo; that can allow processed to be sequentially done to a dataframe. For purposes of this project two pipelines will be built, one will be used for feature engineering and joining the data, the other will be used to handle preparation of numerical, categorical, and date data. See sections: &amp;ldquo;Data Processing Pipeline&amp;rdquo; in &lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-305/blob/main/project/code/ml_pipeline.ipynb&#34;&gt;ml_pipeline.ipynb&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;51-loading-and-joining-data&#34;&gt;5.1 Loading and Joining Data&lt;/h3&gt;
&lt;p&gt;This is the first step of the entire pipeline. This is where both the weather, and the soil moisture data are read in from csv files in their raw format. The soil moisture data is found in many different files, and these all need to be combined. After combining the files, any lines that are full of NAs for the measurements are dropped. Next the weather data is loaded in. Both files have a date field which is the field they will be joined on. To make things consistent, both of these fields need to set be date format.&lt;/p&gt;
&lt;p&gt;When it comes to joining the data, each row should include the moisture content at various depths, as well as the weather information from the past ten days. While this creates a great deal of redundant data, the data is small enough that this is not an issue. Experiments will be done to evaluate just how many days of prior weather data are needed to form accurate results, while trying to minimize the number of the days.&lt;/p&gt;
&lt;h3 id=&#34;52-feature-engineering&#34;&gt;5.2 Feature Engineering&lt;/h3&gt;
&lt;p&gt;Currently only two features are added, the first is a boolean flag that says whether it rained or not on a certain day. The thought behind this is, that for some days prior to the current measurement, the amount of rain might be needed, but for other days, such as 10 days prior, it might be more important to just know if there was rain or not. This feature is engineered within the pipeline.&lt;/p&gt;
&lt;p&gt;The next feature is a categorical feature that is the month of the year. It isn&amp;rsquo;t very import to know the exact date of a measurement, but the month might be helpful in a model. This simplifies the model by not using date as a predictor, while still being able to capture this potentially important feature.&lt;/p&gt;
&lt;p&gt;An excerpt of the code used to create these two features, this comes from &lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-305/blob/main/project/code/ml_pipeline.ipynb&#34;&gt;ml_pipeline.ipynb&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#000&#34;&gt;soil&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#39;Month&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;]&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;pd&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;DatetimeIndex&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;soil&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#39;Date&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;])&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;month&lt;/span&gt;

&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;for&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;i&lt;/span&gt; &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;in&lt;/span&gt; &lt;span style=&#34;color:#204a87&#34;&gt;range&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;17&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;):&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;col_name&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#39;PRCP_&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#204a87&#34;&gt;str&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;i&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;)&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;rain_y_n_name&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#39;RAIN_Y_N_&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#204a87&#34;&gt;str&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;i&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;)&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;X&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;rain_y_n_name&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;]&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;np&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;nan&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;X&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;rain_y_n_name&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;]&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;loc&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;X&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;col_name&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;]&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;0&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;]&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;1&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;X&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;rain_y_n_name&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;]&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;loc&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;X&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;col_name&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;]&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;0&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;]&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;0&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;X&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;rain_y_n_name&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;]&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;X&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;rain_y_n_name&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;]&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;astype&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#39;object&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;53-generic-pipeline&#34;&gt;5.3 Generic Pipeline&lt;/h3&gt;
&lt;p&gt;After doing operations that are specific to the current dataset, some built in processors from sklearn are used to make sure the data can be used in a machine learning model. This means that for numerical data types, the pipeline will fill in missing values with 0 instead of leaving them as NaN. Also, the various numerical fields must be standardized, this is important for models such as linear regression so one large variable isn&amp;rsquo;t dominating the model.&lt;/p&gt;
&lt;p&gt;As far as text and categorical features, the imputer will be used to fill in missing data as well. Then a process called one hot encoding will be used to handle the categorical variables so that they can be read into sklearns estimators. Lastly, these two main processes will be put together to make a single pipeline step. Then this pipeline step will be added to a regressor of some sort to create the entire process.&lt;/p&gt;
&lt;h2 id=&#34;6-multiple-models-for-multiple-soil-depths&#34;&gt;6. Multiple Models for Multiple Soil Depths&lt;/h2&gt;
&lt;p&gt;There are a few different approaches for modeling for this particular problem. The issue is that we have multiple things we would like to predict with the same predictors. It is unlikely that the model that predicts for a depth of 30 cm, would accurately predict for a depth of 150 cm. In order to adjust the models, a separate model will be created for each depth, with that said, the predictors are all the same for each depth, but the trained output is different. To accomplish this, five different datasets were constructed, each one representing a depth. All rows in which the predicted value is not available for that depth were pruned from the dataset.&lt;/p&gt;
&lt;p&gt;In each experiment, there will be 5 different models created. Initially, these 5 models will use the same hyper-parameters for all the depths. It might turn out that all the models will need the same hyper-parameters, or each soil depth could be different. This will be examined through experimentation.&lt;/p&gt;
&lt;h2 id=&#34;7-splitting-data-into-train-and-test&#34;&gt;7. Splitting Data into Train and Test&lt;/h2&gt;
&lt;p&gt;In order to test any model created, there must be a split between test and training data. This is done by using a function in sklearn. In this case, there are about 76k rows in the data set. For the training data, 80% of the total data will be used, or about 60.8k records. The split is done after shuffling the rows so that it does not just pick the top 80% every time. Lastly the data is split using a stratified method. As we want to have models that take the specific area of the field into account, that means that we need to have the different areas of the field represented equally in both the training and testing dataset. This means that if 10% of the data came from sensor CAF0003, then roughly 10% of the training data will come from CAF0003 as well as 10% of the test data will be from this location.&lt;/p&gt;
&lt;h2 id=&#34;8-preliminary-analysis-and-eda&#34;&gt;8. Preliminary Analysis and EDA&lt;/h2&gt;
&lt;p&gt;Before building a machine learning model, it is important to get a general idea of how the data looks, to see if any insights can be made right away. The actual visualizations were built using a python package called Altair. This created the visualizations well, but the actual notebook that would contain these images was too large to include in their entirety.&lt;/p&gt;
&lt;p&gt;The first two visualizations (&lt;a href=&#34;https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-305/main/project/images/one.png&#34;&gt;viz_1&lt;/a&gt;, &lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-305/blob/main/project/images/two.png&#34;&gt;viz_2&lt;/a&gt;) are grids that show the entire distribution of measurements across each sensor. The first grid is the volume of water at 30 cm, and the second grid is the water volume at 150 cm. Each chart could be looked at and examined on it&amp;rsquo;s own, but what is most important to note is the variability of the measures from location to location. These different sensors are not that far away, but show that different areas of the farm do retain water in different ways. See Figure 3 for a small section of the grid from the visualization on the sensors at 30cm.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-305/main/project/images/one_small.png&#34; alt=&#34;Figure 3&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 3:&lt;/strong&gt; Six locations soil moisture level over time at 30 cm depth&lt;/p&gt;
&lt;p&gt;The third and fourth grid shows the temperature at 150 cm, the results are what would logically be expected. The different sensors do not show much variance from location to location.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-305/main/project/images/four_small.png&#34; alt=&#34;Figure 4&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 4:&lt;/strong&gt; Six locations soil temperature over time at 150 cm depth&lt;/p&gt;
&lt;h2 id=&#34;9-initial-model-testing-regressor&#34;&gt;9. Initial Model Testing (Regressor)&lt;/h2&gt;
&lt;p&gt;Once the pipelines were setup, the first model could be tested for accuracy. As the output data is continuous in nature, the easiest machine learning algorithm to test to make sure everything is correct, was a linear regression model. It seems fairly likely that a linear regression model would do rather well with this data. The weather is the driving factor in soil moisture in a non-irrigated field, so this test is a litmus test to make sure that the data is good and provide a baseline measurement for future models. The experiment log below shows the returned values from the test that was run. Over the course of experimentation, a log such as this will be kept.&lt;/p&gt;
&lt;p&gt;The results are as follows:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Experiment&lt;/th&gt;
&lt;th&gt;Depth&lt;/th&gt;
&lt;th&gt;Fit_Time&lt;/th&gt;
&lt;th&gt;Pred_Time&lt;/th&gt;
&lt;th&gt;r2_score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;First Linear Reg&lt;/td&gt;
&lt;td&gt;30cm&lt;/td&gt;
&lt;td&gt;2.029387&lt;/td&gt;
&lt;td&gt;0.169824&lt;/td&gt;
&lt;td&gt;9.16E-01&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;First Linear Reg&lt;/td&gt;
&lt;td&gt;60cm&lt;/td&gt;
&lt;td&gt;2.002373&lt;/td&gt;
&lt;td&gt;0.17377&lt;/td&gt;
&lt;td&gt;-1.42E+15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;First Linear Reg&lt;/td&gt;
&lt;td&gt;90cm&lt;/td&gt;
&lt;td&gt;2.080393&lt;/td&gt;
&lt;td&gt;0.162992&lt;/td&gt;
&lt;td&gt;9.49E-01&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;First Linear Reg&lt;/td&gt;
&lt;td&gt;120cm&lt;/td&gt;
&lt;td&gt;2.299457&lt;/td&gt;
&lt;td&gt;0.18056&lt;/td&gt;
&lt;td&gt;9.46E-01&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;First Linear Reg&lt;/td&gt;
&lt;td&gt;150cm&lt;/td&gt;
&lt;td&gt;2.573193&lt;/td&gt;
&lt;td&gt;0.186042&lt;/td&gt;
&lt;td&gt;9.43E-01&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Figure 5:&lt;/strong&gt; Baseline experiment results&lt;/p&gt;
&lt;p&gt;These results show that the data is pretty well correlated and that there is reason to believe that we could predict soil moisture from weather alone. Although an r^2 of around 0.916-0.949 are pretty good, with such highly related predictors, there is definitely room for model improvement. Also for a depth of 60 cm, something is not predicting correctly and is resulting in a small negative r^2&lt;/p&gt;
&lt;h2 id=&#34;10-classifier-vs-regressor&#34;&gt;10. Classifier vs. Regressor&lt;/h2&gt;
&lt;p&gt;While the output is continuous, there is an argument to use a categorical classifier model. For a specific plant, an optimal moisture range could be studied. For examples sake, the range could be 0.2-0.4 units. Then it would not matter if the soil is 0.2 or 0.3, both would be in the acceptable range. With this in mind, certain levels could be created to alert the farmer of which category they could be experiencing. For example there might be five levels: too dry, acceptable dryness, optimal, acceptable wetness, and too wet. The training data could be adjusted to fit into these categories.&lt;/p&gt;
&lt;p&gt;Code to create a categorical variable for each of the depth measurements can be found in the section &amp;ldquo;Make Classifier Label&amp;rdquo; in the file: &lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-305/blob/main/project/code/ml_pipeline.ipynb&#34;&gt;ml_pipeline.ipynb&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In the end, the decision to not use classifier methods was made. After using a regressor, the output could be converted to a categorical feature if the user or application so desired this. As our output is continuous in nature, precision would be lost.&lt;/p&gt;
&lt;h2 id=&#34;11-various-other-linear-regression-model-experiments&#34;&gt;11. Various Other Linear Regression Model Experiments&lt;/h2&gt;
&lt;p&gt;The next set of experiments came up with the results in the following table. This was a test to see if baseline Lasso or Ridge Regression would improve on the basic linear regression model. Results and code for this portion can be found in &lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-305/blob/main/project/code/ml_pipeline.ipynb&#34;&gt;ml_pipeline.ipynb&lt;/a&gt; under the &amp;ldquo;Linear Regression Tests&amp;rdquo; section.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Experiment&lt;/th&gt;
&lt;th&gt;Depth&lt;/th&gt;
&lt;th&gt;Fit_Time&lt;/th&gt;
&lt;th&gt;Pred_Time&lt;/th&gt;
&lt;th&gt;r2_score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Ridge Reg, Alpha = 1&lt;/td&gt;
&lt;td&gt;30cm&lt;/td&gt;
&lt;td&gt;1.321553&lt;/td&gt;
&lt;td&gt;0.173714&lt;/td&gt;
&lt;td&gt;0.916211&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Ridge Reg, Alpha = 1&lt;/td&gt;
&lt;td&gt;60cm&lt;/td&gt;
&lt;td&gt;1.29167&lt;/td&gt;
&lt;td&gt;0.187392&lt;/td&gt;
&lt;td&gt;0.942757&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Ridge Reg, Alpha = 1&lt;/td&gt;
&lt;td&gt;90cm&lt;/td&gt;
&lt;td&gt;1.393526&lt;/td&gt;
&lt;td&gt;0.197152&lt;/td&gt;
&lt;td&gt;0.94879&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Ridge Reg, Alpha = 1&lt;/td&gt;
&lt;td&gt;120cm&lt;/td&gt;
&lt;td&gt;1.307926&lt;/td&gt;
&lt;td&gt;0.176656&lt;/td&gt;
&lt;td&gt;0.946032&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Ridge Reg, Alpha = 1&lt;/td&gt;
&lt;td&gt;150cm&lt;/td&gt;
&lt;td&gt;1.33738&lt;/td&gt;
&lt;td&gt;0.179585&lt;/td&gt;
&lt;td&gt;0.94332&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Lasso Reg, Alpha = 1&lt;/td&gt;
&lt;td&gt;30cm&lt;/td&gt;
&lt;td&gt;1.45102&lt;/td&gt;
&lt;td&gt;0.170752&lt;/td&gt;
&lt;td&gt;-0.00018&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Lasso Reg, Alpha = 1&lt;/td&gt;
&lt;td&gt;60cm&lt;/td&gt;
&lt;td&gt;1.419546&lt;/td&gt;
&lt;td&gt;0.174177&lt;/td&gt;
&lt;td&gt;-4.6E-05&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Lasso Reg, Alpha = 1&lt;/td&gt;
&lt;td&gt;90cm&lt;/td&gt;
&lt;td&gt;1.4632&lt;/td&gt;
&lt;td&gt;0.176657&lt;/td&gt;
&lt;td&gt;-5.7E-06&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Lasso Reg, Alpha = 1&lt;/td&gt;
&lt;td&gt;120cm&lt;/td&gt;
&lt;td&gt;1.553091&lt;/td&gt;
&lt;td&gt;0.182349&lt;/td&gt;
&lt;td&gt;-1.1E-06&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Lasso Reg, Alpha = 1&lt;/td&gt;
&lt;td&gt;150cm&lt;/td&gt;
&lt;td&gt;1.437419&lt;/td&gt;
&lt;td&gt;0.163967&lt;/td&gt;
&lt;td&gt;-0.00018&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Ridge Reg - GSCV&lt;/td&gt;
&lt;td&gt;30cm&lt;/td&gt;
&lt;td&gt;3.914718&lt;/td&gt;
&lt;td&gt;0.203007&lt;/td&gt;
&lt;td&gt;0.916235&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Ridge Reg - GSCV&lt;/td&gt;
&lt;td&gt;60cm&lt;/td&gt;
&lt;td&gt;3.726651&lt;/td&gt;
&lt;td&gt;0.172752&lt;/td&gt;
&lt;td&gt;0.942757&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Ridge Reg - GSCV&lt;/td&gt;
&lt;td&gt;90cm&lt;/td&gt;
&lt;td&gt;4.135154&lt;/td&gt;
&lt;td&gt;0.200589&lt;/td&gt;
&lt;td&gt;0.948796&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Ridge Reg - GSCV&lt;/td&gt;
&lt;td&gt;120cm&lt;/td&gt;
&lt;td&gt;4.03203&lt;/td&gt;
&lt;td&gt;0.193512&lt;/td&gt;
&lt;td&gt;0.946032&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Ridge Reg - GSCV&lt;/td&gt;
&lt;td&gt;150cm&lt;/td&gt;
&lt;td&gt;4.361977&lt;/td&gt;
&lt;td&gt;0.191296&lt;/td&gt;
&lt;td&gt;0.943328&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Figure 6:&lt;/strong&gt; Further Linear Regression Experiment Results&lt;/p&gt;
&lt;p&gt;For the first two experiments, an alpha of 1 was used for both ridge and lasso regression. The third experiment used a special regressor that uses cross validation to try to find the best alpha value and then fit the model based on that. The best alpha value seemed to not have much effect at all on the results. Still the ridge regression so far was the best performing model.&lt;/p&gt;
&lt;h2 id=&#34;12-other-models&#34;&gt;12. Other Models&lt;/h2&gt;
&lt;p&gt;While there were great results in the different linear regression models, other models should be evaluated to make sure that something is not missed. Three models were chosen to check, Stochastic Gradient Descent, Support Vector Machine, and Random Forest. All of these models were tested with default parameters and their results are shown below in Figure 7, and the code can be found in the section called &amp;ldquo;Other Regressors Tests&amp;rdquo;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Experiment&lt;/th&gt;
&lt;th&gt;Depth&lt;/th&gt;
&lt;th&gt;Fit_Time&lt;/th&gt;
&lt;th&gt;Pred_Time&lt;/th&gt;
&lt;th&gt;r2_score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Random Forest&lt;/td&gt;
&lt;td&gt;30cm&lt;/td&gt;
&lt;td&gt;60.06952&lt;/td&gt;
&lt;td&gt;0.250543&lt;/td&gt;
&lt;td&gt;0.977118&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Random Forest&lt;/td&gt;
&lt;td&gt;60cm&lt;/td&gt;
&lt;td&gt;62.17435&lt;/td&gt;
&lt;td&gt;0.216641&lt;/td&gt;
&lt;td&gt;0.989113&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Random Forest&lt;/td&gt;
&lt;td&gt;90cm&lt;/td&gt;
&lt;td&gt;62.29475&lt;/td&gt;
&lt;td&gt;0.243051&lt;/td&gt;
&lt;td&gt;0.99158&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Random Forest&lt;/td&gt;
&lt;td&gt;120cm&lt;/td&gt;
&lt;td&gt;64.48227&lt;/td&gt;
&lt;td&gt;0.256666&lt;/td&gt;
&lt;td&gt;0.991274&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Random Forest&lt;/td&gt;
&lt;td&gt;150cm&lt;/td&gt;
&lt;td&gt;68.47001&lt;/td&gt;
&lt;td&gt;0.240149&lt;/td&gt;
&lt;td&gt;0.991748&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SVM&lt;/td&gt;
&lt;td&gt;30cm&lt;/td&gt;
&lt;td&gt;38.83822&lt;/td&gt;
&lt;td&gt;5.712513&lt;/td&gt;
&lt;td&gt;0.676934&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SVM&lt;/td&gt;
&lt;td&gt;60cm&lt;/td&gt;
&lt;td&gt;106.2816&lt;/td&gt;
&lt;td&gt;7.897556&lt;/td&gt;
&lt;td&gt;0.766008&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SVM&lt;/td&gt;
&lt;td&gt;90cm&lt;/td&gt;
&lt;td&gt;102.9438&lt;/td&gt;
&lt;td&gt;7.763206&lt;/td&gt;
&lt;td&gt;0.788833&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SVM&lt;/td&gt;
&lt;td&gt;120cm&lt;/td&gt;
&lt;td&gt;79.76476&lt;/td&gt;
&lt;td&gt;6.985236&lt;/td&gt;
&lt;td&gt;0.760895&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SVM&lt;/td&gt;
&lt;td&gt;150cm&lt;/td&gt;
&lt;td&gt;96.46352&lt;/td&gt;
&lt;td&gt;7.548365&lt;/td&gt;
&lt;td&gt;0.760936&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SGD&lt;/td&gt;
&lt;td&gt;30cm&lt;/td&gt;
&lt;td&gt;1.382992&lt;/td&gt;
&lt;td&gt;0.171777&lt;/td&gt;
&lt;td&gt;0.89019&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SGD&lt;/td&gt;
&lt;td&gt;60cm&lt;/td&gt;
&lt;td&gt;1.392753&lt;/td&gt;
&lt;td&gt;0.15128&lt;/td&gt;
&lt;td&gt;0.931394&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SGD&lt;/td&gt;
&lt;td&gt;90cm&lt;/td&gt;
&lt;td&gt;1.399587&lt;/td&gt;
&lt;td&gt;0.142493&lt;/td&gt;
&lt;td&gt;0.941092&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SGD&lt;/td&gt;
&lt;td&gt;120cm&lt;/td&gt;
&lt;td&gt;1.438626&lt;/td&gt;
&lt;td&gt;0.150302&lt;/td&gt;
&lt;td&gt;0.936692&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SGD&lt;/td&gt;
&lt;td&gt;150cm&lt;/td&gt;
&lt;td&gt;1.403488&lt;/td&gt;
&lt;td&gt;0.14933&lt;/td&gt;
&lt;td&gt;0.92957&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Figure 7:&lt;/strong&gt; Further Linear Regression Experiment Results&lt;/p&gt;
&lt;p&gt;The random forest regressor performed amazingly in predicting the soil moisture. While the lower depths of soil did perform better than the depth of 30 cm. As random forests performed so well out of the box, some attempts were made to tune the hyperparameters, but most experiments turned out to be computationally expensive.&lt;/p&gt;
&lt;h2 id=&#34;13-conclusion&#34;&gt;13. Conclusion&lt;/h2&gt;
&lt;p&gt;The end results of all experimentation was a process in which two datasets could be joined and fed into a model to predict the soil moisture with great accuracy, an r^2 score of between 0.977 and 0.991 depending on the depth using a Random Forest Regressor with default settings. This process could be a repeatable process in which a farmer contracts a company to gather training data on their land specifically for a growing season. As the collection of the sensor data could be cumbersome and expensive to deal with as a farmer, so this is an alternative that is cheaper and still gives nearly the same results as having sensors constantly running. Alternatively, this process could be a subprocess in a larger suite of software that farmers could use for predictive analysis or even to have data on soil moisture from a grow season to use in post season analysis of their crop produced. As long as large scale AI programs are still expensive and cumbersome for farmers to deal with, there will be a low rate of adoption. This project has shown that a solution for large scale soil moisture prediction software could be done with relatively low computational cost.&lt;/p&gt;
&lt;h2 id=&#34;14-acknowledgements&#34;&gt;14. Acknowledgements&lt;/h2&gt;
&lt;p&gt;The author would like to thank Dr. Gregor Von Laszewski, Dr. Geoffrey Fox, and the associate instructors in the &lt;em&gt;FA20-BL-ENGR-E534-11530: Big Data Applications&lt;/em&gt; course (offered in the Fall 2020 semester at Indiana University, Bloomington) for their continued assistance and suggestions with regard to exploring this idea and also for their aid with preparing the various drafts of this article.&lt;/p&gt;
&lt;h2 id=&#34;15-references&#34;&gt;15. References&lt;/h2&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;O. Denmead and R. Shaw, &amp;ldquo;The Effects of Soil Moisture Stress at Different Stages of Growth on the Development and Yield of Corn 1&amp;rdquo;, Agronomy Journal, vol. 52, no. 5, pp. 272-274, 1960. Available: 10.2134/agronj1960.00021962005200050010x. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;K. Shang, S. Wang, Y. Ma, Z. Zhou, J. Wang, H. Liu and Y. Wang, &amp;ldquo;A scheme for calculating soil moisture content by using routine weather data&amp;rdquo;, Atmospheric Chemistry and Physics, vol. 7, no. 19, pp. 5197-5206, 2007 [Online]. Available: &lt;a href=&#34;https://hal.archives-ouvertes.fr/hal-00302825/document&#34;&gt;https://hal.archives-ouvertes.fr/hal-00302825/document&lt;/a&gt; &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;W. Capehart and T. Carlson, &amp;ldquo;Estimating near-surface soil moisture availability using a meteorologically driven soil-water profile model&amp;rdquo;, Journal of Hydrology, vol. 160, no. 1-4, pp. 1-20, 1994 [Online]. Available: &lt;a href=&#34;https://tinyurl.com/yxjyuy5x&#34;&gt;https://tinyurl.com/yxjyuy5x&lt;/a&gt; &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;F. Pan, C. Peters-Lidard and M. Sale, &amp;ldquo;An analytical method for predicting surface soil moisture from rainfall observations&amp;rdquo;, Water Resources Research, vol. 39, no. 11, 2003 [Online]. Available: &lt;a href=&#34;https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2003WR002142&#34;&gt;https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2003WR002142&lt;/a&gt;. [Accessed: 08- Nov- 2020] &lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;C. Gasch, D. Brown, C. Campbell, D. Cobos, E. Brooks, M. Chahal and M. Poggio, &amp;ldquo;A Field-Scale Sensor Network Data Set for Monitoring and Modeling the Spatial and Temporal Variation of Soil Water Content in a Dryland Agricultural Field&amp;rdquo;, Water Resources Research, vol. 53, no. 12, pp. 10878-10887, 2017 [Online]. Available: &lt;a href=&#34;https://agupubs.onlinelibrary.wiley.com/doi/full/10.1002/2017WR021307&#34;&gt;https://agupubs.onlinelibrary.wiley.com/doi/full/10.1002/2017WR021307&lt;/a&gt;. [Accessed: 08- Nov- 2020] &lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;N. (NCEI), &amp;ldquo;Climate Data Online (CDO) - The National Climatic Data Center&amp;rsquo;s (NCDC) Climate Data Online (CDO) provides free access to NCDC&amp;rsquo;s archive of historical weather and climate data in addition to station history information. | National Climatic Data Center (NCDC)&amp;rdquo;, Ncdc.noaa.gov, 2020. [Online]. Available: &lt;a href=&#34;https://www.ncdc.noaa.gov/cdo-web/&#34;&gt;https://www.ncdc.noaa.gov/cdo-web/&lt;/a&gt;. [Accessed: 19- Oct- 2020]. &lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&amp;ldquo;Data from: A field-scale sensor network data set for monitoring and modeling the spatial and temporal variation of soil moisture in a dryland agricultural field&amp;rdquo;, USDA: Ag Data Commons, 2020. [Online]. Available: &lt;a href=&#34;https://data.nal.usda.gov/dataset/data-field-scale-sensor-network-data-set-monitoring-and-modeling-spatial-and-temporal-variation-soil-moisture-dryland-agricultural-field&#34;&gt;https://data.nal.usda.gov/dataset/data-field-scale-sensor-network-data-set-monitoring-and-modeling-spatial-and-temporal-variation-soil-moisture-dryland-agricultural-field&lt;/a&gt;. [Accessed: 19- Oct- 2020]. &lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-305/test/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-305/test/</guid>
      <description>
        
        
        &lt;p&gt;Testing if I have write access to this repo.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-307/assignment6/assignment6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-307/assignment6/assignment6/</guid>
      <description>
        
        
        &lt;h1 id=&#34;ai-in-precision-medicine&#34;&gt;AI in Precision Medicine&lt;/h1&gt;
&lt;p&gt;In recent years, precision medicine has started to become the new standard when it comes to healthcare. This is moving us from a one size fits all approach to a more personal, data-driven approach that allows hospitals and treatment centers to spend more efficiently and have a higher patient outcome. Precision medicine is using knowledge that is specific to one patient, such as biomarkers, rather than the generic approach to an issue. The overall goal is to &amp;quot;design and optimize the pathway for diagnosis and prognosis through the use of large multidimensional biological datasets that capture different variables such as genes&amp;quot; [1].&lt;/p&gt;
&lt;p&gt;Artificial intelligence (AI) has been increasingly growing in business, society and now is emerging in healthcare. The potential that AI has can completely transform patient care. These technologies can perform to or exceed human capability when it comes to different medical tasks such as cancer diagnosis or disease diagnosis as well as patient engagement and administration tasks. AI has the potential to offer automated care to individuals by providing precision medicine.&lt;/p&gt;
&lt;p&gt;Precision medicine enables patients to not only recover from illnesses faster but to also stay healthy longer. However, with the increased use of precision medicine new challenges arise such as the increasing amount of data, a lack of specialists and ever increasing drug development costs. &amp;quot;Healthcare data is projected to grow by 43 percent by 2020, to roughly 2.3 zettabytes. The size of the data is not the only problem; it&#39;s the kind of data as well. Eighty percent of it is unstructured and mostly unlabeled, making it hard to extract value from the datasets&amp;quot; [2].&lt;/p&gt;
&lt;p&gt;Artificial intelligence (AI) has helped reshape how precision medicine is distributed. AI is able to solve many of the problems that have arisen. For big data challenges, AI methods are able to clear up obstacles that large and unstructured data present. In medical imaging, machine learning can be introduced to help classify what type of issue is present by training a model over thousands of images and predicting on the patient&#39;s image. Neural networks have also been able to make predictions when it comes to precision medicine.&lt;/p&gt;
&lt;p&gt;Neural networks are a more advanced form of AI. The uses in precision medicine is for categorisation applications such as the likelihood of a patient developing a disease. Neural networks look at problems from inputs, outputs, and weights of features to try and associate inputs with the corresponding outputs. &amp;quot;It has been likened to the way that neurons process signals, but the analogy to the brain&#39;s function is relatively weak&amp;quot; [3].&lt;/p&gt;
&lt;p&gt;Deep learning is one of the most complex forms of AI. This involves hundreds or thousands of models with numerous levels of features that are needed to predict the outcomes. Precision medicine takes advantage of this technology through the &amp;quot;recognition of potentially cancerous lesions in radiology images&amp;quot; [4]. Deep learning is able to be applied to fields such as radiomics. This is the practice of detecting features in image data that cannot be detected with the human eye. &amp;quot;Their combination appears to promise greater accuracy in diagnosis than the previous generation of automated tools for image analysis, known as computer-aided detection or CAD&amp;quot; [4].&lt;/p&gt;
&lt;p&gt;AI plays a pivotal role in the future of healthcare. In the development of precision medicine, it is one of the primary components in order to advance care for patients. Efforts to help classify medical imagery more quickly and accurately have proven more effective with the amount of data used to train such models. A big challenge that AI is facing in precision medicine is whether or not this technology will be widely adopted. These systems will need to have some regulations in order to have a universal standard. This will allow doctors and medical personnel to train with this technology so they will be able to provide the care their patients deserve. AI will never replace the human aspect of precision medicine but over time AI will be able to make the jobs and lives of the doctors and patients better and healthier.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] M. Uddin, Y. Wang, and M. Woodbury-Smith, &amp;quot;Artificial intelligence for precision medicine in neurodevelopmental disorders,&amp;quot; &lt;em&gt;Nature News&lt;/em&gt;, 21-Nov-2019. [Online]. Available: &lt;a href=&#34;https://www.nature.com/articles/s41746-019-0191-0&#34;&gt;https://www.nature.com/articles/s41746-019-0191-0&lt;/a&gt;. [Accessed: 11-Oct-2020].&lt;/p&gt;
&lt;p&gt;[2] H. Chamraj, &amp;quot;Powering Precision Medicine with Artificial Intelligence,&amp;quot; &lt;em&gt;Intel&lt;/em&gt;. [Online]. Available: &lt;a href=&#34;https://www.intel.com/content/www/us/en/artificial-intelligence/posts/powering-precision-medicine-artificial-intelligence.html&#34;&gt;https://www.intel.com/content/www/us/en/artificial-intelligence/posts/powering-precision-medicine-artificial-intelligence.html&lt;/a&gt;. [Accessed: 12-Oct-2020].&lt;/p&gt;
&lt;p&gt;[3] T. Davenport and R. Kalakota, &amp;quot;The potential for artificial intelligence in healthcare,&amp;quot; &lt;em&gt;Future healthcare journal&lt;/em&gt;, Jun-2019. [Online]. Available: &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6616181/&#34;&gt;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6616181/&lt;/a&gt;. [Accessed: 12-Oct-2020].&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-307/project/project/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-307/project/project/</guid>
      <description>
        
        
        &lt;h1 id=&#34;analysis-of-financial-markets-based-on-president-trumps-tweets&#34;&gt;Analysis of Financial Markets based on President Trump&amp;rsquo;s Tweets&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-307/actions&#34;&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-307/workflows/Check%20Report/badge.svg&#34; alt=&#34;Check Report&#34;&gt;&lt;/a&gt;
&lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-307/actions&#34;&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-307/workflows/Status/badge.svg&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;
Status: in progress&lt;/p&gt;
&lt;p&gt;Alex Baker, fa20-523-307, &lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-307/blob/master/project/project.md&#34;&gt;Edit&lt;/a&gt;&lt;/p&gt;


&lt;div class=&#34;pageinfo pageinfo-primary&#34;&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;In behavioral economics, it is said that emotions have an effect on individual&amp;rsquo;s behavior and their choice in their decision making process. This can be true for a society at large but can this apply to the leader of the free world? Here we will investigate the collective mood of President Trump&amp;rsquo;s tweets are correlated to the value of the Nasdaq Stock Market (NASDAQ) during certain events during his presidency. The applications of sentiment analysis will help find the correlation that we are looking for. President Trump&amp;rsquo;s tweets will be used to analyze the mood and the NASDAQ movements.&lt;/p&gt;
&lt;p&gt;Contents&lt;/p&gt;
&lt;div class=&#34;toc&#34;&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#1-introduction&#34;&gt;1. Introduction&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#2-datasets&#34;&gt;2. DataSets&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#3-data-cleaning-and-preprocessing&#34;&gt;3. Data Cleaning and Preprocessing&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#31-twitter-data&#34;&gt;3.1 Twitter Data&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#32-stock-data&#34;&gt;3.2 Stock Data&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#4-methodologyprocess&#34;&gt;4. Methodology/Process&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#5-preliminary-analysis-and-eda&#34;&gt;5. Preliminary Analysis and EDA&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#51-twitter-data&#34;&gt;5.1 Twitter Data&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#52-stock-data&#34;&gt;5.2 Stock Data&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#6-defining-events-during-trumps-presidency&#34;&gt;6. Defining events during Trump&amp;rsquo;s presidency&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#61-the-impeachment-of-president-trump&#34;&gt;6.1 The Impeachment of President Trump&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#62-the-dakota-access-and-keystone-xl-pipelines-approval&#34;&gt;6.2 The Dakota Access and Keystone XL pipelines approval&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#63-the-government-shutdown&#34;&gt;6.3 The Government Shutdown&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#7-conclusion&#34;&gt;7. Conclusion&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#8-references&#34;&gt;8. References&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Keywords:&lt;/strong&gt; analysis, finance, stock markets, twitter, politics&lt;/p&gt;
&lt;h2 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;Financial markets have been an area of research in both academia and business. Analysis and predictions has been growing in its accuracy with an every increasing amount of data used to test these models. &amp;ldquo;The Efficient Market Hypothesis (EMH) states that stock market prices are largely driven by &lt;em&gt;new&lt;/em&gt; information and follow a random walk pattern&amp;rdquo;&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. This shows that prices will follow news rather than previous and present prices. Information is unpredictable in terms of its release/publication showing market prices will follow a random walk pattern and the prediction can not be high.&lt;/p&gt;
&lt;p&gt;There are some problems that arise with EMH. One problem is that &amp;ldquo;stock prices does not follow a random walk pattern and can be predicted to a certain degree&amp;rdquo;&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. Another problem associated with EMH is with the information&amp;rsquo;s unpredictability, the unpredictability is called into question with the introduction of social media (Facebook, Twitter, blogs). The rise of social media can be a early indicator for news before it is released/published. This project will analyze the market based on how the President tweets during certain events.&lt;/p&gt;
&lt;h2 id=&#34;2-datasets&#34;&gt;2. DataSets&lt;/h2&gt;
&lt;p&gt;In this project, two datasets will be used -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The NASDAQ values from November 2016 to January 2020. This data was obtained through Yahoo! Finance and includes Date, Open, High, Low, Close, Adj Close, and Volume for a given day.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;President Trump&amp;rsquo;s tweets during the periods of November 2016 to January 2020 is over 41,000 tweets. The data includes id, link, content, date, retweets, favorites, mentions, hashtags, and geo for every tweet in the time frame. Since the performance of the analysis is on a daily basis, tweets will be split up by Date. This data is available on Kaggle (&lt;a href=&#34;https://www.kaggle.com/austinreese/trump-tweets?select=trumptweets.csv)&#34;&gt;https://www.kaggle.com/austinreese/trump-tweets?select=trumptweets.csv)&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To strengthen the analysis, even more, some code from the 2016 election’s analysis of markets may be utilized but the focus will be on the markets during the Trump administration. Rally data maybe introduced in order to have a deeper sense of some of the tweets when it comes to important news that is announces at President Trump&amp;rsquo;s rallies. In order to have a realistic and strong analysis, the financial data needs to be aligned with the timing of tweets but news that has already started to affect the markets before a tweet has been sent out needs to be taken into account.&lt;/p&gt;
&lt;h2 id=&#34;3-data-cleaning-and-preprocessing&#34;&gt;3. Data Cleaning and Preprocessing&lt;/h2&gt;
&lt;p&gt;The data required for this project is stock market data and Twitter data from President Trump. Stock market data was collected from Yahoo Finance&amp;rsquo;s API. This data was saved to a CSV file then imported using Pandas. The Twitter data was collected by a Kaggle user and is imported though Kaggle&amp;rsquo;s API or through the use of a local copy saved from the site. The data obtained needs to be cleaned and pre-processed in order to make it reliable for analysis through the use of Pandas, Regex, and Matplotlib.&lt;/p&gt;
&lt;h3 id=&#34;31-twitter-data&#34;&gt;3.1 Twitter Data&lt;/h3&gt;
&lt;p&gt;When importing the Twitter data, there are several things that are noticed when printing the first five rows. Three of the columns mention, hashtags, and geo are currently showing NaN. After calculating the missing values, all the values in these columns are missing or are zero so we can drop these columns from the dataframe.&lt;/p&gt;
&lt;p&gt;The tweets are one of the last columns needed to be cleaned. The text of the tweets needs to be uniformed in order to conduct analysis. Removing punctuations was the first step followed by removing content specifically seen in tweets. These could be the word retweet, the hashtag symbol(#), the @ symbol followed by a username, and any hyperlinks that could be in a tweet.&lt;/p&gt;
&lt;h3 id=&#34;32-stock-data&#34;&gt;3.2 Stock Data&lt;/h3&gt;
&lt;p&gt;Stock data has a unique set of challenges when it comes to cleaning. Unlike tweets, stock data is only available Monday through Friday and is not available for holidays that the market is closed. In order to have a complete dataset, several options are available. One option is to drop the tweets that fall on a weekend. This would not be useful since markets can react to news that happens on the weekend. Another option is that &amp;ldquo;if the NASDAQ value on a given day is x and the next available data point is y with n days missing in between, we approximate the missing data by estimating the first day after x to be (y+x)/2 and then following the same method recursively till all gaps are filled&amp;rdquo; &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h2 id=&#34;4-methodologyprocess&#34;&gt;4. Methodology/Process&lt;/h2&gt;
&lt;p&gt;The collection of finance and Twitter data will be used to visualize the results. Some of Twitter or dataset data will need to be cleaned and classified to build the model. The methodology is composed of the following steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use data from President Trump&amp;rsquo;s personal twitter and data from Yahoo Finance API to help visualize&lt;/li&gt;
&lt;li&gt;Data cleaning and extraction&lt;/li&gt;
&lt;li&gt;Sentiment Analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Sentiment analysis is a key component to categorize President Trump&amp;rsquo;s tweets.  Polarity and subjectivity are the two metrics that are used to classify each tweet. Polarity measures the opinion or emotion expressed in a piece of text; the value is returned as a float within the range of -1.0 to 1.0. Subjectivity, on the other hand, reflects the feelings or beliefs in a piece of text; the value is returned as a float within the range 0.0 to 1.0 where 0.0 is very objective and 1.0 is very subjective. TextBlob is the library utilized for processing the tweet&amp;rsquo;s polarity and subjectivity. The sentiment method is called along with the methods for polarity and subjectivity in their own functions. The returned values are added into two columns in the dataframe.&lt;/p&gt;
&lt;p&gt;The plot used for the sentiment analysis is a scatter plot. This will allow for each tweet to be plotted with their respected polarity and subjectivity. A line plot is the best plot to used in order to easily visualize market price verses the sentiment of the tweets. In plotting the line for the sentiment of tweets, several issues arise. The first major issues is that multiple tweets are published on a given day with varying degrees of sentiment. The line graph will display a vertical line for all the points represented. One solution is to take the average of the days tweets and use this new value on the graph. This method can be aligned on the same axes as the stock market data.&lt;/p&gt;
&lt;h2 id=&#34;5-preliminary-analysis-and-eda&#34;&gt;5. Preliminary Analysis and EDA&lt;/h2&gt;
&lt;h3 id=&#34;51-twitter-data&#34;&gt;5.1 Twitter Data&lt;/h3&gt;
&lt;p&gt;When starting to conduct preliminary analysis and exploratory data analysis (EDA), it is helpful to first check for any null values in the data and there are no null values in the twitter data.&lt;/p&gt;
&lt;p&gt;The date column is a column that is needed to track the amount of tweets per month and year. In the column, the timestamp and the date are combined so this need to be separated in several ways. The first being separating the date from the timestamp into its own column. This is followed up by separating the date into 4 columns for day, month, year and month-year in order to track tweets based on specified criteria.&lt;/p&gt;
&lt;p&gt;After graphing the amount of tweets per year, the observation is that 2016 and 2020 have a low tweet count. The reminder is that the data starts in November 2016 making 2016 have two months of data compared to 2020 with only one month being January. From 2017 through 2019, we can see that the amount of tweets increases by almost a thousand every year. The tweets per month tell a different story. The amount varies greatly over the years with the greatest amount being near the end of 2016 and the beginning of 2017. The sentiment of the tweets show that a majority of the tweets are a little skewed to the right of the graph. This shows that may of the President tweets are positive in some aspect as well as have a personal opinion, emotion or judgement.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-307/main/project/images/year_tweets.png&#34; alt=&#34;Figure 1: Number of Tweets per Year&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Number of Tweets per Year&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-307/main/project/images/month_tweets.png&#34; alt=&#34;Figure 2: Number of Tweets per Month&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 2:&lt;/strong&gt; Number of Tweets per Month&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-307/main/project/images/sentiment.png&#34; alt=&#34;Figure 3: Sentiment Analysis of Tweets&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 3:&lt;/strong&gt; Sentiment Analysis of Tweets&lt;/p&gt;
&lt;h3 id=&#34;52-stock-data&#34;&gt;5.2 Stock Data&lt;/h3&gt;
&lt;p&gt;Similar to the twitter data, checking for null values is important but since the data is from Yahoo! Finance there are no missing values on the days that the markets are opened.&lt;/p&gt;
&lt;p&gt;Once graphing the open and closed prices of the NASDAQ, there seems to be an general upwards trend in the market over the time period.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-307/main/project/images/market.png&#34; alt=&#34;Figure 4 Open and Close Price of the NASDAQ&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 4:&lt;/strong&gt; Open and Close Price of the NASDAQ&lt;/p&gt;
&lt;h2 id=&#34;6-defining-events-during-trumps-presidency&#34;&gt;6. Defining events during Trump&amp;rsquo;s presidency&lt;/h2&gt;
&lt;h3 id=&#34;61-the-impeachment-of-president-trump&#34;&gt;6.1 The Impeachment of President Trump&lt;/h3&gt;
&lt;p&gt;After weeks of talks among Congress, the House of Representatives have voted to impeach President Trump on two charges: abuse of power and obstruction of Congress on December 18, 2019. Since the country&amp;rsquo;s founding in 1776, only three presidents have faced impeachment from Congress: Andrew Johnson, Bill Clinton and now Donald Trump. This move has been widely advocated for since his election in 2016. &amp;ldquo;In September 2019, news leaked of a phone call between President Trump and Ukrainian President Volodymyr Zelensky regarding an investigation into Hunter Biden, son of then Democratic candidate Joe Biden, for his dealings in Ukraine&amp;rdquo; [^3]. The sentiment analysis preformed on tweets that fall in the time frame of President Trump&amp;rsquo;s impeachment. The graph shows an almost even split in polarity but more tweets have a higher subjectivity suggesting many tweets are personal opinions. The NASDAQ shows stock were down when the Articles of Impeachment were announced but when the vote to approve the articles started in mid December, the stock price is on an upward trend.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-307/main/project/images/impeachment_sentiment.png&#34; alt=&#34;Figure 5: Sentiment Analysis during Impeachment&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 5:&lt;/strong&gt; Sentiment Analysis during Impeachment&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-307/main/project/images/impeachment_stock.png&#34; alt=&#34;Figure 6: Open and Close Price during Impeachment&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 6:&lt;/strong&gt; Open and Close Price during Impeachment&lt;/p&gt;
&lt;h3 id=&#34;62-the-dakota-access-and-keystone-xl-pipelines-approval&#34;&gt;6.2 The Dakota Access and Keystone XL pipelines approval&lt;/h3&gt;
&lt;p&gt;One of the first moves President Trump made when arriving into office was to approve the Dakota Access and Keystone XL pipelines. &amp;ldquo;Both of the pipelines were blocked by the Obama administration due to environmental concerns, but President Trump has questioned climate change and promised to expand energy infrastructure and create jobs&amp;rdquo;[^4]. The Keystone pipeline would span 1,200 miles across six states, moving over 800,000 barrels of oil daily from Canada to the Gulf coast. The Dakota Access pipeline would move oil from North Dakota all the way to Illinois. &amp;ldquo;The Standing Rock Sioux tribe, whose reservation is adjacent to the pipeline, staged protests that drew thousands of climate change activists to the rural area of Cannon Ball, North Dakota&amp;rdquo; [^4]. The sentiment graph shows tweets have a higher polarity but subjectivity is not as high as other events. In the days leading up to the signing of the pipelines on January 24th, the stock price has an upward trend but takes a sharp drop a couple days later possibly due to protests against the pipelines.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-307/main/project/images/dakota_sentiment.png&#34; alt=&#34;Figure 7: Sentiment Analysis during Dakota Approval&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 7:&lt;/strong&gt; Sentiment Analysis during Dakota Approval&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-307/main/project/images/dakota_stock.png&#34; alt=&#34;Figure 8: Open and Close Price during Dakota Approval&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 8:&lt;/strong&gt; Open and Close Price during Dakota Approval&lt;/p&gt;
&lt;h3 id=&#34;63-the-government-shutdown&#34;&gt;6.3 The Government Shutdown&lt;/h3&gt;
&lt;p&gt;On December 21, 2018 the United States Government shutdown. &amp;ldquo;At the heart of the dispute is Trump&amp;rsquo;s demand for just over $5 billion toward a long-promised wall along the US-Mexico border&amp;rdquo; [^5]. The shutdown affected a part of the federal government such as homeland security, transportation, and agriculture. &amp;ldquo;The problems caused by the shutdown are wide-ranging, from waste piling up in national parks to uncertainty for 800,000 federal workers about when their next paycheck will come&amp;rdquo; [^5]. This shutdown was the longest shutdown in the modern era coming to an end on January 25, 2019 after 35 days. The sentiment graph tells that the tweets shared during the shutdown are skewed to the right in terms of polarity with a majority of tweets being at or around 0.6 in subjectivity. Prior to the government shutdown, stock prices fell by 10 dollars with the lowest being around the time the shutdown began. The new year shows stock on a steady increase during the month of January, when the shutdown was lifted.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-307/main/project/images/shutdown_sentiment.png&#34; alt=&#34;Figure 9: Sentiment Analysis during the Government Shutdown&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 9:&lt;/strong&gt; Sentiment Analysis during the Government Shutdown&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-307/main/project/images/shutdown_stock.png&#34; alt=&#34;Figure 10: Open and Close Price during the Government Shutdown&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 10:&lt;/strong&gt; Open and Close Price during the Government Shutdown&lt;/p&gt;
&lt;h2 id=&#34;7-conclusion&#34;&gt;7. Conclusion&lt;/h2&gt;
&lt;p&gt;The investigation into the relation between President Trump&amp;rsquo;s tweets and the NASDAQ during certain events. The results showed that a majority of tweets were positive in polarity with subjectivity being higher or sometimes lower depending on the event. The NASDAQ had some interesting reactions based on the events. In highly important events, the stock price tended to have an upward trajectory but leading up to the event the price would go down. These results show that the content of the President&amp;rsquo;s tweets have some impact in terms of the market movements, but many factors go into the price of the market such as foreign relations and how companies are preforming. Finally, its worth s worth mentioning that the analysis doesn’t take into account some factors. Weekends were a factor that was not included into the stock market data. Tweets from the President&amp;rsquo;s official account were not taken into account in this analysis. All of these remaining areas can be added in future research.&lt;/p&gt;
&lt;h2 id=&#34;8-references&#34;&gt;8. References&lt;/h2&gt;
&lt;p&gt;[^3] President Donald Trump impeached, History.com, 05-Feb-2020. [online]. Available at: &lt;a href=&#34;https://www.history.com/this-day-in-history/president-trump-impeached-house-of-representatives&#34;&gt;https://www.history.com/this-day-in-history/president-trump-impeached-house-of-representatives&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;[^4] D. Smith and A. Kassam, Trump orders revival of Keystone XL and Dakota Access pipelines, The Guardian, 24-Jan-2017. [online]. Available at: &lt;a href=&#34;https://www.theguardian.com/us-news/2017/jan/24/keystone-xl-dakota-access-pipelines-revived-trump-administration&#34;&gt;https://www.theguardian.com/us-news/2017/jan/24/keystone-xl-dakota-access-pipelines-revived-trump-administration&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;[^5] Bryan, B., The government shutdown is now the longest on record and the fight between Trump and Democrats is only getting uglier. Here&amp;rsquo;s everything you missed. 21-Jan-2019. [online]. Available at: &lt;a href=&#34;https://www.businessinsider.com/government-shutdown-timeline-deadline-trump-democrats-2019-1&#34;&gt;https://www.businessinsider.com/government-shutdown-timeline-deadline-trump-democrats-2019-1&lt;/a&gt;.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Goel, A. and Mittal, A., 2011. Stock Prediction Using Twitter Sentiment Analysis. [online] cs229.stanford.edu. Available at: &lt;a href=&#34;http://cs229.stanford.edu/proj2011/GoelMittal-StockMarketPredictionUsingTwitterSentimentAnalysis.pdf&#34;&gt;http://cs229.stanford.edu/proj2011/GoelMittal-StockMarketPredictionUsingTwitterSentimentAnalysis.pdf&lt;/a&gt;. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;J. Bollen, H. Mao, and X. Zeng, Twitter mood predicts the stock market. Journal of Computational Science, vol. 2, no. 1, pp. 1–8, 2011. &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-308/hw7/task_3_next_steps/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-308/hw7/task_3_next_steps/</guid>
      <description>
        
        
        &lt;h1 id=&#34;next-steps&#34;&gt;Next Steps&lt;/h1&gt;
&lt;p&gt;I am still not 100% that this is the project I want to complete. As the instructions for HW 5 laid out, we do not have to fully commit at this point to the project. I may try to work on a basic deep learning project that can introduce me to that type of work. Next steps for the project I have started here would be to complete the basic descriptive data modeling in charts. Then would be to model the data and pull in the playoff teams from 2009-2019 to compare the model output with the playoff team that qualified for the playoffs. I want to work on this over the next month before moving onto the writing portion of the project.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-308/project/project/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-308/project/project/</guid>
      <description>
        
        
        &lt;h1 id=&#34;nfl-regular-season-skilled-position-player-performance-as-a-predictor-of-playoff-appearance-overtime&#34;&gt;NFL Regular Season Skilled Position Player Performance as a Predictor of Playoff Appearance Overtime&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-308/actions&#34;&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-308/workflows/Check%20Report/badge.svg&#34; alt=&#34;Check Report&#34;&gt;&lt;/a&gt;
&lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-308/actions&#34;&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-308/workflows/Status/badge.svg&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;
Status: final&lt;/p&gt;
&lt;p&gt;Travis Whitaker, &lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-308&#34;&gt;fa20-523-308&lt;/a&gt;, &lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-308/blob/main/project/project.md&#34;&gt;Edit&lt;/a&gt;&lt;/p&gt;


&lt;div class=&#34;pageinfo pageinfo-primary&#34;&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;The present research investigates the value of in-game performance metrics for NFL skill position players (i.e., Quarterback, Wide Receiver, Tight End, Running Back and Full Back) in predicting post-season qualification. Utilizing nflscrapR-data that collects all regular season in-game performance metrics between 2009-2018, we are able to analyze the value of each of these in-game metrics by including them in a regression model that explores each variables strength in predicting post-season qualification. We also explore a comparative analysis between two time periods in the NFL (2009-2011 vs 2016-2018) to see if there is a shift in the critical metrics that predict post-season qualification for NFL teams. Theoretically, this could help inform the debate as to whether there has been a shift in the style of play in the NFL across the previous decade and where those changes may be taking place according to the data. Implications and future research are discussed.&lt;/p&gt;
&lt;p&gt;Contents&lt;/p&gt;
&lt;div class=&#34;toc&#34;&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#1-introduction&#34;&gt;1. Introduction&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#2-background-research-and-previous-work&#34;&gt;2. Background Research and Previous Work&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#3-choice-of-data-sets&#34;&gt;3. Choice of Data-sets&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#4-methodology&#34;&gt;4. Methodology&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#5-results&#34;&gt;5. Results&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#inference&#34;&gt;Inference&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#preliminary-results&#34;&gt;Preliminary Results&lt;/a&gt;
          &lt;ul&gt;
            &lt;li&gt;&lt;a href=&#34;#2009-2011-skill-position-player-performance-as-playoff-predictor&#34;&gt;2009-2011 Skill Position Player Performance as Playoff Predictor&lt;/a&gt;&lt;/li&gt;
            &lt;li&gt;&lt;a href=&#34;#2016-2018-skill-position-player-performance-as-playoff-predictor&#34;&gt;2016-2018 Skill Position Player Performance as Playoff Predictor&lt;/a&gt;&lt;/li&gt;
          &lt;/ul&gt;
        &lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#comparative-results&#34;&gt;Comparative Results&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#6-discussion&#34;&gt;6. Discussion&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#limitations-and-future-research&#34;&gt;Limitations and Future Research&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#7-conclusion&#34;&gt;7. Conclusion&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#8-acknowledgements&#34;&gt;8. Acknowledgements&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#9-references&#34;&gt;9. References&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Keywords:&lt;/strong&gt; ANOVA, Comparative Analysis, Exploratory Analysis, Football, Sports, Metrics&lt;/p&gt;
&lt;h2 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;In the modern NFL the biggest negotiating tools for players in signing a new contract is their on-field performance. Many players choose to &amp;ldquo;hold-out&amp;rdquo; of pre-season practice or regular season games as a negotiating tool in their attempt to sign a more lucrative contract. In this situation players feel as though their exceptional performance on the field is not reflected in the monetary compensation structure of their contract. This is most often reflected in skill position players such as wide receivers or running backs whose play on the field is most often celebrated (e.g., touchdowns) and discussed by fans of the game. While these positions are no doubt important to a team’s success, the question remains how important is one players contribution to a team’s overall success? The current project will attempt to evaluate the importance of skill position players&amp;rsquo; (i.e., Quarterback (QB), Wide Receiver (WR), Running Back (RB), and Tight End (TE)) performance during the regular season and use in-game performance metrics as a predictive factor for their team making the playoffs. This is an attempt to answer the question can qualifying for the post-season be predicted by skill position metrics measured during the regular season? If so, then which metrics are most crucial to predicting post-season qualification?&lt;/p&gt;
&lt;p&gt;A secondary analysis in this project will look at a comparison between 2009-2011 vs. 2016-2018 regular season metrics and build separate models for each three year span to investigate whether a shift in performance metric importance has occurred over the past decade. NFL insiders and journalists have noted the shift in play-calling over the past decade in the NFL as well as the change at the quarterback position to more &amp;ldquo;dual-threat&amp;rdquo; (running and throwing) quarterbacks that have transitioned the game to a more &amp;ldquo;aggressive&amp;rdquo; play-style &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. Yet punditry and reporting have not always been supported by performance metrics and this specific claim of a transition over the past decade needs some exploring. Therefore, we will be investigating whether there has been a shift in the performance metrics that are important in predicting team success. Again, team success will be measured by making the post-season.&lt;/p&gt;
&lt;h2 id=&#34;2-background-research-and-previous-work&#34;&gt;2. Background Research and Previous Work&lt;/h2&gt;
&lt;p&gt;There are many playoff predictor models that focus on team performance or team wins vs losses as a predictor of making the playoffs &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. However, few take into consideration individual player performance as an indicator of their team making the post-season playoffs in the NFL. The most famous model that takes into consideration player performance is ELO rating &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;. The first ELO rating was a straightforward model that took head-to-head results and player-vs-team model to predict win probability in an NFL game. However, in 2019 Silver and his team at FiveThirtyEight updated their ELO model to give a value rating to the quarterback position for each team. This quarterback value included metrics such as pass attempts, completions, passing yards, passing touchdowns, interceptions, sacks, rush attempts, rushing yards, and rushing touchdowns. Taking these metrics along with the defensive quality metrics, which is an adjustment of quarterback value based on the opposing defense ranking, gives you an overall value for your quarterback. Thus, this widely accepted model takes head-to-head team comparisons on a week-to-week basis and includes the quarterback value in predicting the winners of these head-to-head matchups. However, no model has taken just player performance and tried to predict team success for an entire regular season based on each of their individual players. These previous models primarily look at offensive vs. defensive units and try to predict win/loss records based off each of these units.&lt;/p&gt;
&lt;p&gt;The goal of the present research is not to compare our model vs previous models, as these standing models are not meant for playoff prediction, rather these previous models are used for a game-by-game matchup comparison. The present research investigates whether looking at position players at each of the skilled positions, maps onto predicting the post-season qualifying teams. Further, how does this predictive model change over time? Looking at 2009-2011 NFL skill player performance vs 2016-2018 skill player performance we will investigate if there are differences in metrics that predict post-season appearance. This will show us whether there are shifts in skill position play that impact predicting post-season playoff appearance. Specifically, by comparing the two models which use two different periods of time (i.e., 2009-2011 vs 2016-2018) we will be able to better investigate specific metrics at each position that are important for predicting success. For example, if the wide receiver position is important, what is most important about that position, yards-after-catch or number of receptions? Further, how might the importance of those metrics shift over time? We hope to explore and understand better the impact of skill players and the metrics that they are measured on in terms of making the post-season.&lt;/p&gt;
&lt;h2 id=&#34;3-choice-of-data-sets&#34;&gt;3. Choice of Data-sets&lt;/h2&gt;
&lt;p&gt;The datasets used are in a github folder that holds nflscrapR-data that originates from NFL.com &lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;. The folder includes play-by-play data, including performance measures, for all regular season games from 2009 to 2019. This file was paired with week-by-week regular season roster data for each team in the NFL. This allowed us to track skilled position player performance during the regular season and then compare this regular season file with the files that contain playoff teams for each year from 2009-2019. Supplemental data was pulled from Pro-Football-Reference.com &lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h2 id=&#34;4-methodology&#34;&gt;4. Methodology&lt;/h2&gt;
&lt;p&gt;The first step we took to understand the data was to use various slices of the data put into scatterplots and bar charts to find trends, as well as various time series charts. This was an exploratory step in understanding the data.&lt;/p&gt;
&lt;p&gt;Then each metric from player performance during the regular season was included in the analysis or models that were built to predict post-season appearance. Post-season appearance is a designation for a team qualifying for the post-season or playoffs. We think it is important to engineer some new features to potentially provide insights.  For instance, it is possible to determine whether a play was during the final two minutes of a half and if a play was in the red zone. During these critical points of a game a win or lose is often determined. Our thought is by weighing these moments and performance metrics with more importance the model will better predict a team’s likelihood of making the playoffs. Another secondary metric that may strengthen the predictive ability of the model would be to use Football Outsider’s Success Rate, which is a determination of a play’s success rate for the offense that is on the field &lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;. This can also provide me with the down and distance to go for the offense and players that are on the field. We will also use college position designations as way to normalize the positions performance across teams. Many NFL teams utilize different player sets. Thus, it is important to use a standard, which college football uses across all teams. Since we are only interested in skill position players this will include Wide Receiver (WR), Running Back (RB), Full Back (FB), Quarterback (QB), and Tight End (TE). These designations will allow the model to compare player performance by position.&lt;/p&gt;
&lt;p&gt;After breaking down the data into key categorical variables to see if there was an impact for these performance variables in making the playoffs for the NFL teams. These individual position statistics were analyzed as a group and then separated into &amp;ldquo;Post Season&amp;rdquo; meaning the player&amp;rsquo;s team qualified for the playoffs, or &amp;ldquo;No Post&amp;rdquo; meaning the player&amp;rsquo;s team did not qualify for the playoffs. By doing this we were able to verify that a reasonable number of players fell into the &amp;ldquo;Post Season&amp;rdquo; group, as only 12 out of 32 teams qualify for the post-season. Further we were able to use these designations in the next step of modeling, where the data was analyzed in an ANOVA to see how important each metric was in predicting post-season appearance for each player.&lt;/p&gt;
&lt;p&gt;Metric measurement needs to be consistent across years. A comparison of year-to-year metrics was completed comparing each years measurements from 2009-2011 and 2016-2018 in order to make sure that the measurement techniques were stable and do not vary across time. If there were changes in the way metrics are measured than either that year will need to be dropped from the model or adjustments will need to be made to the metric to balance it with the other years included in the model. Luckily, there were no variation in metric measurement across years, so all measurements initially included in the model were kept.&lt;/p&gt;
&lt;p&gt;Finally, once all metrics were balanced and the team performance metrics had been aggregated. The ANOVA model was built to analyze metric measurement as a predictor of a player making post-season play. This ANOVA model was created twice, once for the 2009-2011 players and then again for the 2016-2018 players. Once this analysis was run, we were able to see the strength of the model in predicting playoff appearance by player, based on metric measurement.&lt;/p&gt;
&lt;h2 id=&#34;5-results&#34;&gt;5. Results&lt;/h2&gt;
&lt;h3 id=&#34;inference&#34;&gt;Inference&lt;/h3&gt;
&lt;p&gt;An individual player-performance model for NFL skill position players (i.e., quarterback, wide receiver, tight end, and running back) will perform better than chance level (50%) of identifying playoff teams from the NFL during the season&amp;rsquo;s of 2009-2011 and 2016-2018. Further, an exploratory analysis of time periods (2009-2011 vs 2016-2018) will expose differences in the key metrics that are used to predict playoff appearance over time. This will give us a better understanding of the shifts in performance in the NFL at each skill position.&lt;/p&gt;
&lt;h3 id=&#34;preliminary-results&#34;&gt;Preliminary Results&lt;/h3&gt;
&lt;p&gt;The descriptive statistics for the player performance revealed no issues with normality or different metric standards across seasons for player performance measurements. Figure 1 represents a count check for players qualifying for the post-season in the seasons of 2009-2011. Based on the NFL post-season structure 12 out of 32 teams qualify for the post-season, or 37.5%. Figure 1 shows that roughly 1/3 of players qualify for the post season at each position. However, it is important to note that each team structure and roster is different. For example, one team may carry 7 receivers, 2 running backs, 2, quarter backs, 2 tight ends, and 0 full backs, where another team may carry 4 receivers, 4 running backs, 3 quarter backs, 4 tight ends, and 1 full back. This is an important distinction to make because the &amp;ldquo;Post Season&amp;rdquo; players shown in figure 1 are not at an equal percentage across position.&lt;/p&gt;
&lt;h4 id=&#34;2009-2011-skill-position-player-performance-as-playoff-predictor&#34;&gt;2009-2011 Skill Position Player Performance as Playoff Predictor&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-308/raw/main/project/images/Player_summary_2009_2011.png&#34; alt=&#34;Player Qualifying for Post-Season&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Breakdown of Players by Skill Position That Qualified for Post-Season Play (2009-2011)&lt;/p&gt;
&lt;p&gt;We also wanted to investigate whether play-count at each position was balanced across post-season players and players who did not qualify for the post-season. Figure 2 shows that players on teams who did qualify for the post-season were involved in more plays at their position than players at their position who did not qualify for the playoffs. Thinking about this finding as a result of the regular season, players in skilled positions on post-season qualifying teams play on offenses that won more games than teams who did not qualify for the playoffs. While we did not look at time of possession for players by position, it seems fairly reasonable through logic and the results in figure 2 that play count is higher because these teams are more successful and the players on post-season qualifying teams are on the field more than teams with more losses who did not qualify for the post-season.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-308/raw/main/project/images/Play_count_position_2009.png&#34; alt=&#34;Count of Play-type by Post-Season Qualification category&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 2:&lt;/strong&gt; Count of Play-type by Post-Season Qualification category (2009-2011)&lt;/p&gt;
&lt;p&gt;Figure 3 below is a reference table for the features included in the ANOVA regression model determining the key features that predict post-season qualification. These features are used for reference in figures 4 and 7.&lt;/p&gt;
&lt;p&gt;Using the player performance metrics for the regular season, an ANOVA was run to see if these metrics placed together would be a successful predictor of post-season qualification. Figure 4 shows the top 10 metrics that had the highest f-values for predicting post-season qualification, all of which were all significant (p&amp;lt;.05) in the model. Reviewing the model, the most significant factors for predicting post-season qualification for teams in order were; 1. Successful Reception (wide receiver or tight end), 2. Total Receiving Yards (Wide Receiver or Tight End), 3. Yards After Catch (wide receiver or tight end), 4. Total Receiving Touchdowns (Wide Receiver or Tight End), 5. Total Touchdowns (All positions), 6. Receiver Plays (Wide Receiver), 7. Redzone Plays (All positions), 8. Successful Plays (All positions), 9. Yards Gained (All positions), 10. Total Offensive Plays in the 3rd Quarter (All positions). The model accounted for 78.2% of variance. The model successfully accounted for predicting post-season qualifying teams in 78.2% of instances.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-308/raw/main/project/images/figure_description_table.png&#34; alt=&#34;Feature Descriptions&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 3:&lt;/strong&gt; Description for top features included in ANOVA regression model&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-308/raw/main/project/images/Anova_pvalue_table_2009.png&#34; alt=&#34;ANOVA for Metric Importance in Model&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 4:&lt;/strong&gt; ANOVA Table for Metrics Measured as Predictors for Teams Qualifying for Post-Season Play (2009-2011)&lt;/p&gt;
&lt;h4 id=&#34;2016-2018-skill-position-player-performance-as-playoff-predictor&#34;&gt;2016-2018 Skill Position Player Performance as Playoff Predictor&lt;/h4&gt;
&lt;p&gt;We paralleled our analysis from the 2009-2011 analysis above in completing the 2016-2018 analysis represented in Figures 5-7. Figure 5 represents the player count that qualified for post-season play (orange) and the non-post-season players (blue). Again, player count was compared to the roughly 37.5% rate that should be expected for 12 teams out of 32 qualifying for post-season play. However, the rates were a bit below the 37.5% rate. This can be explained by the number of injuries and roster changes that occur throughout the season. Teams shuffle in-and-out players at each position based on injury or performance. Teams will not have a static roster throughout the season, this includes post-season teams who cut players or put players on IR. These players, even though they played for post-season qualifying teams, would be in blue because they are not on the post-season rosters for the teams who qualify for post-season play. This along with the roster structure described for figure 1 explains the lower than 1/3 rate of players qualifying for post-season play.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-308/raw/main/project/images/Player_summary_2016-2018.png&#34; alt=&#34;Player Qualifying for Post-Season&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 5:&lt;/strong&gt; Breakdown of Players by Skill Position That Qualified for Post-Season Play (2016-2018)&lt;/p&gt;
&lt;p&gt;Figure 6 investigates play-count at each position, like figure 2, but this time for the seasons of 2016-2018. Again, the analysis was balanced across post-season players and players who did not qualify for the post-season. Figure 6 shows that players on teams who did qualify for the post-season were involved in more plays at their position than players at their position who did not qualify for the playoffs. Figure 6 shows a consistent pattern with figure 2. Descriptive statistic comparison between the 2009-2011 seasons and the 2016-2018 seasons will be revisited in the discussion section.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-308/raw/main/project/images/Play_Count_Position_2016.png&#34; alt=&#34;Count of Play-type by Post-Season Qualification category&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 6:&lt;/strong&gt; Count of Play-type by Post-Season Qualification category (20016-2018)&lt;/p&gt;
&lt;p&gt;Using the player performance metrics for the regular season, an ANOVA was run to see if these metrics placed together would be a successful predictor of post-season qualification. Figure 7 shows the top 10 metrics that had the highest f-values for predicting post-season qualification, all of which were significant (p&amp;lt;.05) in the model. The model successfully accounted for predicting post-season qualifying teams in 77.8% of instances.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-308/raw/main/project/images/Anova_pvalue_table_2016.png&#34; alt=&#34;ANOVA for Metric Importance in Model&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 7:&lt;/strong&gt; ANOVA Table for Metrics Measured as Predictors for Teams Qualifying for Post-Season Play (2016-2018)&lt;/p&gt;
&lt;h3 id=&#34;comparative-results&#34;&gt;Comparative Results&lt;/h3&gt;
&lt;p&gt;Comparing the 2016-2018 with the 2009-2011 season model, certain shifts have occurred from the 2009-2011 seasons model. Namely, yards-after-catch has become the strongest predictor of post-season qualification, flipping positions with successful reception in the 2009-2011 model. Another notable shift is the importance of number of plays run in the second quarter in the 2016-2018 model, overtaking number of plays run in the third quarter from the 2009-2011. The models themselves also shift in their strength of prediction. The 2009-2011 model shows stronger predictive capability (78.2% vs 77.8%), which is reflected in the f-values for the top 10 metrics of the model. The 2009-2011 model has four variables with f-values above 50 and one above 60. The 2016-2018 model only has one variable with an f-value above 50. These values represent the variance accounted for in the model by a variable. The higher the f-value the more variance accounted for in the model by that specific variable. Since the f-values were so high for many of the top 10 variables listed in each model, the p-values showed highly significant far exceeding the p=.05 level that was needed. The f-values were high because they accounted for so much of the variance in the model, meaning the predictive nature of the model was due in large part to many of the variables in the top 10. Another way to state this is that each of these top 10 variables were significantly better at predicting post-season qualification than would be expected due to chance.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-308/raw/main/project/images/Anova_sig_features_2009.png&#34; alt=&#34;ANOVA Chart for Metric Importance in Model&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 8:&lt;/strong&gt; ANOVA Chart for Metrics Measured as Predictors for Teams Qualifying for Post-Season Play (2009-2011)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-308/raw/main/project/images/Anova_sig_features_2016.png&#34; alt=&#34;ANOVA Chart for Metric Importance in Model&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 9:&lt;/strong&gt; ANOVA Chart for Metrics Measured as Predictors for Teams Qualifying for Post-Season Play (2016-2018)&lt;/p&gt;
&lt;p&gt;Cloudmesh Comon &lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt; is used to create the benchmark.&lt;/p&gt;
&lt;h2 id=&#34;6-discussion&#34;&gt;6. Discussion&lt;/h2&gt;
&lt;p&gt;The first inference of this project was investigating the possibility of using in-game performance metrics as a competent and better-than-chance predictor of selecting skill position players making the NFL post-season. Both the 2009-2011 and the 2016-2018 season models were able to predict player post-season qualification at 78.2% and 77.8% levels of success, both above chance level. This success highlights the critical nature of skill performance players and provides confidence to the modern metric model of NFL players as a useful and qualified tool to evaluate player performance as a measure of success. This also gives clout to the skill position players who believe their contributions on the field are deserving of top dollar compensation in the NFL. According to our models, wide receivers are deserving of high compensation as their game play impacts the likelihood of their team making the playoff more than running backs. However, it is hard to discriminate whether quarterback play is also key to the success of wide receivers. It could well be that these two positions work hand-in-hand.&lt;/p&gt;
&lt;p&gt;Investigating the second inference regarding changes in the predictive model across time. In comparing the descriptive statistic models (figs. 1, 2 vs. 5, 6). There are some noticeable, but not significant differences in the two-time ranges. First, there are more receivers in the 2016-2018 time range, which reflects the NFL’s shift towards a more pass prone league. Since there was not an increase in roster size between the two-time ranges, the increase in receivers lead to a decrease in the number of quarterbacks and fullbacks on a roster, but these additional receivers carried probably took the roster spots of non-skill positions players that are not accounted for here. Both models show the importance of pass plays, successful pass plays, receiving touchdowns and yards, yards after catch, and other passing variables that highlight the importance of wide receivers and tight ends. The NFL has shifted towards a more pass-friendly league &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, and the models built here highlight the reasons why that occurred. Receiver plays are significantly more important in predicting post-season qualification than any other skill position metrics. It is likely that the shift towards receivers and away from running backs has taken place over time. It is possible that we have pulled two time periods that are too close together to reflect the shift in NFL play, and if we had pulled data from the 1990’s or 1980’s (unfortunately this data is not available in the needed metrics) we would see more running back heavy metrics at the tops of our models and significant changes in the two time periods.&lt;/p&gt;
&lt;h3 id=&#34;limitations-and-future-research&#34;&gt;Limitations and Future Research&lt;/h3&gt;
&lt;p&gt;Metrics are not provided for non-skill position players who could be critical in predicting playoff qualification. For instance, if we could include offensive linemen metrics, we would have a stronger model that would be better able to predict post-season qualification. Further, the NFL data we had access to does not measure defensive player metrics that we believe are critical in being able to predict post-season qualification for NFL teams. Future work should look to include defensive player metrics into their model, as well as non-skill position players to improve on this model.&lt;/p&gt;
&lt;p&gt;Though we were able to build a model to predict player qualification for the post-season, future research can build on this model by making a composite of players on a team to then predict a team making the playoffs. The present study is a nice first step in understanding the capabilities of game performance for predicting player success, but NFL teams are equally interested in a team&amp;rsquo;s success, not just individual skill players. Therefore, future research can build on this project by incorporating defensive player metrics, non-skill position offensive metrics, and composites of players on one team to predict a team&amp;rsquo;s projected chances of making the playoffs.&lt;/p&gt;
&lt;h2 id=&#34;7-conclusion&#34;&gt;7. Conclusion&lt;/h2&gt;
&lt;p&gt;Utilizing skill position performance metrics shows to be a successful predictor of player post-season qualification above chance level (50%). Further, there are slight shifts in which metrics are best at predicting post-season qualification between the 2009-2011 and 2016-2018 time periods. However, the key metrics that were significant in both models from the two time periods (2009-2011 and 2016-2018) did not change. Therefore, we cannot say definitively that there has been a shift in style of play from 2009 to 2018.&lt;/p&gt;
&lt;h2 id=&#34;8-acknowledgements&#34;&gt;8. Acknowledgements&lt;/h2&gt;
&lt;p&gt;Thank you to my friends and family who supported me through working on this project.&lt;/p&gt;
&lt;h2 id=&#34;9-references&#34;&gt;9. References&lt;/h2&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Seifert, K. (2020, June 18). How pro football has changed in the past 10 years: 12 ways the NFL evolved this decade. Retrieved November 17, 2020, from &lt;a href=&#34;https://www.espn.com/nfl/story/_/id/28373283/how-pro-football-changed-10-years-12-ways-nfl-evolved-decade&#34;&gt;https://www.espn.com/nfl/story/_/id/28373283/how-pro-football-changed-10-years-12-ways-nfl-evolved-decade&lt;/a&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Zita, C. (2020, September 16). Improving a Famous NFL Prediction Model. Retrieved November 02, 2020, from &lt;a href=&#34;https://towardsdatascience.com/improving-a-famous-nfl-prediction-model-1295a7022859&#34;&gt;https://towardsdatascience.com/improving-a-famous-nfl-prediction-model-1295a7022859&lt;/a&gt; &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Silver, N. (2018, September 05). How Our NFL Predictions Work. Retrieved November 02, 2020, from &lt;a href=&#34;https://fivethirtyeight.com/methodology/how-our-nfl-predictions-work/&#34;&gt;https://fivethirtyeight.com/methodology/how-our-nfl-predictions-work/&lt;/a&gt; &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Ryurko. Ryurko/NflscrapR-Data. 2 Mar. 2020, &lt;a href=&#34;https://github.com/ryurko/nflscrapR-data&#34;&gt;https://github.com/ryurko/nflscrapR-data&lt;/a&gt;. &lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Sports Reference, LLC. Pro Football Statistics and History.  Retrieved October 09, 2020. &lt;a href=&#34;https://www.pro-football-reference.com/&#34;&gt;https://www.pro-football-reference.com/&lt;/a&gt; &lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Gregor von Laszewski, Cloudmesh StopWatch and Benchmark from the Cloudmesh Common Library, &lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-common&#34;&gt;https://github.com/cloudmesh/cloudmesh-common&lt;/a&gt; &lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-309/project/project/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-309/project/project/</guid>
      <description>
        
        
        &lt;h1 id=&#34;analysis-of-various-machine-learning-classification-techniques-in-detecting-heart-disease&#34;&gt;Analysis of Various Machine Learning Classification Techniques in Detecting Heart Disease&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-309/actions&#34;&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-309/workflows/Check%20Report/badge.svg&#34; alt=&#34;Check Report&#34;&gt;&lt;/a&gt;
&lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-309/actions&#34;&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-309/workflows/Status/badge.svg&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;
Status: in progress&lt;/p&gt;
&lt;p&gt;Ethan Nguyen, &lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-309&#34;&gt;fa20-523-309&lt;/a&gt;, &lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-309/blob/main/project/project.md&#34;&gt;Edit&lt;/a&gt;&lt;/p&gt;


&lt;div class=&#34;pageinfo pageinfo-primary&#34;&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;As cardiovascular diseases are the number 1 cause of death in the United States, the study of the factors and early detection and treatment could improve quality of life and lifespans. From investigating how the variety of factors related to cardiovascular health relate to a general trend, it has resulted in general guidelines to reduce the risk of experiencing a cardiovascular disease. However, this is a rudimentary way of preventative care that allows for those who do not fall into these risk categories to fall through. By applying machine learning, one could develop a flexible solution to actively monitor, find trends, and flag patients at risk to be treated immediately. Solving not only the risk categories but has the potential to be expanded to annual checkup data revolutionizing health care.&lt;/p&gt;
&lt;p&gt;Contents&lt;/p&gt;
&lt;div class=&#34;toc&#34;&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#1-introduction&#34;&gt;1. Introduction&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#2-datasets&#34;&gt;2. Datasets&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#21-dataset-cleaning&#34;&gt;2.1 Dataset Cleaning&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#22-dataset-analysis&#34;&gt;2.2 Dataset Analysis&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#3-machine-learning-algorithms-and-implementation&#34;&gt;3. Machine Learning Algorithms and Implementation&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#31-scikit-learn-and-algorithm-types&#34;&gt;3.1 Scikit-Learn and Algorithm Types&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#32-classification-algorithms&#34;&gt;3.2 Classification Algorithms&lt;/a&gt;
          &lt;ul&gt;
            &lt;li&gt;&lt;a href=&#34;#321-support-vector-machines&#34;&gt;3.2.1 Support Vector Machines&lt;/a&gt;&lt;/li&gt;
            &lt;li&gt;&lt;a href=&#34;#322-k-nearest-neighbors&#34;&gt;3.2.2 K-Nearest Neighbors&lt;/a&gt;&lt;/li&gt;
            &lt;li&gt;&lt;a href=&#34;#323-gaussian-naive-bayes&#34;&gt;3.2.3 Gaussian Naive Bayes&lt;/a&gt;&lt;/li&gt;
            &lt;li&gt;&lt;a href=&#34;#324-decision-trees&#34;&gt;3.2.4 Decision Trees&lt;/a&gt;&lt;/li&gt;
          &lt;/ul&gt;
        &lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#33-clustering-algorithms&#34;&gt;3.3 Clustering Algorithms&lt;/a&gt;
          &lt;ul&gt;
            &lt;li&gt;&lt;a href=&#34;#331-k-means&#34;&gt;3.3.1 K-Means&lt;/a&gt;&lt;/li&gt;
            &lt;li&gt;&lt;a href=&#34;#332-mean-shift&#34;&gt;3.3.2 Mean-shift&lt;/a&gt;&lt;/li&gt;
            &lt;li&gt;&lt;a href=&#34;#333-spectral-clustering&#34;&gt;3.3.3 Spectral Clustering&lt;/a&gt;&lt;/li&gt;
          &lt;/ul&gt;
        &lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#34-implementation&#34;&gt;3.4 Implementation&lt;/a&gt;
          &lt;ul&gt;
            &lt;li&gt;&lt;a href=&#34;#341-dataset-preprocessing&#34;&gt;3.4.1 Dataset Preprocessing&lt;/a&gt;&lt;/li&gt;
          &lt;/ul&gt;
        &lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#4-results--discussion&#34;&gt;4. Results &amp;amp; Discussion&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#41-algorithm-metrics&#34;&gt;4.1 Algorithm Metrics&lt;/a&gt;
          &lt;ul&gt;
            &lt;li&gt;&lt;a href=&#34;#411-support-vector-machines&#34;&gt;4.1.1 Support Vector Machines&lt;/a&gt;&lt;/li&gt;
            &lt;li&gt;&lt;a href=&#34;#412-k-nearest-neighbors&#34;&gt;4.1.2 K-Nearest Neighbors&lt;/a&gt;&lt;/li&gt;
            &lt;li&gt;&lt;a href=&#34;#413-gaussian-naive-bayes&#34;&gt;4.1.3 Gaussian Naive Bayes&lt;/a&gt;&lt;/li&gt;
            &lt;li&gt;&lt;a href=&#34;#414-decision-trees&#34;&gt;4.1.4 Decision Trees&lt;/a&gt;&lt;/li&gt;
            &lt;li&gt;&lt;a href=&#34;#415-k-means&#34;&gt;4.1.5 K-Means&lt;/a&gt;&lt;/li&gt;
            &lt;li&gt;&lt;a href=&#34;#416-mean-shift&#34;&gt;4.1.6 Mean-shift&lt;/a&gt;&lt;/li&gt;
            &lt;li&gt;&lt;a href=&#34;#417-spectral-clustering&#34;&gt;4.1.7 Spectral Clustering&lt;/a&gt;&lt;/li&gt;
          &lt;/ul&gt;
        &lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#42-discussion&#34;&gt;4.2 Discussion&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#5-conclusion&#34;&gt;5. Conclusion&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#6-acknowledgements&#34;&gt;6. Acknowledgements&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Keywords:&lt;/strong&gt; health, healthcare, cardiovascular disease, data analysis&lt;/p&gt;
&lt;h2 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;Since cardiovascular diseases are the number 1 cause of death in the United States, early prevention could help in extending one’s life span and possibly quality of life &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. Since there are cases where patients do not show any signs of cardiovascular trouble until an event occurs, having an algorithm predict from their medical history would help in picking up on early warning signs a physician may overlook. Or could also reveal additional risk factors and patterns for research on prevention and treatment. In turn this would be a great tool to apply in preventive care, which is the type of healthcare policy that focuses in diagnosing and preventing health issues that would otherwise require specialized treatment or is not treatable &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. This also has the potential to trickle down and increase the quality of life and lifespan of populations at a reduced cost as catching issues early most likely results in cheaper treatments &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;This project will take a high-level overview of common, widely available classification algorithms and analyze their effectiveness for this specific use case. Notable ones include, Gaussian Naive Bayes, K-Nearest Neighbors, and Support Vector Machines. Additionally, two data sets that contain common features will be used to increase the training and test pool for evaluation. As well as to explore if additional feature types contribute to a better prediction. The goal of this project being a gateway to further research in data preprocessing, tuning, or development of specialized algorithms as well as further ideas on what data could be provided.&lt;/p&gt;
&lt;h2 id=&#34;2-datasets&#34;&gt;2. Datasets&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/johnsmith88/heart-disease-dataset&#34;&gt;https://www.kaggle.com/johnsmith88/heart-disease-dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/sulianova/cardiovascular-disease-dataset&#34;&gt;https://www.kaggle.com/sulianova/cardiovascular-disease-dataset&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The range of creation dates are 1988 and 2019 respectively with different features of which 4 are common between. This does bring up a small hiccup in preprocessing to consider. Namely the possibility of changing diet and culture trends resulting in significantly different trends/patterns within the same age group. As well as possible differences in measurement accuracy. However this large gap is within the scope of the project in exploring which features can help provide an accurate prediction.&lt;/p&gt;
&lt;p&gt;This possible phenomenon may be of interest to explore closely if time allows. Whether a trend itself is even present or there is an overarching trend across different cultures and time periods. Or to consider if this difference is significant enough that the data from the various sets needs to be adjusted to normalize the ages to present day.&lt;/p&gt;
&lt;h3 id=&#34;21-dataset-cleaning&#34;&gt;2.1 Dataset Cleaning&lt;/h3&gt;
&lt;p&gt;The datasets used have already been significantly cleaned from the raw data and has been provided as a csv file. These files were then imported into the python notebook as pandas dataframes for easy manipulation.&lt;/p&gt;
&lt;p&gt;An initial check was made to ensure the integrity of the data matched the description from the source websites. Then some preprocessing was completed to normalize the common features between the datasets. These features were gender, age, and cholesterol levels. The first two adjustments were trivial in conversion however, in the case of cholesterol levels, the 2019 set is on a 1-3 scale while the 1988 dataset provided them as real measurements. A conversion of the 1988 dataset was done based on guidelines found online for the age range of the dataset &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h3 id=&#34;22-dataset-analysis&#34;&gt;2.2 Dataset Analysis&lt;/h3&gt;
&lt;p&gt;From this point on, the 1988 dataset will be referred to as &lt;code&gt;dav_set&lt;/code&gt; and 2019 data set will be referred to as &lt;code&gt;sav_set&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To provide further insight on what to expect and how a model would be applied, the population of the datasets was analysed first. As depicted in Figure 2.1 the population samples of both datasets of gender vs age show the majority of the data is centered around 60 years of age with a growing slope from 30 onwards.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/agevssex.jpg&#34; alt=&#34;Figure 2.1&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 2.1&lt;/strong&gt;: Age vs Gender distributions of the dav_set and sav_set.&lt;/p&gt;
&lt;p&gt;This trend appears to signify that the datasets focused solely on an older population or general trend in society of not monitoring heart conditions as closely in the younger generation.&lt;/p&gt;
&lt;p&gt;Moving on to Figure 2.2, we see an interesting trend with a significant growing trend in the sav_set in older population having more cardiovascular issues compared to the dav_set. While this cannot be seen in the dav_set. This may be caused by the additional life expectancy or a change in diet as noted in the introduction.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/agevstarget.jpg&#34; alt=&#34;Figure 2.2&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 2.2&lt;/strong&gt;: Age vs Target distributions of the dav_set and sav_set.&lt;/p&gt;
&lt;p&gt;In Figure 2.3, the probability of having cardiovascular issues between the sets are interesting. In the dav_set the inequality of higher probability could be attributed to the larger female samples in the dataset. With the sav_set having a more equal probability between the genders.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/gendervsprobability.jpg&#34; alt=&#34;Figure 2.3&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 2.3&lt;/strong&gt;: Gender vs Probability of cardiovascular issues of the dav_set and sav_set.&lt;/p&gt;
&lt;p&gt;Finally, in Figure 2.4 is the probability vs cholesterol levels. This one is very interesting between the two datasets in terms of trend levels. With the dav_set having a higher risk at normal levels compared to the sav_set. This could be another hint of a societal change across the years or may in fact be due to the low sample size. Especially since the sav_set matches the general consensus of higher cholesterol levels increasing risk of cardiovascular issues &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/cholesterolvsprobability.jpg&#34; alt=&#34;Figure 2.4&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 2.4&lt;/strong&gt;: Cholesterol levels vs Probability of cardiovascular issues of the dav_set and sav_set.&lt;/p&gt;
&lt;p&gt;To close out this initial analysis is the correlation map of each of the features. From Figure 2.5 and 2.6 it can be concluded that both of these datasets are viable to conduct machine learning as the correlation factor is below the recommended value of 0.8 &lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;. Although we do see the signs of a low sample amount in the dav_set with a higher correlation factor compared to the sav_set.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/davsetcorrelation.jpg&#34; alt=&#34;Figure 2.5&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 2.5&lt;/strong&gt;: dav_set correlation matrix.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/savsetcorrelation.jpg&#34; alt=&#34;Figure 2.6&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 2.6&lt;/strong&gt;: sav_set correlation matrix.&lt;/p&gt;
&lt;h2 id=&#34;3-machine-learning-algorithms-and-implementation&#34;&gt;3. Machine Learning Algorithms and Implementation&lt;/h2&gt;
&lt;p&gt;With many machine learning algorithms already available and many more in development. Selecting the optimal one for an application can be a challenging balance since each algorithm has both its advantages and disadvantages. As mentioned in the introduction, we will explore applying the most common and established algorithms available to the public.&lt;/p&gt;
&lt;p&gt;Starting off, is selecting a library from the most popular ones available. Namely Keras, Pytorch, Tensorflow, and Scikit-Learn. Upon further investigation it was determined that Scikit-Learn would be used for this project. The reason being Scikit-Learn is a great general machine learning library that also includes pre and post processing functions. While Keras, Pytorch, and Tensorflow are targeted for neural networks and other higher-level deep learning algorithms which are outside of the scope of this project at this time &lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h3 id=&#34;31-scikit-learn-and-algorithm-types&#34;&gt;3.1 Scikit-Learn and Algorithm Types&lt;/h3&gt;
&lt;p&gt;Diving further into the Scikit-Learn library, its key strength appears to be the variety of algorithms available that are relatively easy to implement against a dataset. Of those available, they are classified under three different categories based on the approach each takes. They are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Classification
&lt;ul&gt;
&lt;li&gt;Applied to problems that require identifying the category an object belongs to.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Regression
&lt;ul&gt;
&lt;li&gt;For predicting or modeling continuous values.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Clustering
&lt;ul&gt;
&lt;li&gt;Grouping similar objects into groups.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For this project, we will be investigating the Classification and Clustering algorithms offered by the library due to the nature of our dataset. Since it is a binary answer, the continuous prediction capability of regression algorithms will not fair well. Compared to classification type algorithms which are well suited for determining binary and multi-class classification on datasets &lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;. Along with Clustering algorithms being capable of grouping unlabeled data which is one of the key problem points mentioned in the introduction &lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h3 id=&#34;32-classification-algorithms&#34;&gt;3.2 Classification Algorithms&lt;/h3&gt;
&lt;p&gt;The following algorithms were determined to be candidates for this project based on the documentation available on the Scikit-learn for supervised learning &lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h4 id=&#34;321-support-vector-machines&#34;&gt;3.2.1 Support Vector Machines&lt;/h4&gt;
&lt;p&gt;This algorithm was chosen because classification is one of the target types and has a decent list of advantages that appear to be applicable to this dataset &lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Effective in high dimensional spaces as well as if the number dimensions out number samples.&lt;/li&gt;
&lt;li&gt;Is very versatile.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;322-k-nearest-neighbors&#34;&gt;3.2.2 K-Nearest Neighbors&lt;/h4&gt;
&lt;p&gt;This algorithm was selected due to being a non-parametric method that has been successful in classification applications &lt;sup id=&#34;fnref:9&#34;&gt;&lt;a href=&#34;#fn:9&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;. From the dataset analysis, it is appears that the decision boundary may be very irregular which is a strong point of this type of method.&lt;/p&gt;
&lt;h4 id=&#34;323-gaussian-naive-bayes&#34;&gt;3.2.3 Gaussian Naive Bayes&lt;/h4&gt;
&lt;p&gt;Is an implementation of the Naive Bayes theorem that has been targeted for classification. The advantages of this algorithm is its speed and requires a small training set compared to more advanced algorithms &lt;sup id=&#34;fnref:10&#34;&gt;&lt;a href=&#34;#fn:10&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;10&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h4 id=&#34;324-decision-trees&#34;&gt;3.2.4 Decision Trees&lt;/h4&gt;
&lt;p&gt;This algorithm was chosen to investigate another non-parametric method to determine their efficacy against this dataset application. This algorithm also has some advantages over K-Nearest namely &lt;sup id=&#34;fnref:11&#34;&gt;&lt;a href=&#34;#fn:11&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;11&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Simple to interpret and visualize&lt;/li&gt;
&lt;li&gt;Requires little data preparation
&lt;ul&gt;
&lt;li&gt;Handles numerical and categorical data instead of needing to normalize&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Can validate the model and is possible to audit from a liability standpoint.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;33-clustering-algorithms&#34;&gt;3.3 Clustering Algorithms&lt;/h3&gt;
&lt;p&gt;The following algorithms were determined to be candidates for this project based on the table of clustering algorithms available on the Scikit-learn &lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h4 id=&#34;331-k-means&#34;&gt;3.3.1 K-Means&lt;/h4&gt;
&lt;p&gt;The usecase for this algorithm is general purpose with even and low number of clusters &lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;. Of which the sav_set appears to have with the even distribution across most of the features.&lt;/p&gt;
&lt;h4 id=&#34;332-mean-shift&#34;&gt;3.3.2 Mean-shift&lt;/h4&gt;
&lt;p&gt;This algorithm was chosen for its strength in dealing with uneven cluster sizes and non-flat geometry &lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;. Though it is not easily scalable the application of our small dataset size might be of interest.&lt;/p&gt;
&lt;h4 id=&#34;333-spectral-clustering&#34;&gt;3.3.3 Spectral Clustering&lt;/h4&gt;
&lt;p&gt;As an inverse, this algorithm was chosen for its strength with fewer uneven clusters &lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;. In comparison to Mean-shift, this maybe the better algorithm for this application.&lt;/p&gt;
&lt;h3 id=&#34;34-implementation&#34;&gt;3.4 Implementation&lt;/h3&gt;
&lt;p&gt;The implementation of these algorithms were done under the direction of the documentation page for each respective algorithm. The jupyter notebook used for this project is available at &lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-309/blob/main/project/data_analysis/ml_algorithms.ipynb&#34;&gt;https://github.com/cybertraining-dsc/fa20-523-309/blob/main/project/data_analysis/ml_algorithms.ipynb&lt;/a&gt; with each algorithm having a corresponding cell. A benchmarking library is also included to determine the efficiency of each algorithm in processing time. One thing of note is the lack of functions used for the classification compared to the clustering algorithms. The justification for this discrepancy is due to inexperience in creating optimal implementations as well as determining that not being implemented in a function would not have a significant impact on performance. Additionally, graphs representing the test data were included to help visualize the performance of the clustering algorithms utilizing example code from the documentation &lt;sup id=&#34;fnref:12&#34;&gt;&lt;a href=&#34;#fn:12&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;12&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h4 id=&#34;341-dataset-preprocessing&#34;&gt;3.4.1 Dataset Preprocessing&lt;/h4&gt;
&lt;p&gt;Pre-processing of the cleaned datasets for the classification algorithms was done under guidance of the scikit learn documentation &lt;sup id=&#34;fnref:13&#34;&gt;&lt;a href=&#34;#fn:13&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;13&lt;/a&gt;&lt;/sup&gt;. Overall, each algorithm was trained and tested with the same split for each run. While the split data could have been passed directly to the algorithms, they were normalized further using the built-in fit_transform function for the best results possible.&lt;/p&gt;
&lt;p&gt;Pre-processing of the cleaned datasets for the clustering algorithms was done under guidance of the scikit learn documentation &lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;. Compared to the classification algorithms, a dimensionality reduction was conducted using Principal component analysis (PCA). This step condenses the multiple features into a 2 feature array which the clustering algorithms were optimized for, increasing the odds for the best results possible. Another note is the dataset split was conducted during execution of the algorithm. Upon further investigation, it was determined that this does not have an effect on the ending results as the randomization was disabled due to setting the same random_state parameter for each call.&lt;/p&gt;
&lt;h2 id=&#34;4-results--discussion&#34;&gt;4. Results &amp;amp; Discussion&lt;/h2&gt;
&lt;h3 id=&#34;41-algorithm-metrics&#34;&gt;4.1 Algorithm Metrics&lt;/h3&gt;
&lt;p&gt;The metrics used to determine the viability of each of the algorithms are precision, recall, and f1-score. These are simple metrics based on the values from a confusion matrix which is a visualization of the False and True Positives and Negatives. Precision is essentially how accurate was the algorithm in classifying each data point. This however, is not a good metric to solely base performance as precision does not account for imbalanced distributions within a dataset &lt;sup id=&#34;fnref:14&#34;&gt;&lt;a href=&#34;#fn:14&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;14&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;This is where the recall metric comes in which is defined as how many samples were accurately classified by the algorithm. This is a more versatile metric as it can compensate for imbalanced datasets. While it may not be in our case as seen in the dataset analysis where we have a relatively balanced ratio. It still gives great insight on the performance for our application.&lt;/p&gt;
&lt;p&gt;Finally is the f1-score which is the harmonic mean of the precision and recall metric &lt;sup id=&#34;fnref:14&#34;&gt;&lt;a href=&#34;#fn:14&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;14&lt;/a&gt;&lt;/sup&gt;. This will be the key metric we will mainly focus on as it strikes a good balance between the two more primitive metrics. Since one may think in medical applications one would want to maximize recall, it is at the cost of precision which ends up in more false predictions which is essentially an overfitting scenario &lt;sup id=&#34;fnref:14&#34;&gt;&lt;a href=&#34;#fn:14&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;14&lt;/a&gt;&lt;/sup&gt;. Something that reduces the viability of the model to the application especially since we have a relatively balanced dataset, more customized weighting is not as necessary.&lt;/p&gt;
&lt;p&gt;The metrics for each algorithm implementation are as follows.&lt;/p&gt;
&lt;h4 id=&#34;411-support-vector-machines&#34;&gt;4.1.1 Support Vector Machines&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Table 4.1:&lt;/strong&gt; dav_set metrics&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Precision&lt;/th&gt;
&lt;th&gt;Recall&lt;/th&gt;
&lt;th&gt;f1-score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;No Disease&lt;/td&gt;
&lt;td&gt;0.99&lt;/td&gt;
&lt;td&gt;0.94&lt;/td&gt;
&lt;td&gt;0.96&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Has Disease&lt;/td&gt;
&lt;td&gt;0.95&lt;/td&gt;
&lt;td&gt;0.99&lt;/td&gt;
&lt;td&gt;0.97&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Training Time&lt;/td&gt;
&lt;td&gt;0.038 sec&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Table 4.2:&lt;/strong&gt; sav_set metrics&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Precision&lt;/th&gt;
&lt;th&gt;Recall&lt;/th&gt;
&lt;th&gt;f1-score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;No Disease&lt;/td&gt;
&lt;td&gt;0.99&lt;/td&gt;
&lt;td&gt;0.94&lt;/td&gt;
&lt;td&gt;0.96&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Has Disease&lt;/td&gt;
&lt;td&gt;0.95&lt;/td&gt;
&lt;td&gt;0.99&lt;/td&gt;
&lt;td&gt;0.97&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Training Time&lt;/td&gt;
&lt;td&gt;167.897 sec&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;412-k-nearest-neighbors&#34;&gt;4.1.2 K-Nearest Neighbors&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Table 4.3:&lt;/strong&gt; dav_set metrics&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Precision&lt;/th&gt;
&lt;th&gt;Recall&lt;/th&gt;
&lt;th&gt;f1-score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;No Disease&lt;/td&gt;
&lt;td&gt;0.88&lt;/td&gt;
&lt;td&gt;0.86&lt;/td&gt;
&lt;td&gt;0.87&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Has Disease&lt;/td&gt;
&lt;td&gt;0.87&lt;/td&gt;
&lt;td&gt;0.90&lt;/td&gt;
&lt;td&gt;0.88&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Training Time&lt;/td&gt;
&lt;td&gt;0.025 sec&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Table 4.4:&lt;/strong&gt; sav_set metrics&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Precision&lt;/th&gt;
&lt;th&gt;Recall&lt;/th&gt;
&lt;th&gt;f1-score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;No Disease&lt;/td&gt;
&lt;td&gt;0.62&lt;/td&gt;
&lt;td&gt;0.74&lt;/td&gt;
&lt;td&gt;0.67&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Has Disease&lt;/td&gt;
&lt;td&gt;0.67&lt;/td&gt;
&lt;td&gt;0.54&lt;/td&gt;
&lt;td&gt;0.60&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Training Time&lt;/td&gt;
&lt;td&gt;10.116 sec&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;413-gaussian-naive-bayes&#34;&gt;4.1.3 Gaussian Naive Bayes&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Table 4.5:&lt;/strong&gt; dav_set metrics&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Precision&lt;/th&gt;
&lt;th&gt;Recall&lt;/th&gt;
&lt;th&gt;f1-score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;No Disease&lt;/td&gt;
&lt;td&gt;0.88&lt;/td&gt;
&lt;td&gt;0.81&lt;/td&gt;
&lt;td&gt;0.84&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Has Disease&lt;/td&gt;
&lt;td&gt;0.83&lt;/td&gt;
&lt;td&gt;0.90&lt;/td&gt;
&lt;td&gt;0.86&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Training Time&lt;/td&gt;
&lt;td&gt;0.011 sec&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Table 4.6:&lt;/strong&gt; sav_set metrics&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Precision&lt;/th&gt;
&lt;th&gt;Recall&lt;/th&gt;
&lt;th&gt;f1-score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;No Disease&lt;/td&gt;
&lt;td&gt;0.56&lt;/td&gt;
&lt;td&gt;0.90&lt;/td&gt;
&lt;td&gt;0.69&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Has Disease&lt;/td&gt;
&lt;td&gt;0.72&lt;/td&gt;
&lt;td&gt;0.28&lt;/td&gt;
&lt;td&gt;0.40&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Training Time&lt;/td&gt;
&lt;td&gt;0.057 sec&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;414-decision-trees&#34;&gt;4.1.4 Decision Trees&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Table 4.7:&lt;/strong&gt; dav_set metrics&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Precision&lt;/th&gt;
&lt;th&gt;Recall&lt;/th&gt;
&lt;th&gt;f1-score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;No Disease&lt;/td&gt;
&lt;td&gt;0.92&lt;/td&gt;
&lt;td&gt;0.97&lt;/td&gt;
&lt;td&gt;0.95&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Has Disease&lt;/td&gt;
&lt;td&gt;0.97&lt;/td&gt;
&lt;td&gt;0.93&lt;/td&gt;
&lt;td&gt;0.95&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Training Time&lt;/td&gt;
&lt;td&gt;0.009 sec&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Table 4.8:&lt;/strong&gt; sav_set metrics&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Precision&lt;/th&gt;
&lt;th&gt;Recall&lt;/th&gt;
&lt;th&gt;f1-score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;No Disease&lt;/td&gt;
&lt;td&gt;0.71&lt;/td&gt;
&lt;td&gt;0.80&lt;/td&gt;
&lt;td&gt;0.75&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Has Disease&lt;/td&gt;
&lt;td&gt;0.76&lt;/td&gt;
&lt;td&gt;0.66&lt;/td&gt;
&lt;td&gt;0.71&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Training Time&lt;/td&gt;
&lt;td&gt;0.272 sec&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;415-k-means&#34;&gt;4.1.5 K-Means&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Figure 4.1:&lt;/strong&gt; dav_set algorithm visualization. The axis have no corresponding unit due to the PCA operation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/davkmeans.jpg&#34; alt=&#34;Figure 4.1&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Table 4.9:&lt;/strong&gt; dav_set metrics&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Precision&lt;/th&gt;
&lt;th&gt;Recall&lt;/th&gt;
&lt;th&gt;f1-score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;No Disease&lt;/td&gt;
&lt;td&gt;0.22&lt;/td&gt;
&lt;td&gt;0.29&lt;/td&gt;
&lt;td&gt;0.25&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Has Disease&lt;/td&gt;
&lt;td&gt;0.12&lt;/td&gt;
&lt;td&gt;0.09&lt;/td&gt;
&lt;td&gt;0.10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Training Time&lt;/td&gt;
&lt;td&gt;0.376 sec&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Figure 4.2:&lt;/strong&gt; sav_set algorithm visualization. The axis have no corresponding unit due to the PCA operation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/savkmeans.jpg&#34; alt=&#34;Figure 4.2&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Table 4.10:&lt;/strong&gt; sav_set metrics&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Precision&lt;/th&gt;
&lt;th&gt;Recall&lt;/th&gt;
&lt;th&gt;f1-score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;No Disease&lt;/td&gt;
&lt;td&gt;0.51&lt;/td&gt;
&lt;td&gt;0.69&lt;/td&gt;
&lt;td&gt;0.59&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Has Disease&lt;/td&gt;
&lt;td&gt;0.52&lt;/td&gt;
&lt;td&gt;0.34&lt;/td&gt;
&lt;td&gt;0.41&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Training Time&lt;/td&gt;
&lt;td&gt;1.429 sec&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;416-mean-shift&#34;&gt;4.1.6 Mean-shift&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Figure 4.3:&lt;/strong&gt; dav_set algorithm visualization. The axis have no corresponding unit due to the PCA operation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/davmeanshift.jpg&#34; alt=&#34;Figure 4.3&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Table 4.11:&lt;/strong&gt; dav_set metrics&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Precision&lt;/th&gt;
&lt;th&gt;Recall&lt;/th&gt;
&lt;th&gt;f1-score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;No Disease&lt;/td&gt;
&lt;td&gt;0.47&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;0.64&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Has Disease&lt;/td&gt;
&lt;td&gt;0.00&lt;/td&gt;
&lt;td&gt;0.00&lt;/td&gt;
&lt;td&gt;0.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Training Time&lt;/td&gt;
&lt;td&gt;0.461 sec&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Figure 4.4:&lt;/strong&gt; sav_set algorithm visualization. The axis have no corresponding unit due to the PCA operation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/savmeanshift.jpg&#34; alt=&#34;Figure 4.4&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Table 4.12:&lt;/strong&gt; sav_set metrics&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Precision&lt;/th&gt;
&lt;th&gt;Recall&lt;/th&gt;
&lt;th&gt;f1-score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;No Disease&lt;/td&gt;
&lt;td&gt;0.50&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;0.67&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Has Disease&lt;/td&gt;
&lt;td&gt;0.00&lt;/td&gt;
&lt;td&gt;0.00&lt;/td&gt;
&lt;td&gt;0.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Training Time&lt;/td&gt;
&lt;td&gt;193.93 sec&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;417-spectral-clustering&#34;&gt;4.1.7 Spectral Clustering&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Figure 4.5:&lt;/strong&gt; dav_set algorithm visualization. The axis have no corresponding unit due to the PCA operation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/davspectral.jpg&#34; alt=&#34;Figure 4.5&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Table 4.13:&lt;/strong&gt; dav_set metrics&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Precision&lt;/th&gt;
&lt;th&gt;Recall&lt;/th&gt;
&lt;th&gt;f1-score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;No Disease&lt;/td&gt;
&lt;td&gt;0.86&lt;/td&gt;
&lt;td&gt;0.74&lt;/td&gt;
&lt;td&gt;0.79&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Has Disease&lt;/td&gt;
&lt;td&gt;0.79&lt;/td&gt;
&lt;td&gt;0.89&lt;/td&gt;
&lt;td&gt;0.84&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Training Time&lt;/td&gt;
&lt;td&gt;0.628 sec&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Figure 4.6:&lt;/strong&gt; sav_set algorithm visualization. The axis have no corresponding unit due to the PCA operation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-309/main/project/images/savspectral.jpg&#34; alt=&#34;Figure 4.6&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Table 4.14:&lt;/strong&gt; sav_set metrics&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Precision&lt;/th&gt;
&lt;th&gt;Recall&lt;/th&gt;
&lt;th&gt;f1-score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;No Disease&lt;/td&gt;
&lt;td&gt;0.56&lt;/td&gt;
&lt;td&gt;0.57&lt;/td&gt;
&lt;td&gt;0.57&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Has Disease&lt;/td&gt;
&lt;td&gt;0.56&lt;/td&gt;
&lt;td&gt;0.56&lt;/td&gt;
&lt;td&gt;0.56&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Training Time&lt;/td&gt;
&lt;td&gt;208.822 sec&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;42-discussion&#34;&gt;4.2 Discussion&lt;/h3&gt;
&lt;p&gt;In analyzing the resulting metrics in section 4.1, two major trends between the algorithms are apparent.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The classification algorithms perform significantly better than the clustering algorithms.&lt;/li&gt;
&lt;li&gt;Significant signs of overfitting for the dav_set.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Addressing the first point, it is obvious from the metric performance where on average the classification algorithms were higher than the clustering algorithms. At a lower training time cost as well, which indicates that classification algorithms are well suited for this application than clustering. Especially when looking at the results for Mean-Shift in section 4.1.6 where the algorithm failed to identify any patient with a disease. This also illustrates the discussion on the metrics used to determine performance as the recall was 100% at the cost of missing every patient that would have required treatment illustrated by Figure 4.3 and 4.4. On this topic, comparing the actual data graphs for each of the clustering algorithms and comparing them to the example clustering figures within the scikit documentation, it solidifies that this is not the correct algorithm type for this dataset &lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;Moving on to the next point, it can be seen that overfitting is occurring for the dav_set in comparing the performance to the sav_set for the same algorithm which can be seen in the corresponding tables in sections 4.1.2, 4.1.3, and 4.1.4. Here the performance gap is at least 20% between the two compared to what one would assume should be relatively close to each other. While this could also illustrate the affect the various features have on the algorithm, it was determined that this is most likely due to the small dataset size having a larger influence than anticipated.&lt;/p&gt;
&lt;h2 id=&#34;5-conclusion&#34;&gt;5. Conclusion&lt;/h2&gt;
&lt;p&gt;Reviewing these results, a clear conclusion cannot be accurately be determined due to the considerable amount of variables involved that were not able to be isolated to a desirable level. Namely the  compromises that were mentioned in section 2.1 and general dataset availability. However, it was determined that the main goal of this project was accomplished where the Support Vector Machine algorithm was narrowed down as a viable candidate for future work. Due in part to the overall f1-score performance for both datasets, providing confidence that overfitting may not occur. While there is a downside in scalability due to the significant increase in training time between the smaller dav_set and larger sav_set. This could indicate that further research should be focused on either improving this algorithm or creating a new one based on the underlying mechanism.&lt;/p&gt;
&lt;p&gt;In relation to the types of features, it could be interpreted from this project that further efforts require a more expansive and modern dataset to perform to a level suitable for real world applications. As possible factors affecting the performance are in the accuracy and granularity of the measurements and factors available to learn from. This however, is seen to be a difficult challenge due to the nature of privacy laws on health data but, as proposed in the introduction. It would be very interesting to apply this project&amp;rsquo;s findings on more general health data that is retrieved in annual visits.&lt;/p&gt;
&lt;h2 id=&#34;6-acknowledgements&#34;&gt;6. Acknowledgements&lt;/h2&gt;
&lt;p&gt;The author would like to thank Dr. Gregor Von Laszewski, Dr. Geoffrey Fox, and the associate instructors in the &lt;em&gt;FA20-BL-ENGR-E534-11530: Big Data Applications&lt;/em&gt; course (offered in the Fall 2020 semester at Indiana University, Bloomington) for their assistance and suggestions with regard to this project.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Centers for Disease Control and Prevention. 2020. Heart Disease Facts | Cdc.Gov. [online] Available at: &lt;a href=&#34;https://www.cdc.gov/heartdisease/facts.htm&#34;&gt;https://www.cdc.gov/heartdisease/facts.htm&lt;/a&gt; [Accessed 16 November 2020]. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Amadeo, K., 2020. Preventive Care: How It Lowers Healthcare Costs In America. [online] The Balance. Available at: &lt;a href=&#34;https://www.thebalance.com/preventive-care-how-it-lowers-aca-costs-3306074&#34;&gt;https://www.thebalance.com/preventive-care-how-it-lowers-aca-costs-3306074&lt;/a&gt; [Accessed 16 November 2020]. &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;WebMD. 2020. Understanding Your Cholesterol Report. [online] Available at: &lt;a href=&#34;https://www.webmd.com/cholesterol-management/understanding-your-cholesterol-report&#34;&gt;https://www.webmd.com/cholesterol-management/understanding-your-cholesterol-report&lt;/a&gt; [Accessed 21 October 2020]. &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;R, V., 2020. Feature Selection — Correlation And P-Value. [online] Medium. Available at: &lt;a href=&#34;https://towardsdatascience.com/feature-selection-correlation-and-p-value-da8921bfb3cf&#34;&gt;https://towardsdatascience.com/feature-selection-correlation-and-p-value-da8921bfb3cf&lt;/a&gt; [Accessed 21 October 2020]. &lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Stack Overflow. 2020. Differences In Scikit Learn, Keras, Or Pytorch. [online] Available at: &lt;a href=&#34;https://stackoverflow.com/questions/54527439/differences-in-scikit-learn-keras-or-pytorch&#34;&gt;https://stackoverflow.com/questions/54527439/differences-in-scikit-learn-keras-or-pytorch&lt;/a&gt; [Accessed 27 October 2020]. &lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Scikit-learn.org. 2020. 1.4. Support Vector Machines — Scikit-Learn 0.23.2 Documentation. [online] Available at: &lt;a href=&#34;https://scikit-learn.org/stable/modules/svm.html#classification&#34;&gt;https://scikit-learn.org/stable/modules/svm.html#classification&lt;/a&gt; [Accessed 27 October 2020]. &lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Scikit-learn.org. 2020. 2.3. Clustering — Scikit-Learn 0.23.2 Documentation. [online] Available at: &lt;a href=&#34;https://scikit-learn.org/stable/modules/clustering.html#clustering&#34;&gt;https://scikit-learn.org/stable/modules/clustering.html#clustering&lt;/a&gt; [Accessed 27 October 2020]. &lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:8&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Scikit-learn.org. 2020. 1. Supervised Learning — Scikit-Learn 0.23.2 Documentation. [online] Available at: &lt;a href=&#34;https://scikit-learn.org/stable/supervised_learning.html#supervised-learning&#34;&gt;https://scikit-learn.org/stable/supervised_learning.html#supervised-learning&lt;/a&gt; [Accessed 27 October 2020]. &lt;a href=&#34;#fnref:8&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:9&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Scikit-learn.org. 2020. 1.6. Nearest Neighbors — Scikit-Learn 0.23.2 Documentation. [online] Available at: &lt;a href=&#34;https://scikit-learn.org/stable/modules/neighbors.html&#34;&gt;https://scikit-learn.org/stable/modules/neighbors.html&lt;/a&gt; [Accessed 27 October 2020]. &lt;a href=&#34;#fnref:9&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:10&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Scikit-learn.org. 2020. 1.9. Naive Bayes — Scikit-Learn 0.23.2 Documentation. [online] Available at: &lt;a href=&#34;https://scikit-learn.org/stable/modules/naive_bayes.html&#34;&gt;https://scikit-learn.org/stable/modules/naive_bayes.html&lt;/a&gt; [Accessed 27 October 2020]. &lt;a href=&#34;#fnref:10&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:11&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Scikit-learn.org. 2020. 1.10. Decision Trees — Scikit-Learn 0.23.2 Documentation. [online] Available at: &lt;a href=&#34;https://scikit-learn.org/stable/modules/tree.html&#34;&gt;https://scikit-learn.org/stable/modules/tree.html&lt;/a&gt; [Accessed 27 October 2020]. &lt;a href=&#34;#fnref:11&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:12&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Scikit-learn.org. 2020. A Demo Of K-Means Clustering On The Handwritten Digits Data — Scikit-Learn 0.23.2 Documentation. [online] Available at: &lt;a href=&#34;https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html#sphx-glr-auto-examples-cluster-plot-kmeans-digits-py&#34;&gt;https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html#sphx-glr-auto-examples-cluster-plot-kmeans-digits-py&lt;/a&gt; [Accessed 17 November 2020]. &lt;a href=&#34;#fnref:12&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:13&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Scikit-learn.org. 2020. 6.3. Preprocessing Data — Scikit-Learn 0.23.2 Documentation. [online] Available at: &lt;a href=&#34;https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing&#34;&gt;https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing&lt;/a&gt; [Accessed 17 November 2020]. &lt;a href=&#34;#fnref:13&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:14&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Mianaee, S., 2020. 20 Popular Machine Learning Metrics. Part 1: Classification &amp;amp; Regression Evaluation Metrics. [online] Medium. Available at: &lt;a href=&#34;https://towardsdatascience.com/20-popular-machine-learning-metrics-part-1-classification-regression-evaluation-metrics-1ca3e282a2ce&#34;&gt;https://towardsdatascience.com/20-popular-machine-learning-metrics-part-1-classification-regression-evaluation-metrics-1ca3e282a2ce&lt;/a&gt; [Accessed 10 November 2020]. &lt;a href=&#34;#fnref:14&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-312/assignment6/assignment6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-312/assignment6/assignment6/</guid>
      <description>
        
        
        &lt;h1 id=&#34;engr-e-534-assignment-6-ai-in-health-and-medicine&#34;&gt;ENGR-E 534 Assignment 6: AI in Health and Medicine&lt;/h1&gt;
&lt;h1 id=&#34;ai-enabled-covid-19-diagnostic-framework-utilizing-smartphone-based-embedded-sensors&#34;&gt;AI-enabled COVID-19 diagnostic framework utilizing Smartphone-based Embedded Sensors&lt;/h1&gt;
&lt;p&gt;Saptarshi Sinha, &lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-312/&#34;&gt;fa20-523-312&lt;/a&gt;, &lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-312/blob/master/project/project.md&#34;&gt;Edit&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Keywords:&lt;/strong&gt; smartphone, neural networks, CNN, RNN, embedded sensors, symptom detection, cloud computing&lt;/p&gt;
&lt;h2 id=&#34;1-background-the-need-for-smarter-and-more-pervasive-covid-19-monitoring&#34;&gt;1. Background: The need for smarter and more pervasive COVID-19 monitoring&lt;/h2&gt;
&lt;p&gt;As mankind grapples with the menacing threat of an ongoing pandemic involving the novel COVID-19 (coronavirus infection), researchers and clinicians across the board have tirelessly involved themselves in myriad efforts for controlling the relentless proliferation of this virus so as to check the viral-driven casualties across the globe. It might seem that for the very first time, science and technology have been put to its greatest test ever. It seems that only time can tell if our scientific valor is indeed powerful enough to succeed in such a test, or if the virus would instead claim a major portion of the world’s population as its unfortunate casualty.&lt;/p&gt;
&lt;p&gt;Numerous scientific approaches have been fielded in a relatively short amount of time to deal with the current problem. Many approaches involve novel technologies such as remote video surveillance using assistive robots that monitor virus-inflicted patients, while also protecting those healthcare workers by not involving them in such in-person diagnostic processes. Other approaches involve using machine learning based methodologies for sorting out patents with the virus from those without it simply by using an efficient algorithmic procedure of analyzing different aspects of patients’ CT scans. Major companies have also stepped in to assist in a war-footing format. As an example, Amazon Care is providing pick-up and delivery-based services of test-kits in particular virus-prone locations. Apple’s Siri is now able to provide symptom-based guidance in relation to COVID-19. Microsoft helped creating the Adaptive Biotechnologies platform that studies how our immune system responds to the virus which can provide insights for establishing drug development procedures. Finally, various biotechnological companies all across the world have started conducting extensive research into vaccine development and drug development procedures to combat this novel strain of the virus.&lt;/p&gt;
&lt;p&gt;As amazing these techniques might seem at a superficial glance, the major setback the world is suffering from is with the extent of the viral spread that is amplified due to the lack of testing capabilities. They are either inadequate or cannot handle an entire nation’s population. Although proactive actions have been employed in many nations, testing kits are still being produced slowly. This gives the virus an unfair advantage as time is of the essence. People (esp. asymptomatic individuals) with the virus remain undiagnosed for a greater length of time during which they can inadvertently aid with the proliferation of the viral disease. In this particular context, a very novel strategy for COVID-19 testing and diagnosis will be discussed that utilizes something that we all possess – a smartphone device.&lt;/p&gt;
&lt;h2 id=&#34;2-design--working-principle-ai-based-diagnostic-framework-for-covid-19-utilizing-smartphone-based-embedded-sensors-and-artificial-neural-networks&#34;&gt;2. Design &amp;amp; Working Principle: AI-based diagnostic framework for COVID-19 utilizing Smartphone-based Embedded Sensors and Artificial Neural Networks&lt;/h2&gt;
&lt;p&gt;Cornell University’s archive on Human Computer Interaction (HCI) features a recent article that discusses a strategy involving COVID-19 diagnosis with smartphone-based sensors. In its simplest form, the framework includes the smartphone, and its accompanying sensors and algorithms. External hardware accessories with high-power consumption, or access to specialized equipment is not required for this design &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. Since the application framework involves something that common people use on a daily basis, no tutorials or expert assistance is required to work with such an application. To understand the framework better, we must first note the various symptom types that are exhibited by COVID-19 patients which include high fever, tiredness, dry cough, intense headache, shortness of breath, nausea, etc. To efficiently capture the symptoms, an essential piece of information to keep in mind here is that modern smartphone devices come equipped with various in-built sensors viz. camera sensor, inertial sensor, temperature sensor, accelerometer sensor, microphones, etc. Many previous endeavors utilized such sensors to detect symptoms for other diseases &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. For instance, temperature-fingerprint sensor was used previously for measuring fever-levels; camera sensors (with accelerometers) were utilized earlier to analyze fatigue levels via pattern-recognition algorithms for human-gait analysis; camera sensor (with inertial sensor) were also used for analyzing neck posture to evaluate the headache severities; and, even the microphone was utilized previously for analyzing a patient’s cough-noise in a diagnostic process &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;The research article describes a strategy which uses these various smartphone sensors and their respective algorithms. This is followed up by creating a dataset record comprising predicted levels of the different symptoms which are collected from different patients and studied using deep learning approaches &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. Chiefly, it uses Convolutional Neural Networks (CNN) to analyze spatial data (viz. imaging data from the camera sensor), and Recurrent Neural Networks (RNN) for temporal data (viz. signal or text-based measurements) &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. The entire prediction-based framework can be summarized as follows:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;images/Picture1.png&#34; alt=&#34;Smartphone-based framework for COVID-19 testing&#34;&gt;&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;path_to_image&#34; alt&gt;
    &lt;em&gt;Figure 1: Smartphone-based framework for COVID-19 testing; Source: Adapted from [^1]&lt;/em&gt;
&lt;/p&gt;
&lt;p&gt;The above framework can be sub-divided into four important layers which provides further insights into the different procedures going on in the background while the system makes the disease predictions.&lt;/p&gt;
&lt;h3 id=&#34;i-reading&#34;&gt;i. Reading&lt;/h3&gt;
&lt;p&gt;The first layer involves reading based functionalities for the data coming from different smartphone sensors. This could refer to arrays of different types of data coming from different sources (viz. CT scan imageries, accelerometer readings, microphone sound signals, etc.).&lt;/p&gt;
&lt;h3 id=&#34;ii-configurations&#34;&gt;ii. Configurations&lt;/h3&gt;
&lt;p&gt;The second layer deals with configuring onboard sensors for varied metrics such as time intervals, image resolution, etc. Readings from these first two steps are fed as inputs for the “symptoms algorithm” that can be executed as a smartphone application.&lt;/p&gt;
&lt;h3 id=&#34;iii-symptoms-prediction&#34;&gt;iii. Symptoms Prediction&lt;/h3&gt;
&lt;p&gt;The third layer deals with symptoms-level evaluation. The result is stored as a record that can be fed as an input for the next layer.&lt;/p&gt;
&lt;h3 id=&#34;iv-covid-19-prediction&#34;&gt;iv. COVID-19 Prediction&lt;/h3&gt;
&lt;p&gt;Finally, the last layer involves the application of deep learning (DL) based algorithms to the input data for predicting whether the patient has been afflicted with the virus. A CNN and RNN based combined process is utilized here such that the system can analyze both the spatial data (viz. image pixels) as well as the temporal data (viz. text/signal information) [1].&lt;/p&gt;
&lt;h2 id=&#34;3-discussions-augmentation-with-cloud-comuputing-capabilities&#34;&gt;3. Discussions: Augmentation with Cloud-Comuputing capabilities&lt;/h2&gt;
&lt;p&gt;To enhance the performance of this framework, the recorded data and predicted results can be uploaded to cloud-computing servers. This can help researchers and medical professionals from all around the globe in exchanging information and insights involving accurate patient diagnosis. Such applications are already developing. For instance, IBM recently launched the COVID-19 High Performance Computing Consortium &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. As the name suggests, this consortium has been explicitly designed to tackle the threat of COVID-19 by harnessing enormous computing power for streamlining the search for more information, aiding the hunt of possible treatment paths, and creating drug-and-disease based informational repositories that are made available to appropriate and eligible researchers and institutions strewn all across the globe &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;All in all, it is indeed very commendable on the part of these researchers to facilitate the design of such a low-cost yet effective method of diagnosing COVID-19 when testing capacities are severely limited. If used appropriately, it can stem the spread of this virus by making it possible to diagnose patients sooner and quarantining them. Of course, the strategy does not focus on the treatment itself. But in the current scenario, where we have arrived at a breaking point with this disease, it would greatly assist healthcare personnel with locating and quarantining patients, that would indirectly help saving scores of other lives.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References:&lt;/h2&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Maghdid, Halgurd S., et al. “A Novel AI-Enabled Framework to Diagnose Coronavirus COVID 19 Using Smartphone Embedded Sensors: Design Study.” ArXiv:2003.07434 [Cs, q-Bio], May 2020. arXiv.org, &lt;a href=&#34;http://arxiv.org/abs/2003.07434&#34;&gt;http://arxiv.org/abs/2003.07434&lt;/a&gt; &lt;/br&gt; &lt;/br&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;D. Gil, “IBM Releases Novel AI-Powered Technologies to Help Health and Research Community Accelerate the Discovery of Medical Insights and Treatments for COVID-19”, ibm.com, Apr. 3, 2020. [Online]. Available: &lt;a href=&#34;https://www.ibm.com/blogs/research/2020/04/ai-powered-technologies-accelerate-discovery-covid-19/&#34;&gt;https://www.ibm.com/blogs/research/2020/04/ai-powered-technologies-accelerate-discovery-covid-19/&lt;/a&gt; [Accessed Oct. 17, 2020] &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-312/project/project/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-312/project/project/</guid>
      <description>
        
        
        &lt;h1 id=&#34;aquatic-toxicity-analysis-with-the-aid-of-autonomous-surface-vehicle-asv&#34;&gt;Aquatic Toxicity Analysis with the aid of Autonomous Surface Vehicle (ASV)&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Please use references instead of using URLs to cite the work.&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Benchmark the separate parts of your analysis&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; verify if you need time on the x axis, datapoints would not allow to relate the points if they are gathered in different time intervals. THis has to be clarified while revieing the original data,&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; some plots seem only to be distinguished by the plotstyle but may not be content different. PLease verify&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Extension for project granted till last day of regular semester in December&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; modification of report beyond that granted and expected according to author.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-312/actions&#34;&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-312/workflows/Check%20Report/badge.svg&#34; alt=&#34;Check Report&#34;&gt;&lt;/a&gt;
&lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-312/actions&#34;&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-312/workflows/Status/badge.svg&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;
Status: in progress&lt;/p&gt;
&lt;p&gt;Saptarshi Sinha, &lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-312/&#34;&gt;fa20-523-312&lt;/a&gt;, &lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-312/blob/main/project/project.md&#34;&gt;Edit&lt;/a&gt;&lt;/p&gt;


&lt;div class=&#34;pageinfo pageinfo-primary&#34;&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;With the passage of time, human activities have created and contributed much to the aggrandizing problems of various forms of environmental pollution. Massive amounts of industrial effluents and agricultural waste wash-offs, that often comprise pesticides and other forms of agricultural chemicals, find their way to fresh water bodies, to lakes, and eventually to the oceanic systems. Such events start producing a gradual increase in the toxicity levels of marine ecosystems thereby perturbing the natural balance of such water-bodies. In this endeavor, an attempt will be made to measure the various water quality metrics (viz. temperature, pH, dissolved-oxygen level, and conductivity) with the help of an autonomous surface vehicle (ASV). This collected data will then be analyzed to ascertain if these values exhibit aberration from the established values that are found from USGS and EPA databases for water-quality standards. In the event, the collected data significantly deviates from the standard values of unpolluted sources in nearby geographical areas, that are obtained from the above databases, it can be concluded that the aquatic system in question has been degraded and may no longer be utilized for any form of human usage, such as being sourced for drinking water.&lt;/p&gt;
&lt;p&gt;Contents&lt;/p&gt;
&lt;div class=&#34;toc&#34;&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#1-introduction&#34;&gt;1. Introduction&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#2-background-research-and-previous-work&#34;&gt;2. Background Research and Previous Work&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#3-choice-of-data-sets&#34;&gt;3. Choice of Data-sets&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#4-methodology&#34;&gt;4. Methodology&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#41-hardware-component&#34;&gt;4.1 Hardware Component&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#42-software-component&#34;&gt;4.2 Software Component&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#5-inference&#34;&gt;5. Inference&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#benchmark-results&#34;&gt;Benchmark Results&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#6-conclusion&#34;&gt;6. Conclusion&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#7-acknowledgements&#34;&gt;7. Acknowledgements&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#8-references&#34;&gt;8. References&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Keywords:&lt;/strong&gt; toxicology, pollution, autonomous systems, surface vehicle, sensors, arduino, water quality, data analysis, environment, big data, ecosystem&lt;/p&gt;
&lt;h2 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;When it comes to revolutionizing our qualities of life and improving standards, there is not another branch of science and technology that has made more impact than the myriad technological capabilities offered by the areas of Artificial Intelligence (AI) and its sub-fields involving Computer Vision, Robotics, Machine Learning, Deep Learning, Reinforcement Learning, etc. It should be borne in mind that AI was developed to allow machines/computer processors to work in the same way as the human brain works and which could make intelligent decisions at every conscious level. It was meant to help with tasks for rendering scientific applications more smarter and efficient. There are many tasks that can be performed in a far more dexterous fashion by employing smart-machines and algorithms than by involving human beings. But even more importantly, AI has also been designed to perform tasks that cannot be successfully completed by employing human beings. This could either be due to the prolonged boredom of the task itself, or a task that involves hazardous environments that cannot sustain life-forms for a long time. Some examples in this regard would involve exploring deep mines or volcanic trenches for mineral deposits, exploring the vast expanse of the universe and heavenly bodies, etc. And this is where the concept employing AI/Robotics based technology fits in perfectly for aquatic monitoring and oceanographical surveillance based applications.&lt;/p&gt;
&lt;p&gt;Toxicity analysis of ecologically vulnerable water-bodies, or any other marine ecosystem for that matter, could give us a treasure trove of information regarding biodiversity, mineral deposits, unknown biophysical phenomenon, but most importantly, it could also provide meaningful and scientific information related to the biodegradation of the ecosystem itself. In this research project, an attempt will be made to design a simple foundation of an aquatic Autonomous Surface Vehicle (ASV) that will be deployed in marine ecosystems. Such a vehicle would be embedded with different kind of electronic sensors, that are capable of measuring physical quantities such as temperature, pH, conductance, dissolved oxygen level, etc. The data collected by such a system can either be over a period of time (temporal data), or it could cover a vast aquatic geographical region (spatial data). This data will then be compared with existing datasets that are made publicly available by various environmental organizations in the United States, most importantly the Environmental Protection Agency (EPA) and the US Geological Survey (USGS). A comparative data analysis task between the data collected by the ASV and the vast array of environmental data that are made available from these agencies can then give us an indication about the status of the aquatic degradation of the ecosystem in question by measuring the extent to which the current data deviates from relevant historical data trends.&lt;/p&gt;
&lt;h2 id=&#34;2-background-research-and-previous-work&#34;&gt;2. Background Research and Previous Work&lt;/h2&gt;
&lt;p&gt;After reviewing the necessary background literture and previous work that has been done in this field, it can be stated that most of such endeavors focussed majorly on continuous environmental data collection with the help of sensors attached to a stationary buoy in a particular location of a water-body. Some of the other endeavors did involve deploying a non-stationary vehicle that collected data from large swaths of geographical areas in various water bodies &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. Other research attempts focussed on niche areas such as study of the migration pattern exhibited by zooplanktons upon natural and aritifical irradiance &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;, and detection and monitoring of marine fauna &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;. However, neither did such attempts focus much on the data analysis portion for multiple sensory input(s) nor did it involve an intricate procedure to compare the collected data with historical trends so as to arrive at a suitable conclusion regarding the extent of environmental degradation.&lt;/p&gt;
&lt;p&gt;As mentioned in the previous section, this research project will exhaustively focus not just on the data-collection portion by a non-stationary vehicle, but it will also involve employing deeper study towards the subject of big-data analysis of both the current data of the system in question and the past data obtained for similar aquatic profiles. In this way, it would be possible to learn more about the toxicological aspects of the ecosystem in question.&lt;/p&gt;
&lt;h2 id=&#34;3-choice-of-data-sets&#34;&gt;3. Choice of Data-sets&lt;/h2&gt;
&lt;p&gt;Upon exploring a wide array of available datasets, the following data repositories were chosen to get the required water quality based data over a particular period of time and for a particular geographical region:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://waterdata.usgs.gov/nwis/qw&#34;&gt;USGS Water Quality Data&lt;/a&gt; &lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.epa.gov/waterdata/water-quality-data-download&#34;&gt;EPA Water Quality Data&lt;/a&gt; &lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The USGS dataset is very commensurate with the research goal of this endeavor and hence, focus will be put more on the USGS dataset over the EPA dataset. Some previous work was conducted on this USGS dataset by a particular research team &lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;. However, the emphasis of such work was done from a very broad perspective so as to create an overview of how to use and visualize the data from the USGS water quality portal. Besides, such a work emphasizes on characterizing the seasonal variation of lake water clarity in different regions throughout the continental US, something that is very deviant from what would be addressed in this particular article which majorly involves environmental degradation and toxicology analysis using an autonomous surface vehicle.&lt;/p&gt;
&lt;p&gt;To address the questions involving existence of multiple data-sets and motivation of using multiple data-sets, we must keep in mind that the very nature of this study is based on historical trends of the nature of water-quality in a particular region from the past and how it relates to the current situation. Because of these reasons, multiple data-sets will be referred to from multiple sources so as to achieve robust data-analytical results. This would ensure that too much focus is not given on outlier cases, that may be relevant to just a particular geographical region or an aberration in the data that may only have arisen due to an unknown underlying phenomenon or some form of cataclysmic event from the past. Using multiple datasets from different sources would help to get a resultant data structure that is more likely to converge towards an approximate level of historical thresholds and which can then be used to find out how the current observed data deviates from such previous patterns.&lt;/p&gt;
&lt;h2 id=&#34;4-methodology&#34;&gt;4. Methodology&lt;/h2&gt;
&lt;h3 id=&#34;41-hardware-component&#34;&gt;4.1 Hardware Component&lt;/h3&gt;
&lt;p&gt;The rough outline of the autonomous surface vehicle (ASV) in question has been perceived in the Autodesk Fusion 360 software model. A preliminary model has been designed in this software so as to 3D print the system. It will then be interfaced with the appropriate sensors in question. Then system will be driven by an Arduino-Uno based microcontroller, and it will have different types of environmental sensors that will collect and log data. These sensors have been purchased from the vendor, &amp;ldquo;Atlas Scientific&amp;rdquo;. As of now, the sensors that have been chosen for this ASV are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PT-1000 Temperature sensor kit - &lt;a href=&#34;https://atlas-scientific.com/kits/pt-1000-temperature-kit/&#34;&gt;https://atlas-scientific.com/kits/pt-1000-temperature-kit/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Potential of Hydrogen (pH) sensor kit - &lt;a href=&#34;https://atlas-scientific.com/kits/ph-kit/&#34;&gt;https://atlas-scientific.com/kits/ph-kit/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Dissolved Oxygen (DO) sensor kit - &lt;a href=&#34;https://atlas-scientific.com/kits/dissolved-oxygen-kit/&#34;&gt;https://atlas-scientific.com/kits/dissolved-oxygen-kit/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Conductivity K 1.0 sensor kit - &lt;a href=&#34;https://atlas-scientific.com/kits/conductivity-k-1-0-kit/&#34;&gt;https://atlas-scientific.com/kits/conductivity-k-1-0-kit/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A very rudimentary framework of the system has been realized in the Autodesk Fusion 360 software architecture as shown below. The design provided below is a very simplistic platform but which will lay the foundation of the final structure of the system (see Figure 1).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/ASVSS1.png&#34; alt=&#34;ASV from Fusion 360&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Nascent framework of the ASV system in Fusion 360&lt;/p&gt;
&lt;p&gt;With the chassis framework out of the way, a careful analysis was conducted towards the successful developmental work to be done to complete the build process for a fully functional prototype ASV. In essence, an ASV can be thought of being composed of certain key sub-elements. From a broad perspective, they comprise the hardware makeup, a suitable propulsion system, a sensing system, a communication system, and an appropriate source of onboard power source. The hardware makeup being out of the way, the other aspects can now be elaborated as follows:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Propulsion System:&lt;/strong&gt; The two possibilities that was considered in this regard involve using either a single servo motor with an assortment of rudders and propellers for appropriate steering, or using two separate servo motors, one of which will drive the left-hand side of the system and the other would drive the right-hand side. The second arrangement is preferred in this regard as it provides with better maneuverability and control of the system. For instance, to move forward in a rectilinear fashion, both the motors would be given the same level of power. Whereas for steering the system in a particular direction, one of the motors would be assigned a lower power level than the other, thereby enabling the system to curve inwards on the side which has the motor with a lower power level. Of course, there will always be perturbations and natural disturbances that will deter the system from making these correct path changes. For this reason, a Proportional-Integral-Derivative (PID) controlled response would be augmented for the locomotion algorithm.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sensing System:&lt;/strong&gt; As discussed previously, an arrangement involving four different sensors will be integrated in the ASV. This way, when the entire ASV system is deployed in an aquatic environment, it will be able to simultaneously provide readings for all four water-quality parameters in this case. Precisely, these water-quality parameters would be temperature, potential of hydrogen (pH), dissolved oxygen level, and specific conductance value. It should be noted in this perspective that it is possible to include even more sensors in this ASV system. However, the reason why it was not necessary to go beyond these four sensor types is primarily because of two reasons. Firstly, these parameters are important to most toxicological analysis studies &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; and the readings provided by such a sensory system could be considered as a foundation which could provide future directions (including adding more sensors, if needed). Secondly, we should also keep in mind that the hardware system has certain constraints. In this scenario, it involves a sensory shield (that can be integrated with the Arduino microcontroller) that is being used but which has a maximum of four ports for four different sensors only. Though it is possible to add another layer of shield on top of the current one (thereby raising the capability of integrating the number of sensors to eight), it adds unnecessary bandwidth issues, memory depletion possibilities, along with an increased demand of higher power supply. These issues will especially be more consequential in this case as we are dealing with a microcontroller that has limited memory and power, unlike a microprocessor. Hence, the decision to stick with four sensors only was made.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Communication System:&lt;/strong&gt; This is possibly the most important part of the ASV system as we need to device a technique to offload the data that is collected by the vehicle back to a remote computer/server that would likely be located at a considerable distance away from the ASV. There are different options that have been considered in this regard for establishing a proper communicative functionality between the ASV and the remote computer. Some options that have been considered involved Bluetooth, IR signals, RF signals, GPS-based system, satellite communication, etc. There are both pros and cons when it comes to using any of these different communication systems for the ASV. However, the most important metric in this case involves the maximum range the communication system could span over. Obviously, some of the options (viz. Bluetooth) would not be possible in this regard as they have a very limited communication range. Some others (viz. satellite communication systems) have a very high range but were nevertheless found unsuitable for this endeavor as they required too much onboard processing power to even carry out their most basic operations. Hence, a balanced approach was followed in this scenario and a GPS/RF-based system was eventually chosen as the most appropriate candidate for carrying out the proposed tasks of the ASV.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Power Source:&lt;/strong&gt; Finally, we need an onboard processing power system that can provide the required amount of power to all the functional entities housed in the ASV system. The characteristic of such a desired power source would be that it would not require charging often and can sustain proper ASV operations for at least five to six hours. Additionally, the weight of the power source should also not be too clumsy that might put the stability of the entire ASV system in jeopardy. It must have a suitable weight, and should also come in an appropriate shape and size such that the weight of the entire system is evenly distributed over a large area, thereby further reinforcing the stability of the system.&lt;/p&gt;
&lt;h3 id=&#34;42-software-component&#34;&gt;4.2 Software Component&lt;/h3&gt;
&lt;p&gt;After the data has been collected by the ASV either on a temporal scale (over a period of time) or a spatial scale (over a geographical area), it will then be analyzed to decipher the median convergent values of the water body for the four different parameters that have been measured (i.e. Temperature, pH, DO, and Conductivity). The results of this data analysis task will then be used to find out if such water quality parametric values manifested by the aquatic ecosystem in question deviates by a large proportion from the other result that is obtained after analyzing the historical data from USGS and EPA for a nearby and unpolluted source of water. The USGS and EPA websites make it easier to find data from a nearby geographical region by making it possible to enter the desired location prior to searching for water quality data in their huge databases. In this way, it can be figured out if the water quality parameters of the particular ecological system varies wildly from a neighboring system that has almost the same geographical and ecological attributes.&lt;/p&gt;
&lt;p&gt;The establishment of the degree of variance of the data from the historical data will be carried out by documenting the particular quartile range that the current data lies in with respect to the median data that is obtained from the past/historical datasets. For instance, if the current data resides in the second quartile, it can be demarcated as being more or less consistent with previously established values. However, if it resides in the first or third quartile then it might will that the system has aberrant aspects which might need to be investigated for possible levels of outside pollutants (viz. industrial effluents, agricultural wash-off, etc.), or presence of harmful invasive species that might be altering the delicate natural balance of the ecosystem in question.&lt;/p&gt;
&lt;p&gt;The software logic is included at &lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-312/blob/main/project/code/toxicologyASV.ipynb&#34;&gt;https://github.com/cybertraining-dsc/fa20-523-312/blob/main/project/code/toxicologyASV.ipynb&lt;/a&gt;. It helps in postulating the inferences and conclusions of this research attempt, and which are described in detail in the subsequent sections. This is majorly achieved in the following manner:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First, the collected data was plotted (that was obtained for a particular duration of time in a specific seasonal time of the year) to observe the overall trends of the variation of the four water-quality parameters. This gives an approximate idea regarding what we should normally expect from the water-quality data and which further helps in detecting any kind of outlier/aberrant situations that might arise either due to the presence of artifacts, or nuisance environmental variables.&lt;/li&gt;
&lt;li&gt;This part will be updated soon.&lt;/li&gt;
&lt;li&gt;This part will be updated soon.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Furthermore, the code has been appropriately commented and documented in such a way that it can be easily reproduced. Important instructions and vital information about different aspects of the code have been properly written down wherever necessary.&lt;/p&gt;
&lt;h2 id=&#34;5-inference&#34;&gt;5. Inference&lt;/h2&gt;
&lt;p&gt;NOTE: This section will continue to be updated until project completion.&lt;/p&gt;
&lt;p&gt;The first preliminary set of results are displayed below. To get a better idea, the processed data (viewed as a data-frame in python) was first analyzed to understand the four primary water-quality parameters that are being worked upon. The data-frame for one of the big data sets is shown below, along with the median values that were evalauted for the respective parameters.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/prelimresults.png&#34; alt=&#34;Preliminary results&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 2:&lt;/strong&gt; First set of preliminary output results (in python data-frame) and median values for the four water-quality parameters&lt;/p&gt;
&lt;p&gt;In the above set of results, it should be worthwhile to note that temperature is measured in the celsius scale, specific conductance is measured in microsiemens per centimeter at 25 degree celsius, pH is measured in the usual standard range (between 0-14), and the level of dissolved oxygen is measured in milligrams per liter.&lt;/p&gt;
&lt;p&gt;Next, the content of the dataset, after it is processed in a software architecture, is shown and it displays the alteration of the values (expressed in line plots, scatter plots, and histograms) of the four main water-quality parameters (viz. Temperature, Specific Conductance, pH, and Dissolved Oxygen) over the period of time that starts from October 9, 2020 to October 16, 2020.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/L_temp.png&#34; alt=&#34;Temperature plots&#34;&gt;
&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/S_temp.png&#34; alt=&#34;Temperature plots&#34;&gt;
&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/H_temp.png&#34; alt=&#34;Temperature plots&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 3:&lt;/strong&gt; Line, Scatter, and Histogram plots for the water-quality parameter involving &amp;ldquo;Temperature&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/L_cond.png&#34; alt=&#34;Specific Conductance plots&#34;&gt;
&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/S_cond.png&#34; alt=&#34;Specific Conductance plots&#34;&gt;
&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/H_cond.png&#34; alt=&#34;Specific Conductance plots&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 4:&lt;/strong&gt; Line, Scatter, and Histogram plots for the water-quality parameter involving &amp;ldquo;Specific Conductance&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/L_pH.png&#34; alt=&#34;pH plots&#34;&gt;
&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/S_pH.png&#34; alt=&#34;pH plots&#34;&gt;
&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/H_pH.png&#34; alt=&#34;pH plots&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 5:&lt;/strong&gt; Line, Scatter, and Histogram plots for the water-quality parameter involving &amp;ldquo;pH&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/L_dO2.png&#34; alt=&#34;Dissolved Oxygen level&#34;&gt;
&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/S_dO2.png&#34; alt=&#34;Dissolved Oxygen level&#34;&gt;
&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/H_dO2.png&#34; alt=&#34;Dissolved Oxygen level&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 6:&lt;/strong&gt; Line, Scatter, and Histogram plots for the water-quality parameter involving &amp;ldquo;Dissolved Oxygen level&amp;rdquo;&lt;/p&gt;
&lt;h3 id=&#34;benchmark-results&#34;&gt;Benchmark Results&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;First Benchmark results (To be updated further)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-312/raw/main/project/images/firstbnmk.png&#34; alt=&#34;First Benchmark results&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 7:&lt;/strong&gt; First Benchmark results for the created data analysis framework&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Second Benchmark results (Will be updated soon)&lt;/li&gt;
&lt;li&gt;Third Benchmark results (Will be updated soon)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;6-conclusion&#34;&gt;6. Conclusion&lt;/h2&gt;
&lt;p&gt;NOTE: This section will be addressed upon project completion.&lt;/p&gt;
&lt;h2 id=&#34;7-acknowledgements&#34;&gt;7. Acknowledgements&lt;/h2&gt;
&lt;p&gt;The author would like to thank Dr. Gregor Von Laszewski, Dr. Geoffrey Fox, and the associate instructors in the &lt;em&gt;FA20-BL-ENGR-E534-11530: Big Data Applications&lt;/em&gt; course (offered in the Fall 2020 semester at Indiana University, Bloomington) for their continued assistance and suggestions with regard to exploring this idea and also for their aid with preparing the various drafts of this article.&lt;/p&gt;
&lt;h2 id=&#34;8-references&#34;&gt;8. References&lt;/h2&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Valada A., Velagapudi P., Kannan B., Tomaszewski C., Kantor G., Scerri P. (2014) Development of a Low Cost Multi-Robot Autonomous Marine Surface Platform. In: Yoshida K., Tadokoro S. (eds) Field and Service Robotics. Springer Tracts in Advanced Robotics, vol 92. Springer, Berlin, Heidelberg. &lt;a href=&#34;https://doi.org/10.1007/978-3-642-40686-7_43&#34;&gt;https://doi.org/10.1007/978-3-642-40686-7_43&lt;/a&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;M. Ludvigsen, J. Berge, M. Geoffroy, J. H. Cohen, P. R. De La Torre, S. M. Nornes, H. Singh, A. J. Sørensen, M. Daase, G. Johnsen, Use of an Autonomous Surface Vehicle reveals small-scale diel vertical migrations of zooplankton and susceptibility to light pollution under low solar irradiance. Sci. Adv. 4, eaap9887 (2018). &lt;a href=&#34;https://advances.sciencemag.org/content/4/1/eaap9887/tab-pdf&#34;&gt;https://advances.sciencemag.org/content/4/1/eaap9887/tab-pdf&lt;/a&gt; &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Verfuss U., et al., (2019, March). A review of unmanned vehicles for the detection and monitoring of marine fauna. Marine Pollution Bulletin, Volume 140, Pages 17-29. Retrieved from &lt;a href=&#34;https://doi.org/10.1016/j.marpolbul.2019.01.009&#34;&gt;https://doi.org/10.1016/j.marpolbul.2019.01.009&lt;/a&gt; &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;USGS Water Quality Data, Accessed: Nov. 2020, &lt;a href=&#34;https://waterdata.usgs.gov/nwis/qw&#34;&gt;https://waterdata.usgs.gov/nwis/qw&lt;/a&gt; &lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;EPA Water Quality Data, Accessed Nov. 2020, &lt;a href=&#34;https://www.epa.gov/waterdata/water-quality-data-download&#34;&gt;https://www.epa.gov/waterdata/water-quality-data-download&lt;/a&gt; &lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Read, E. K., Carr, L., De Cicco, L., Dugan, H. A., Hanson, P. C., Hart, J. A., Kreft, J., Read, J. S., and Winslow, L. A. (2017), Water quality data for national‐scale aquatic research: The Water Quality Portal, Water Resour. Res., 53, 1735– 1745, doi:10.1002/2016WR019993. &lt;a href=&#34;https://agupubs.onlinelibrary.wiley.com/doi/epdf/10.1002/2016WR019993&#34;&gt;https://agupubs.onlinelibrary.wiley.com/doi/epdf/10.1002/2016WR019993&lt;/a&gt; &lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-313/assignment5/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-313/assignment5/</guid>
      <description>
        
        
        &lt;h1 id=&#34;homework-5&#34;&gt;Homework 5&lt;/h1&gt;
&lt;h2 id=&#34;student-name-fauzan-isnaini&#34;&gt;Student Name: Fauzan Isnaini&lt;/h2&gt;
&lt;h1 id=&#34;predicting-stock-market-recovery-in-indonesia-after-covid-19-crash&#34;&gt;Predicting Stock Market Recovery in Indonesia after COVID-19 Crash&lt;/h1&gt;
&lt;h2 id=&#34;team&#34;&gt;Team&lt;/h2&gt;
&lt;p&gt;I will conduct this study by myself.&lt;/p&gt;
&lt;h2 id=&#34;topic&#34;&gt;Topic&lt;/h2&gt;
&lt;p&gt;The COVID-19 Pandemic is not just a crisis in the public health sector.
It also impacts unemployment rates, business revenues, and mass
psychology, which in the end lead to crashes in global stock markets.
While some stock indexes like the Dow Jones Industrial Average (DJIA)
and NASDAQ Composite already recovered, the Indonesian Stock Market
Index (IDX Composite) is still far below its price before the pandemic.
Some of the possible causes are: 1. Foreign investments represent about
50% of the total fund in the IDX stock exchange. In a pandemic
situation, foreign investors might choose to withdraw their stocks and
find another safer country to invest in. 2. Unpredictability of the
pandemic situation drives investors to reallocate their funds in safer
assets, such as cash, gold, or USD. 3. Changes in the macroeconomic
situation, such as unemployment rate, Indonesian Rupiah (IDR) exchange
rate, and interest rate. 4. Changes in the consumer buying power also
change the business revenues, thus changing fundamental data. 5. Mass
psychology of investors that the stock market is not safe in this
pandemic situation, holding them from returning to the stock market To
predict the time needed for IDX Composite to recover, there are two
indicators that can be utilized: 1. Fundamental indicators, which
represent the financial aspect. This can be in the form of macroeconomic
data and a company financial report 2. Technical indicators, which
represent the mass psychology of investors. This can be obtained from
news, social media, or statistical analysis of how the stock market
moves&lt;/p&gt;
&lt;h2 id=&#34;dataset&#34;&gt;Dataset&lt;/h2&gt;
&lt;p&gt;In predicting the outcome, I will utilize these datasets: 1. Yahoo
Finance (finance.yahoo.com). Yahoo Finance contains a lot of both
fundamental and technical data, and they are free of charge. 2. Twitter
(twitter.com). I can conduct a content analysis on Twitter to represent
the mass psychology regarding the economic condition in Indonesia 3.
News channel. I can also utilize data crawler software to conduct
content analysis to compare positive and negative news in Indonesia. 4.
Third party stock data feeder. There are some third parties who provide
more comprehensive stock data on a subscription basis. This is another
option if the above datasets are not sufficient&lt;/p&gt;
&lt;h2 id=&#34;what-needs-to-be-done-to-get-a-great-grade&#34;&gt;What needs to be done to get a great grade&lt;/h2&gt;
&lt;p&gt;I will build a Python program to learn these data and generate a
prediction on when the IDX Composite will recover. I will also consider
the recovery rate of each of these sectors: 1. Mining 2. Agriculture 3.
Finance 4. Infrastructure 5. Miscellaneous Industries 6. Consumers&amp;rsquo;
Goods 7. Property 8. Trading 9. Basic Industry While they are
incorporated in the IDX Composite, the recovery rate of each sector may
be different because of their respective nature of the industry. For
example, consumer&amp;rsquo;s goods may be impacted less in this COVID-19
pandemic, thus resulting in a faster recovery. On the other hand, the
property sector might be the most impacted sector in this pandemic, thus
resulting in a long time to recover. The program will be able to learn
continuously, so if new data is available, it can renew the analysis and
give a more accurate prediction.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-313/assignment6/assignment6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-313/assignment6/assignment6/</guid>
      <description>
        
        
        &lt;h1 id=&#34;homework-6&#34;&gt;Homework 6&lt;/h1&gt;
&lt;p&gt;Student Name: Fauzan Isnaini&lt;/p&gt;
&lt;h1 id=&#34;how-ai-helps-diagnosis-and-decision-making-in-health-care-facilities&#34;&gt;How AI Helps Diagnosis and Decision Making in Health Care Facilities&lt;/h1&gt;
&lt;h2 id=&#34;radiology-assistant&#34;&gt;Radiology Assistant&lt;/h2&gt;
&lt;p&gt;Radiology is a branch of medicine that uses imaging technology to diagnose and treat disease. Diagnostic radiology helps health care providers see structures inside your body. Using the diagnostic images, the radiologist or other physicians can often [3]:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Diagnose the cause of your symptoms&lt;/li&gt;
&lt;li&gt;Monitor how well your body is responding to a treatment you are receiving for your disease or condition&lt;/li&gt;
&lt;li&gt;Screen for different illnesses, such as breast cancer, colon cancer, or heart disease
Within radiology, trained physicians visually assess medical images and report findings to detect, characterize and monitor diseases. Such assessment is often based on education and experience and can be, at times, subjective. In contrast to such qualitative reasoning, AI excels at recognizing complex patterns in imaging data and can provide a quantitative assessment in an automated fashion. More accurate and reproducible radiology assessments can then be made when AI is integrated into the clinical workflow as a tool to assist physicians.[1]
Some examples of AI’s clinical application in radiology are [1]:&lt;/li&gt;
&lt;li&gt;Thoracic imaging
AI can help in identifying pulmonary nodules, which can be applied in early detection of lung cancer&lt;/li&gt;
&lt;li&gt;Abdominal and pelvic imaging
AI can help in detecting lesions in abdominal and pelvic. For example, AI can analyze data from computed topography (CT) and magnetic resonance imaging (MRI) to detect liver lesions, and characterize these lesions as benign or malignant. Furthermore, AI can also help in suggesting the follow-up actions for the patient.&lt;/li&gt;
&lt;li&gt;Colonoscopy
Colonic polyps that are undetected or misclassified pose a potential risk of colorectal cancer. AI can help in making an early detection and consistent monitoring of this risk.&lt;/li&gt;
&lt;li&gt;Mammography
Analyzing mammography is technically challenging, even for a trained expert. AI can assist in interpreting the image. For example, AI can identify and characterize microcalcifications. Microcalcifications are tiny deposits of calcium salts that are too small to be felt but can be detected by imaging, and can be an early sign of breast cancer. They can be scattered throughout the mammary gland, or occur in clusters. [4]&lt;/li&gt;
&lt;li&gt;Brain imaging
AI can help in making diagnostic prediction of brain tumors, which are characterized by abnormal growth of brain tissue.&lt;/li&gt;
&lt;li&gt;Radiation oncology
Radiation treatment planning can be automated by segmenting tumours for radiation dose optimization. Furthermore, assessing response to treatment by monitoring over time is essential for evaluating the success of radiation therapy efforts. AI is able to perform these assessments, thereby improving accuracy and speed.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;ai-in-clinical-decision-support&#34;&gt;AI in Clinical Decision Support&lt;/h2&gt;
&lt;p&gt;Other than analyzing radiology images, AI can also digest data from blood tests, electrocardiogram (EKG), genomics, and patient medical history do give a better treatment to the patient. AI-enabled clinical decision support includes diagnosis and prognosis, and involves classification or regression algorithms that can predict the probability of a medical outcome or the risk for a certain disease.[5]
Here are some examples of how AI helps clinical decision [6]:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Accumulation of medical histories from birth alongside linked maternal electronic health record (HER) information in a healthcare facility, enabled the prediction of high obesity risk children as early as two years after birth, possibly allowing life-altering preventative interventions.&lt;/li&gt;
&lt;li&gt;The Advanced Alert Monitoring system developed and deployed by Kaiser Permanente uses Intensive Care Unit (ICU) data to predict fatally deteriorating cases and alert staff to the need of life-saving interventions.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-313/broken/assignment5/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-313/broken/assignment5/</guid>
      <description>
        
        
        &lt;h1 id=&#34;homework-5&#34;&gt;Homework 5&lt;/h1&gt;
&lt;h2 id=&#34;student-name-fauzan-isnaini&#34;&gt;Student Name: Fauzan Isnaini&lt;/h2&gt;
&lt;h1 id=&#34;predicting-stock-market-recovery-in-indonesia-after-covid-19-crash&#34;&gt;Predicting Stock Market Recovery in Indonesia after COVID-19 Crash&lt;/h1&gt;
&lt;h2 id=&#34;team&#34;&gt;Team&lt;/h2&gt;
&lt;p&gt;I will conduct this study by myself.&lt;/p&gt;
&lt;h2 id=&#34;topic&#34;&gt;Topic&lt;/h2&gt;
&lt;p&gt;The COVID-19 Pandemic is not just a crisis in the public health sector.
It also impacts unemployment rates, business revenues, and mass
psychology, which in the end lead to crashes in global stock markets.
While some stock indexes like the Dow Jones Industrial Average (DJIA)
and NASDAQ Composite already recovered, the Indonesian Stock Market
Index (IDX Composite) is still far below its price before the pandemic.
Some of the possible causes are: 1. Foreign investments represent about
50% of the total fund in the IDX stock exchange. In a pandemic
situation, foreign investors might choose to withdraw their stocks and
find another safer country to invest in. 2. Unpredictability of the
pandemic situation drives investors to reallocate their funds in safer
assets, such as cash, gold, or USD. 3. Changes in the macroeconomic
situation, such as unemployment rate, Indonesian Rupiah (IDR) exchange
rate, and interest rate. 4. Changes in the consumer buying power also
change the business revenues, thus changing fundamental data. 5. Mass
psychology of investors that the stock market is not safe in this
pandemic situation, holding them from returning to the stock market To
predict the time needed for IDX Composite to recover, there are two
indicators that can be utilized: 1. Fundamental indicators, which
represent the financial aspect. This can be in the form of macroeconomic
data and a company financial report 2. Technical indicators, which
represent the mass psychology of investors. This can be obtained from
news, social media, or statistical analysis of how the stock market
moves&lt;/p&gt;
&lt;h2 id=&#34;dataset&#34;&gt;Dataset&lt;/h2&gt;
&lt;p&gt;In predicting the outcome, I will utilize these datasets: 1. Yahoo
Finance (finance.yahoo.com). Yahoo Finance contains a lot of both
fundamental and technical data, and they are free of charge. 2. Twitter
(twitter.com). I can conduct a content analysis on Twitter to represent
the mass psychology regarding the economic condition in Indonesia 3.
News channel. I can also utilize data crawler software to conduct
content analysis to compare positive and negative news in Indonesia. 4.
Third party stock data feeder. There are some third parties who provide
more comprehensive stock data on a subscription basis. This is another
option if the above datasets are not sufficient&lt;/p&gt;
&lt;h2 id=&#34;what-needs-to-be-done-to-get-a-great-grade&#34;&gt;What needs to be done to get a great grade&lt;/h2&gt;
&lt;p&gt;I will build a Python program to learn these data and generate a
prediction on when the IDX Composite will recover. I will also consider
the recovery rate of each of these sectors: 1. Mining 2. Agriculture 3.
Finance 4. Infrastructure 5. Miscellaneous Industries 6. Consumers&amp;rsquo;
Goods 7. Property 8. Trading 9. Basic Industry While they are
incorporated in the IDX Composite, the recovery rate of each sector may
be different because of their respective nature of the industry. For
example, consumer&amp;rsquo;s goods may be impacted less in this COVID-19
pandemic, thus resulting in a faster recovery. On the other hand, the
property sector might be the most impacted sector in this pandemic, thus
resulting in a long time to recover. The program will be able to learn
continuously, so if new data is available, it can renew the analysis and
give a more accurate prediction.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-313/project/project/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-313/project/project/</guid>
      <description>
        
        
        &lt;h1 id=&#34;analyzing-lstm-performance-on-predicting-stock-market-for-multiple-time-steps&#34;&gt;Analyzing LSTM Performance on Predicting Stock Market for Multiple Time Steps&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-313/actions&#34;&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-313/workflows/Check%20Report/badge.svg&#34; alt=&#34;Check Report&#34;&gt;&lt;/a&gt;
&lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-313/actions&#34;&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-313/workflows/Status/badge.svg&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;
Status: in progress&lt;/p&gt;
&lt;p&gt;Fauzan Isnaini, &lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-313/&#34;&gt;fa20-523-313&lt;/a&gt;, &lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-313/blob/main/project/project.md&#34;&gt;Edit&lt;/a&gt;&lt;/p&gt;


&lt;div class=&#34;pageinfo pageinfo-primary&#34;&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Predicting stock market has been an attractive field of research for a long time because it promises big wealth for anyone who can find the secret sauce. For a long time, traders around the world have been relying on technical analysis to analyze patterns in the stock price movement and predict the trend. With the advancement of big data, some financial institutions are beginning to predict the market by creating a model of the market using machine learning. While some researches produce promising results, most of them are directed on predicting the next day market behavior. In this study, we created an LSTM model to predict the market for multiple time steps. We then analyzed the performance of the model for different time period. From our observations, LSTM is good at predicting 5 time steps ahead, but the prediction became inaccurate as the time frame gets longer.&lt;/p&gt;
&lt;p&gt;Contents&lt;/p&gt;
&lt;div class=&#34;toc&#34;&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#1-introduction&#34;&gt;1. Introduction&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#2-background-research-and-previous-work&#34;&gt;2. Background Research and Previous Work&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#21-predicting-long-term-stock-price-movement-using-the-random-forest-algorithm&#34;&gt;2.1 Predicting Long Term Stock Price Movement using the Random Forest Algorithm&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#22-predicting-the-next-recession-using-long-short-term-memory-lstm-algorithm&#34;&gt;2.2 Predicting the Next Recession using Long Short-term Memory (LSTM) Algorithm&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#23-stock-market-prediction-using-text-mining&#34;&gt;2.3 Stock Market Prediction Using Text Mining&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#3-choice-of-data-sets&#34;&gt;3. Choice of Data-sets&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#4-methodology&#34;&gt;4. Methodology&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#5-inference&#34;&gt;5. Inference&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#6-conclusion&#34;&gt;6. Conclusion&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#7-acknowledgements&#34;&gt;7. Acknowledgements&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#8-references&#34;&gt;8. References&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Keywords:&lt;/strong&gt; stock, market, predictive analytics, LSTM, random forest, regression, technical analysis&lt;/p&gt;
&lt;h2 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;Predicting the stock market is a complex task with lots of different variables comes into play. As Giles &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; explained, financial forecasting is an instance of signal processing problem which is difficult because of high noise, small sample size, non-stationary, and non-linearity.&lt;/p&gt;
&lt;p&gt;The noisy characteristics mean the incomplete information gap between past stock trading price and volume with a future price. The stock market is sensitive with the political and macroeconomic environment. However, these two kinds of information are too complex and unstable to gather. The above information that cannot be included in features are considered as noise.&lt;/p&gt;
&lt;p&gt;The sample size of financial data is determined by the real world transaction records. On one hand, a larger sample size refers to a longer period of transaction records; on the other hand, large sample size increases the uncertainty of financial environment.&lt;/p&gt;
&lt;p&gt;By non-stationarity, one means that the distribution of stock data is various during time changing. Non-linearity implies that feature correlation of different individual stocks is various.&lt;/p&gt;
&lt;p&gt;Efficient Market Hypothesis was developed by Burton G. Malkiel in 1991. In Burton’s hypothesis, he indicates that predicting or forecasting the financial market is unrealistic, because price changes in the real world are unpredictable. All the changes in prices of the financial market are based on immediate economic events or news. Investors are profit-oriented, their buying or selling decisions are made according to most recent events regardless past analysis or plans. The argument about this Efficient Market Hypothesis has never been ended. So far, there is no strong proof that can verify if the efficient market hypothesis is proper or not.&lt;/p&gt;
&lt;p&gt;However, as Yaser claims, financial markets are predictable to a certain extent. The past experience of many price changes over a certain period of time in the financial market and the undiscounted serial correlations among vital economic events affecting the future financial market are two main pieces of evidence opposing the Efficient Market Hypothesis.&lt;/p&gt;
&lt;p&gt;In recent years, machine learning methods have been extensively researched for their potentials in forecasting and prediction of the financial market. Multi-layer feed forward neural networks, SVM, reinforcement learning, relevance vector machines, and recurrent neural networks are the hottest topics of many approaches in financial market prediction field. Among all the machine learning methods, neural networks are well studied and have been successfully used for forecasting and modeling financial market.&lt;/p&gt;
&lt;p&gt;Unlike traditional machine learning models, the network learns from the examples by constructing an input-output mapping for the problem at hand. Such an approach brings to mind the study of  nonparametric statistical inference; the term nonparametric is used here to signify the fact that no prior assumptions are made on a statistical model for the input data.&lt;/p&gt;
&lt;p&gt;As Francis E.H. Tay and Lijuan Cao explained in their studies, Neural networks are more noise tolerant and more flexible compared with traditional statistical models. By noise tolerance, one means neural networks have the ability to be trained by incomplete and overlapped data. Flexibility refers to that neural networks have the capability to learn dynamic systems through a retraining process using new data patterns.&lt;/p&gt;
&lt;p&gt;Long short-term memory is a recurrent neural network introduced by Sepp Hochreite and Jurgen Schmidhuber in 1997. LSTM is designed to forecast, predict and classify time series data even long time lags between vital events happened before. LSTMs have been applied to solve several of problems; among those, handwriting Recognition and speech recognition made LSTM famous.&lt;/p&gt;
&lt;p&gt;LSTM has advantages compared with traditional back-propagation neural networks and normal recurrent neural networks. The constant error back propagation inside memory blocks enables in LSTM ability to overcome long time lags in case of problems similar to those discussed above; LSTM can handle noise, distributed representations, and continuous values; LSTM requires no need for parameter fine-tuning, it works well over a broad range of parameters such as learning rate, input gate bias, and output gate bias.&lt;/p&gt;
&lt;p&gt;In this study, we created an LSTM model to predict the market for multiple time steps. We then analyzed the performance of the model for different time period.&lt;/p&gt;
&lt;h2 id=&#34;2-background-research-and-previous-work&#34;&gt;2. Background Research and Previous Work&lt;/h2&gt;
&lt;p&gt;Predicting the stock market offers great profit, thus it is a widely popular research area. While there is no specific paper addressing how to predict market recovery, there are some researches related to this area that can be utilized in this study.&lt;/p&gt;
&lt;h3 id=&#34;21-predicting-long-term-stock-price-movement-using-the-random-forest-algorithm&#34;&gt;2.1 Predicting Long Term Stock Price Movement using the Random Forest Algorithm&lt;/h3&gt;
&lt;p&gt;Milosevic &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; uses some machine learning algorithms to predict whether some company’s value will be 10% higher or not over the period of one year. He evaluated the companies’ financial indicators and found out that the Random Forest algorithm can get 0.765 precision, which is an amazing performance in the stock market.&lt;/p&gt;
&lt;h3 id=&#34;22-predicting-the-next-recession-using-long-short-term-memory-lstm-algorithm&#34;&gt;2.2 Predicting the Next Recession using Long Short-term Memory (LSTM) Algorithm&lt;/h3&gt;
&lt;p&gt;Another long term prediction in this area is from Khedkar &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;. He used LSTM in predicting the next recession in India. In this study, the stock closing price data is used, instead of financial data – which is technical analysis. LSTM networks are well-suited to classifying, processing, and making predictions based on time series data, since there can be lags of unknown duration between important events in a time series. The study managed to predict the stock market crash in 2020.&lt;/p&gt;
&lt;h3 id=&#34;23-stock-market-prediction-using-text-mining&#34;&gt;2.3 Stock Market Prediction Using Text Mining&lt;/h3&gt;
&lt;p&gt;Text mining usually accompanies the technical analysis method. We can conduct sentiment analysis on news and social media to predict whether the stock market goes up or down. Sentiments on Twitter usually indicate market volatility and volume, while sentiments on news are a better predictor of stock movement [4].&lt;/p&gt;
&lt;h2 id=&#34;3-choice-of-data-sets&#34;&gt;3. Choice of Data-sets&lt;/h2&gt;
&lt;p&gt;In predicting the outcome, these datasets will be utilized:
Yahoo Finance (finance.yahoo.com). Yahoo Finance contains a lot of both fundamental and technical data, and they are free of charge.
Twitter (twitter.com). Sentiment analysis on Twitter will be conducted to represent the mass psychology regarding the economic condition in Indonesia&lt;/p&gt;
&lt;h2 id=&#34;4-methodology&#34;&gt;4. Methodology&lt;/h2&gt;
&lt;p&gt;In this study, we will predict market movement in the next two years using technical approach accompanied by sentiment analysis. While fundamental approach is a strong predictor of the long term stock price movement for an individual stock, it is very difficult to utilize it in an index like IDX composite because it will takes too many variables. The technical approach will utilize LSTM, which has been widely used in time series analysis.
While there are previous works on predicting stock markets, most of them are based on 60 days sliding windows to predict the price one day ahead. While this approach give a good result for a single time-step analysis, there is no evidence that this approach can work fine for multiple time-steps.&lt;/p&gt;
&lt;h2 id=&#34;5-inference&#34;&gt;5. Inference&lt;/h2&gt;
&lt;p&gt;This section will be addressed upon project completion.&lt;/p&gt;
&lt;h2 id=&#34;6-conclusion&#34;&gt;6. Conclusion&lt;/h2&gt;
&lt;p&gt;This section will be addressed upon project completion.&lt;/p&gt;
&lt;h2 id=&#34;7-acknowledgements&#34;&gt;7. Acknowledgements&lt;/h2&gt;
&lt;p&gt;The author would like to thank Dr. Geoffrey Fox, Dr. Gregor Von Laszewski, and the associate instructors in the FA20-BL-ENGR-E534-11530: Big Data Applications course (offered in the Fall 2020 semester at Indiana University, Bloomington) for their continued assistance and suggestions concerning exploring this idea and also for their aid with preparing the various drafts of this article.&lt;/p&gt;
&lt;h2 id=&#34;8-references&#34;&gt;8. References&lt;/h2&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;A. Nikfarjam, E. Emadzadeh, and S. Muthaiyah, &amp;ldquo;Text mining approaches for stock market prediction,&amp;rdquo; 2010 The 2nd International Conference on Computer and Automation Engineering (ICCAE), 2010. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;N. Milosevic, &amp;ldquo;Equity forecast: Predicting long term stock price movement using machine learning,&amp;rdquo; 2018. &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;S. Khedkar, &amp;ldquo;Stock Market Prediction Using Deep Learning and Python,&amp;rdquo; Medium, 27-Sep-2019. [Online]. Available: &lt;a href=&#34;https://medium.com/analytics-vidhya/stock-market-prediction-using-python-article-4-the-next-recession-923185a2736f&#34;&gt;https://medium.com/analytics-vidhya/stock-market-prediction-using-python-article-4-the-next-recession-923185a2736f&lt;/a&gt;. [Accessed: 20-Oct-2020]. &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-313/test/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-313/test/</guid>
      <description>
        
        
        &lt;h2 id=&#34;this-is-testmd&#34;&gt;This is Test.md&lt;/h2&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-314/assignment_6/assignment_6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-314/assignment_6/assignment_6/</guid>
      <description>
        
        
        &lt;h1 id=&#34;artificial-intelligence-in-health&#34;&gt;Artificial Intelligence in Health&lt;/h1&gt;
&lt;p&gt;Artificial Intelligence has influenced most of the sectors in the world like health, agriculture, sports and so much more. In the last decade, one of the most influenced sector is healthcare. In the beginning, AI was considered only a technology to help in the medical decision-making. Little were we aware that it will bring a tremendous transition in the healthcare industry. From simple decision-making, it advanced to autonomous surgical robots, image diagnosis, virtual nursing assistants etc. Most of the websites/apps are now offering symptom checkers which enable the patients to reduce the number of hospital visits drastically.&lt;/p&gt;
&lt;h2 id=&#34;ai-in-cancer-prognosis-and-diagnosis&#34;&gt;AI in Cancer Prognosis and Diagnosis&lt;/h2&gt;
&lt;p&gt;Chronic health conditions like cancer, diabetics, heart diseases etc. are mostly benefited from artificial intelligence as shown in Figure 1.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-314/blob/master/assignment_6/images/ai_1.jpg&#34; alt=&#34;Figure 1&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Chronic health conditions that benefit most from AI/ML&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Cancer is an aggressive disease with high mortality rate. But if the disease is diagnosed in early stages some of the cancers like lung cancer, breast cancer, thyroid cancer etc can be cured or controlled. A decade ago, doctors spent hours just for diagnosing cancer but still lacked accuracy. Now with the introduction of AI, the time for diagnosis has reduced considerably with much more precision.&lt;/p&gt;
&lt;p&gt;AI model can be trained with vast amount of image data from tests like Ultrasound scanning, sonography, Endoscopy etc. Integrative processing and extraction  of these images will result in more efficient diagnosis. A set of computer algorithms used to process medical images and extract details are refered to as DL. It can be used to inform doctors on prognosis, molecular status or treatment sensitivity.&lt;/p&gt;
&lt;p&gt;Integration of AI technology in cancer care has increased the survival rate of patients. Research is being conducted on introducing AI to provide a customized care to people according to their genes and history.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.healthcareitnews.com/news/3-charts-show-where-artificial-intelligence-making-impact-healthcare-right-now&#34;&gt;https://www.healthcareitnews.com/news/3-charts-show-where-artificial-intelligence-making-impact-healthcare-right-now&lt;/a&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-314/project/project/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-314/project/project/</guid>
      <description>
        
        
        &lt;h1 id=&#34;residential-power-usage-prediction&#34;&gt;Residential Power Usage Prediction&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-314/actions&#34;&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-314/workflows/Check%20Report/badge.svg&#34; alt=&#34;Check Report&#34;&gt;&lt;/a&gt;
&lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-314/actions&#34;&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-314/workflows/Status/badge.svg&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;
Status: final&lt;/p&gt;
&lt;p&gt;Siny P. Raphel, &lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-314/&#34;&gt;fa20-523-314&lt;/a&gt;, &lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-314/blob/main/project/project.md&#34;&gt;Edit&lt;/a&gt;&lt;/p&gt;


&lt;div class=&#34;pageinfo pageinfo-primary&#34;&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;We are living in a technology-driven world. Innovations make human life easier. As science advances, the usage of electrical and electronic gadgets are leaping. This leads to the shoot up of power consumption. Weather plays an important role in power usage. Even the outbreak of Covid-19 has impacted daily power utilization. Similarly, many factors influence the use of electricity-driven appliances at homes. Monitoring these factors and consolidating them will result in a humungous amount of data. But analyzing this data will help to keep track of power consumption. This system provides a prediction of usage of electric power at residences in the future and will enable people to plan ahead of time and not be surprised by the monthly electricity bill.&lt;/p&gt;
&lt;p&gt;Contents&lt;/p&gt;
&lt;div class=&#34;toc&#34;&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#1-introduction&#34;&gt;1. Introduction&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#2-reason-to-choose-this-dataset&#34;&gt;2. Reason to choose this dataset&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#3-datasets&#34;&gt;3. Datasets&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#4-data-preprocessing&#34;&gt;4. Data preprocessing&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#41-data-download-and-load&#34;&gt;4.1 Data download and load&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#42-data-descriptive-analysis&#34;&gt;4.2 Data descriptive analysis&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#43-preprocessing-data&#34;&gt;4.3 Preprocessing data&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#44-merge-datasets&#34;&gt;4.4 Merge datasets&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#5-exploratory-data-analysis&#34;&gt;5. Exploratory Data Analysis&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#6-modeling&#34;&gt;6. Modeling&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#61-split-data&#34;&gt;6.1 Split Data&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#62-pipelines&#34;&gt;6.2 Pipelines&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#63-metrics&#34;&gt;6.3 Metrics&lt;/a&gt;
          &lt;ul&gt;
            &lt;li&gt;&lt;a href=&#34;#631-mean-squared-errormse&#34;&gt;6.3.1 Mean squared error(MSE)&lt;/a&gt;&lt;/li&gt;
            &lt;li&gt;&lt;a href=&#34;#632-root-mean-squared-errorrmse&#34;&gt;6.3.2 Root mean squared error(RMSE)&lt;/a&gt;&lt;/li&gt;
            &lt;li&gt;&lt;a href=&#34;#633-r-squaredr2-score&#34;&gt;6.3.3 R-Squared(R2) Score&lt;/a&gt;&lt;/li&gt;
          &lt;/ul&gt;
        &lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#64-baseline-linear-regression-model&#34;&gt;6.4 Baseline Linear Regression model&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#65-other-regression-models&#34;&gt;6.5 Other regression models&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#66-results&#34;&gt;6.6 Results&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#7-conclusion&#34;&gt;7. Conclusion&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#8-acknowledgments&#34;&gt;8. Acknowledgments&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#9-references&#34;&gt;9. References&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Keywords:&lt;/strong&gt; power usage, big data, regression&lt;/p&gt;
&lt;h2 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;Electricity is an inevitable part of our day-to-day life. The residential power sector consumes about one-fifth of the total energy in the U.S. economy&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. Most of the appliances in a household use electricity for its working. The usage of electricity in a residence depends on the standard of living of the country, weather conditions, family size, type of residence, etc&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. Most of the houses in the USA are equipped with lightings and refrigerators using electric power. The usage of air conditioners is also increasing. From Figure 1, we can see that the top three categories for energy consumption are air conditioning, space heating, water heating as of 2015.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/chart.png&#34; alt=&#34;Figure 1&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Residential electricity consumption by end use, 2015&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;Climate change is one of the biggest challenges in our current time. As a result, temperatures are rising. Therefore, to analyze energy consumption, understanding weather variations are critical&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;. As the temperature rises, the use of air conditioners is also rising. As shown in Figure 1, air conditioning is the primary source of power consumption in households. The weather change has also resulted in a drop in temperatures and variation in humidity. These results in secondary power consumers.&lt;/p&gt;
&lt;p&gt;Even though weather plays an important role in power usage, factors like household income, age of the residents, family type, etc also influence consumption. During the holidays&amp;rsquo; many people tend to spend time outside which reduces power utilization at their homes. Similarly, during the weekend, since most people have work off, the appliances will be frequently consumed compared to weekdays when they go to work. Our world is currently facing an epidemic. Most of the countries had months of lockdown periods. Schools and many workplaces were closed. People were not allowed to go out and so they were stuck in their homes. As a result, power expending reduced drastically everywhere other than residences. But during the lockdown period, the energy consumption of residences spiked.&lt;/p&gt;
&lt;p&gt;Most of the electric service providers like Duke, Dominion provide customers their consumption data so that customers are aware of their usages. Some providers give predictions on their future usages so that they are prepared.&lt;/p&gt;
&lt;h2 id=&#34;2-reason-to-choose-this-dataset&#34;&gt;2. Reason to choose this dataset&lt;/h2&gt;
&lt;p&gt;There were many datasets on residential power usage analysis in Kaggle itself. But most of them were three or four years old. This dataset has the recent data of power consumptions together with weather data of each day. Since the pandemic hit the world in 2019-2020, the availability of recent data is considered to be significant for the analysis.&lt;/p&gt;
&lt;p&gt;This dataset is chosen because,&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It has the latest power usage data - till August 2020.&lt;/li&gt;
&lt;li&gt;It has marked covid lockdown, vacations, weekdays and weekends which is a challenge for the prediction.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;3-datasets&#34;&gt;3. Datasets&lt;/h2&gt;
&lt;p&gt;This project is based on the dataset, &lt;em&gt;Residential Power Usage 3 years data&lt;/em&gt; in Kaggle datasets&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;. The dataset contains data of hourly power consumption of a 2 storied house in Houston, Texas from 01-06-2016 to August 2020 and also weather conditions of each day like temperatures, humidity wind etc of that area. Each day is marked whether it is a weekday, weekend, vacation or COVID-19 lockdown.&lt;/p&gt;
&lt;p&gt;The project is intending to build a model to predict the future power consumption of a house with similar environments from the available data. Python&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt; is used for the development and since the expected output is a continuous variable, linear regression is considered for the baseline model. Later the performance of the base model is compared to one or two other models like tuned linear regression, gradient boosting, Light Gbm, or random forest.&lt;/p&gt;
&lt;p&gt;Data is spread across two csv files.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;power_usage_2016_to_2020.csv&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This file depicts the hourly electricity usage of the house for three years, from 2016 to 2020. It contains basic details like startdate with hour, the value of power consumption in kwh, day of the week and notes. It has 4 features and 35953 instances.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/fig-1.png&#34; alt=&#34;Figure 2&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 2:&lt;/strong&gt; First five rows of power_usage_2016_to_2020 data&lt;/p&gt;
&lt;p&gt;Figure 2 provides a snapshot of the first few rows of the data. Day of the week is an integer value with 0 being Monday. The column &lt;em&gt;notes&lt;/em&gt; layout details like whether that day was weekend, weekday, covid lockdown or vacation, as shown in Figure 3.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/fig-2.png&#34; alt=&#34;Figure 3&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 3:&lt;/strong&gt; Details in notes column&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;weather_2016_2020_daily.csv&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The second file or the weather file imparts the weather conditions of that particular area on each day. It has 19 features and 1553 instances. Figure 4 is the snapshot of the first few rows and columns of this file.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/fig-3.png&#34; alt=&#34;Figure 4&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 4:&lt;/strong&gt; First few rows of weather_2016_2020_daily data&lt;/p&gt;
&lt;p&gt;Each feature in this data has different units and the units of the features are given in Table 1.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Table 1:&lt;/strong&gt; Feature units&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Feature names&lt;/th&gt;
&lt;th&gt;Units&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Temperature&lt;/td&gt;
&lt;td&gt;F deg&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Dew Point&lt;/td&gt;
&lt;td&gt;F deg&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Humidity&lt;/td&gt;
&lt;td&gt;%age&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Wind&lt;/td&gt;
&lt;td&gt;mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Pressure&lt;/td&gt;
&lt;td&gt;Hg&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Precipitation&lt;/td&gt;
&lt;td&gt;inch&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Weather file has additional features like &lt;em&gt;date&lt;/em&gt; and &lt;em&gt;day&lt;/em&gt; of the date.&lt;/p&gt;
&lt;h2 id=&#34;4-data-preprocessing&#34;&gt;4. Data preprocessing&lt;/h2&gt;
&lt;p&gt;The data has to be preprocessed before modelling for predictions.&lt;/p&gt;
&lt;h3 id=&#34;41-data-download-and-load&#34;&gt;4.1 Data download and load&lt;/h3&gt;
&lt;p&gt;The data in this project is directly downloaded from &lt;em&gt;Kaggle&lt;/em&gt;. The downloaded file is then unzipped and loaded to two dataframes using python codes. For more detailed explanation and codes for download and load of data, see &lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-314/blob/main/project/code/residential_power_usage_prediction.ipynb&#34;&gt;python code&lt;/a&gt; &lt;em&gt;Download datasets&lt;/em&gt; and &lt;em&gt;Load datasets&lt;/em&gt; sections.&lt;/p&gt;
&lt;h3 id=&#34;42-data-descriptive-analysis&#34;&gt;4.2 Data descriptive analysis&lt;/h3&gt;
&lt;p&gt;The data loaded has to be analyzed properly before it can be preprocessed. An analysis is made on the existence of missing values, the range of each feature, etc. On analysis, it is determined that there are no missing values and the date format in both tables is different. The &lt;em&gt;StartDate&lt;/em&gt; feature of the power_usage dataset and &lt;em&gt;Date&lt;/em&gt; feature of the weather dataset is to be used as a key to merge the two datasets. But the format of both features is different. StartDate feature is the combination of date and hour. Whereas, &lt;em&gt;Date&lt;/em&gt; feature of weather is just the date. Hence, these issues will have to be taken care of before merging data.&lt;/p&gt;
&lt;h3 id=&#34;43-preprocessing-data&#34;&gt;4.3 Preprocessing data&lt;/h3&gt;
&lt;p&gt;In this step, the column name &lt;em&gt;Values (kWh)&lt;/em&gt;  is renamed to &lt;em&gt;Value&lt;/em&gt; and also date format issue is addressed. Firstly, StartDate column is split into Date and Hour columns. Since the StartDate column is in Pandas Period type, the function strftime() is used for converting to the required format.&lt;/p&gt;
&lt;h3 id=&#34;44-merge-datasets&#34;&gt;4.4 Merge datasets&lt;/h3&gt;
&lt;p&gt;For proper analysis of data, it is critical that the analyst should be able to analyze the relationships of each feature concerning the target feature(Value in kWh in this project). Therefore, both power_usage and weather tables are merged with respect to the Date column. The resulting table has a total of 35952 instances and 22 features.&lt;/p&gt;
&lt;h2 id=&#34;5-exploratory-data-analysis&#34;&gt;5. Exploratory Data Analysis&lt;/h2&gt;
&lt;p&gt;Here we analyze different features, their relationship with each other, and with the target.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/dow.png&#34; alt=&#34;Figure 5&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 5:&lt;/strong&gt; Average power usage by day of the week&lt;/p&gt;
&lt;p&gt;In Figure 5, the average power usage by the day of the week is plotted&lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;. It is analyzed that Saturday and Friday have the most usage compared to other days of the week. Since the day of the week represents values Sunday-Saturday, we can consider it as a categorical feature.
Similarly, from Figure 6, there is a huge dip in power usage during vacation. Other three occasions like covid lockdown, weekend and weekdays have almost the same power usage, even though consumption during weekends outweigh.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/tod.png&#34; alt=&#34;Figure 6&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 6:&lt;/strong&gt; Average power usage by type of the day&lt;/p&gt;
&lt;p&gt;In Figure 7, we compare the monthly power consumption for three years - 2018, 2019, 2020&lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;. The overall power usage in 2019 is less compared to 2018. But in 2020 may be due to Covid-lockdown the power consumption shoots. Also, power consumption peaks in the months of June, July, and August.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/monthly_power.png&#34; alt=&#34;Figure 7&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 7:&lt;/strong&gt; Average power usage per month for three years&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/corr_plot.png&#34; alt=&#34;Figure 8&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 8:&lt;/strong&gt; Correlation plot between features&lt;/p&gt;
&lt;p&gt;The correlation plot in Figure 8, depicts the inter-correlation between features. We can see that features like temperature, dew and pressure has a high correlation to our target feature. Also, different temperatures and dew features are inter-correlated. Therefore, all the intercorrelated features except for temp_avg can be dropped during feature selection.&lt;/p&gt;
&lt;h2 id=&#34;6-modeling&#34;&gt;6. Modeling&lt;/h2&gt;
&lt;p&gt;Modeling of the data includes splitting data into train and test, include cross-validation, create pipelines, select metrics for measuring performance, run data in regression models, and discuss results.&lt;/p&gt;
&lt;h3 id=&#34;61-split-data&#34;&gt;6.1 Split Data&lt;/h3&gt;
&lt;p&gt;For measuring the accuracy of the model, the main data is split into train and test. 20% of data is selected as test data and the remaining 80% is the train data. The proportion of notes(vacation, weekday, weekend, and covid lockdown) are different. Therefore, we stratify the data according to the notes column. After the split, train data has 28761 rows and test data has 7191 rows.&lt;/p&gt;
&lt;h3 id=&#34;62-pipelines&#34;&gt;6.2 Pipelines&lt;/h3&gt;
&lt;p&gt;Categorical variables and numeric variables are separated and processed in pipelines separately.
Categorical features are one hot encoded before feeding to the model. Similarly, numerical features are standardized before modeling. Later these two pipelines are joined and modeled used Linear regression and other models.&lt;/p&gt;
&lt;h3 id=&#34;63-metrics&#34;&gt;6.3 Metrics&lt;/h3&gt;
&lt;p&gt;Our target is a continuous variable and hence we implement regression models for prediction. To determine how accurate a regression model is, we use the following metrics.&lt;/p&gt;
&lt;h4 id=&#34;631-mean-squared-errormse&#34;&gt;6.3.1 Mean squared error(MSE)&lt;/h4&gt;
&lt;p&gt;MSE is the average of squares of error. The larger the MSE score, the larger the errors are. Models with lower values of MSE is considered to perform well. But, since MSE is the squared value, the scale of the target variable and MSE will be different. Therefore, we go for RMSE values.&lt;/p&gt;
&lt;h4 id=&#34;632-root-mean-squared-errorrmse&#34;&gt;6.3.2 Root mean squared error(RMSE)&lt;/h4&gt;
&lt;p&gt;RMSE is the square root of MSE scores. The square root is introduced to make the scale of the errors to be the same as the scale of targets. Similar to MSE, the lower scores for RMSE means better model performance. Therefore, in this project, the models with lower RMSE values will be monitored&lt;sup id=&#34;fnref:9&#34;&gt;&lt;a href=&#34;#fn:9&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h4 id=&#34;633-r-squaredr2-score&#34;&gt;6.3.3 R-Squared(R2) Score&lt;/h4&gt;
&lt;p&gt;R2 score is the goodness-of-fit measure. It&amp;rsquo;s a statistical measure that ranges between 0 and 1. R2 score helps the analyst to understand how similar the fitted line is to the data it is fitted to. The closer it is to one, the more likely the model predicts its variance. Similarly, if the score is zero, the model doesn&amp;rsquo;t predict any variance.
In this project, the R2 score of the test data is calculated. The model with the highest R2 scores will be considered&lt;sup id=&#34;fnref:9&#34;&gt;&lt;a href=&#34;#fn:9&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h3 id=&#34;64-baseline-linear-regression-model&#34;&gt;6.4 Baseline Linear Regression model&lt;/h3&gt;
&lt;p&gt;We use linear regression as our baseline model. For the baseline model, we are not hyperparameter tuning. For the baseline model, the train RMSE score was 0.6783, and R2 for the test set was 0.4460. These values are then compared to other regression models with hyperparameter tuning.&lt;/p&gt;
&lt;h3 id=&#34;65-other-regression-models&#34;&gt;6.5 Other regression models&lt;/h3&gt;
&lt;p&gt;After developing a baseline model, we are developing four other regression models and comparing the results. We implement feature selection and hyperparameter tuning. As we analyzed in exploratory data analysis, some features have strong inter-correlation and these features are dropped. The parameters for the regression models are hyper tuned and modeled in GridsearchCV of sklearn package.&lt;/p&gt;
&lt;p&gt;The models used for prediction are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linear regression with hyperparameter tuning&lt;/li&gt;
&lt;li&gt;Gradient boosting&lt;/li&gt;
&lt;li&gt;XGBoost&lt;/li&gt;
&lt;li&gt;Light GBM&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Similar to the baseline model, the metrics like train RMSE, test RMSE, and test R2 scores are calculated.&lt;/p&gt;
&lt;h3 id=&#34;66-results&#34;&gt;6.6 Results&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-314/raw/main/project/images/result.png&#34; alt=&#34;Figure 9&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 9:&lt;/strong&gt; Performance of all the regression models&lt;/p&gt;
&lt;p&gt;Figure 9 documents the performance of all the regression models used.&lt;/p&gt;
&lt;p&gt;cloudmesh.common benchmark and stopwatch framework are used to monitor and record the time taken for each step in this project&lt;sup id=&#34;fnref:10&#34;&gt;&lt;a href=&#34;#fn:10&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;10&lt;/a&gt;&lt;/sup&gt;. Time taken for critical steps like downloading data, loading data, preprocessing data, training and predictions of each model are recorded. The StopWatch recordings are shown in Table 2. StopWatch recordings played an important role in the selection of the best model. Benchmark also provides a detailed report on the system or device information as shown in Table 2.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Table 2:&lt;/strong&gt; Benchmark results&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Attribute&lt;/th&gt;
&lt;th&gt;Value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;BUG_REPORT_URL&lt;/td&gt;
&lt;td&gt;&amp;ldquo;&lt;a href=&#34;https://bugs.launchpad.net/ubuntu/%22&#34;&gt;https://bugs.launchpad.net/ubuntu/&amp;quot;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DISTRIB_CODENAME&lt;/td&gt;
&lt;td&gt;bionic&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DISTRIB_DESCRIPTION&lt;/td&gt;
&lt;td&gt;&amp;ldquo;Ubuntu 18.04.5 LTS&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DISTRIB_ID&lt;/td&gt;
&lt;td&gt;Ubuntu&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DISTRIB_RELEASE&lt;/td&gt;
&lt;td&gt;18.04&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;HOME_URL&lt;/td&gt;
&lt;td&gt;&amp;ldquo;&lt;a href=&#34;https://www.ubuntu.com/%22&#34;&gt;https://www.ubuntu.com/&amp;quot;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ID&lt;/td&gt;
&lt;td&gt;ubuntu&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ID_LIKE&lt;/td&gt;
&lt;td&gt;debian&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NAME&lt;/td&gt;
&lt;td&gt;&amp;ldquo;Ubuntu&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;PRETTY_NAME&lt;/td&gt;
&lt;td&gt;&amp;ldquo;Ubuntu 18.04.5 LTS&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;PRIVACY_POLICY_URL&lt;/td&gt;
&lt;td&gt;&amp;ldquo;&lt;a href=&#34;https://www.ubuntu.com/legal/terms-and-policies/privacy-policy%22&#34;&gt;https://www.ubuntu.com/legal/terms-and-policies/privacy-policy&amp;quot;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SUPPORT_URL&lt;/td&gt;
&lt;td&gt;&amp;ldquo;&lt;a href=&#34;https://help.ubuntu.com/%22&#34;&gt;https://help.ubuntu.com/&amp;quot;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;UBUNTU_CODENAME&lt;/td&gt;
&lt;td&gt;bionic&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;VERSION&lt;/td&gt;
&lt;td&gt;&amp;ldquo;18.04.5 LTS (Bionic Beaver)&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;VERSION_CODENAME&lt;/td&gt;
&lt;td&gt;bionic&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;VERSION_ID&lt;/td&gt;
&lt;td&gt;&amp;ldquo;18.04&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;cpu_count&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mem.active&lt;/td&gt;
&lt;td&gt;1.0 GiB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mem.available&lt;/td&gt;
&lt;td&gt;11.2 GiB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mem.free&lt;/td&gt;
&lt;td&gt;8.5 GiB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mem.inactive&lt;/td&gt;
&lt;td&gt;2.6 GiB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mem.percent&lt;/td&gt;
&lt;td&gt;11.6 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mem.total&lt;/td&gt;
&lt;td&gt;12.7 GiB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mem.used&lt;/td&gt;
&lt;td&gt;1.9 GiB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;platform.version&lt;/td&gt;
&lt;td&gt;#1 SMP Thu Jul 23 08:00:38 PDT 2020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;python&lt;/td&gt;
&lt;td&gt;3.6.9 (default, Oct  8 2020, 12:12:24)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;[GCC 8.4.0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;python.pip&lt;/td&gt;
&lt;td&gt;19.3.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;python.version&lt;/td&gt;
&lt;td&gt;3.6.9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;sys.platform&lt;/td&gt;
&lt;td&gt;linux&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;uname.machine&lt;/td&gt;
&lt;td&gt;x86_64&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;uname.node&lt;/td&gt;
&lt;td&gt;a1f46a7ed3c2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;uname.processor&lt;/td&gt;
&lt;td&gt;x86_64&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;uname.release&lt;/td&gt;
&lt;td&gt;4.19.112+&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;uname.system&lt;/td&gt;
&lt;td&gt;Linux&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;uname.version&lt;/td&gt;
&lt;td&gt;#1 SMP Thu Jul 23 08:00:38 PDT 2020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;user&lt;/td&gt;
&lt;td&gt;collab&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;Status&lt;/th&gt;
&lt;th&gt;Time&lt;/th&gt;
&lt;th&gt;Sum&lt;/th&gt;
&lt;th&gt;Start&lt;/th&gt;
&lt;th&gt;tag&lt;/th&gt;
&lt;th&gt;Node&lt;/th&gt;
&lt;th&gt;User&lt;/th&gt;
&lt;th&gt;OS&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Data download&lt;/td&gt;
&lt;td&gt;ok&lt;/td&gt;
&lt;td&gt;2.652&lt;/td&gt;
&lt;td&gt;2.652&lt;/td&gt;
&lt;td&gt;2020-11-30 12:43:50&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;a1f46a7ed3c2&lt;/td&gt;
&lt;td&gt;collab&lt;/td&gt;
&lt;td&gt;Linux&lt;/td&gt;
&lt;td&gt;#1 SMP Thu Jul 23 08:00:38 PDT 2020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Data load&lt;/td&gt;
&lt;td&gt;ok&lt;/td&gt;
&lt;td&gt;0.074&lt;/td&gt;
&lt;td&gt;0.074&lt;/td&gt;
&lt;td&gt;2020-11-30 12:43:53&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;a1f46a7ed3c2&lt;/td&gt;
&lt;td&gt;collab&lt;/td&gt;
&lt;td&gt;Linux&lt;/td&gt;
&lt;td&gt;#1 SMP Thu Jul 23 08:00:38 PDT 2020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Data preprocessing&lt;/td&gt;
&lt;td&gt;ok&lt;/td&gt;
&lt;td&gt;67.618&lt;/td&gt;
&lt;td&gt;67.618&lt;/td&gt;
&lt;td&gt;2020-11-30 12:43:53&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;a1f46a7ed3c2&lt;/td&gt;
&lt;td&gt;collab&lt;/td&gt;
&lt;td&gt;Linux&lt;/td&gt;
&lt;td&gt;#1 SMP Thu Jul 23 08:00:38 PDT 2020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Baseline Linear Regression&lt;/td&gt;
&lt;td&gt;ok&lt;/td&gt;
&lt;td&gt;2.814&lt;/td&gt;
&lt;td&gt;2.814&lt;/td&gt;
&lt;td&gt;2020-11-30 12:45:03&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;a1f46a7ed3c2&lt;/td&gt;
&lt;td&gt;collab&lt;/td&gt;
&lt;td&gt;Linux&lt;/td&gt;
&lt;td&gt;#1 SMP Thu Jul 23 08:00:38 PDT 2020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Linear Regression&lt;/td&gt;
&lt;td&gt;ok&lt;/td&gt;
&lt;td&gt;5.581&lt;/td&gt;
&lt;td&gt;5.581&lt;/td&gt;
&lt;td&gt;2020-11-30 12:45:06&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;a1f46a7ed3c2&lt;/td&gt;
&lt;td&gt;collab&lt;/td&gt;
&lt;td&gt;Linux&lt;/td&gt;
&lt;td&gt;#1 SMP Thu Jul 23 08:00:38 PDT 2020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Gradient Boosting&lt;/td&gt;
&lt;td&gt;ok&lt;/td&gt;
&lt;td&gt;244.868&lt;/td&gt;
&lt;td&gt;244.868&lt;/td&gt;
&lt;td&gt;2020-11-30 12:45:12&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;a1f46a7ed3c2&lt;/td&gt;
&lt;td&gt;collab&lt;/td&gt;
&lt;td&gt;Linux&lt;/td&gt;
&lt;td&gt;#1 SMP Thu Jul 23 08:00:38 PDT 2020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;XGBoost&lt;/td&gt;
&lt;td&gt;ok&lt;/td&gt;
&lt;td&gt;2946.7&lt;/td&gt;
&lt;td&gt;2946.7&lt;/td&gt;
&lt;td&gt;2020-11-30 12:49:16&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;a1f46a7ed3c2&lt;/td&gt;
&lt;td&gt;collab&lt;/td&gt;
&lt;td&gt;Linux&lt;/td&gt;
&lt;td&gt;#1 SMP Thu Jul 23 08:00:38 PDT 2020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Light GBM&lt;/td&gt;
&lt;td&gt;ok&lt;/td&gt;
&lt;td&gt;770.967&lt;/td&gt;
&lt;td&gt;770.967&lt;/td&gt;
&lt;td&gt;2020-11-30 13:38:23&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;a1f46a7ed3c2&lt;/td&gt;
&lt;td&gt;collab&lt;/td&gt;
&lt;td&gt;Linux&lt;/td&gt;
&lt;td&gt;#1 SMP Thu Jul 23 08:00:38 PDT 2020&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;For the baseline model, the RMSE values were high and R2 scores were small compared to all other regression models. The hyperparameter tuned linear regression model scores are better compared to the baseline model. But the other three models outweigh both linear models. XGBoost has the lowest RMSE and highest R2 score of all other models. But the time taken for execution is too long. Therefore, XGBoost is computationally expensive which leads us to ignore its scores. Gradient boosting and Light GBM have similar scores and hence the time taken for execution has to be considered as the deciding factor here. Gradient boosting completed 135 fits in 244.868 seconds whereas LightGBM took around 770.967 seconds for executing 3645 fits and then prediction. Since per fit execution time for Light GBM is too small, we consider Light GBM as the best model for predicting daily power usage of a residence with similar background conditions.&lt;/p&gt;
&lt;p&gt;The RMSE scores for Light GBM are .2896 for train and .2910 for the test. The R2 score for the test set is .6526.&lt;/p&gt;
&lt;h2 id=&#34;7-conclusion&#34;&gt;7. Conclusion&lt;/h2&gt;
&lt;p&gt;As the importance of electricity is increasing, the need to know how or where the power usage increase will be a lifesaver for the electricity consumers. In this project, the daily power consumption of a house is analyzed and modeled for a prediction of electricity usage for residences with similar environments. The model considered a set of parameters like weather conditions, weekdays, type of days, etc. for prediction. Since the output is power consumption in kWh, we selected regression for modeling and prediction. Experiments are conducted on five regression models. After analyzing the experiment results, we concluded that the performance of the Light GBM model is better and faster compared to all other models.&lt;/p&gt;
&lt;h2 id=&#34;8-acknowledgments&#34;&gt;8. Acknowledgments&lt;/h2&gt;
&lt;p&gt;The author would like to express special thanks to Dr. Geoffrey Fox, Dr. Gregor Von Laszewski, and all the associate instructors of the Big Data Applications course (FA20-BL-ENGR-E534-11530) offered by Indiana University, Bloomington for their continuous guidance and support throughout the project.&lt;/p&gt;
&lt;h2 id=&#34;9-references&#34;&gt;9. References&lt;/h2&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Jia Li and Richard E. Just, Modeling household energy consumption and adoption of energy efficient technology, Energy Economics, vol. 72, pp. 404-415, 2018.
Available: &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0140988318301440#bbb0180&#34;&gt;https://www.sciencedirect.com/science/article/pii/S0140988318301440#bbb0180&lt;/a&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Domestic Power Consumption, [Online resource] &lt;a href=&#34;https://en.wikipedia.org/wiki/Domestic_energy_consumption&#34;&gt;https://en.wikipedia.org/wiki/Domestic_energy_consumption&lt;/a&gt; &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Use of energy explained - Energy use in homes, [Online resource] &lt;a href=&#34;https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php&#34;&gt;https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php&lt;/a&gt; &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Yating Li, William A. Pizer, and Libo Wu, Climate change and residential electricity consumption in the Yangtze River Delta, China, Research article, Available: &lt;a href=&#34;https://www.pnas.org/content/116/2/472#ref-1&#34;&gt;https://www.pnas.org/content/116/2/472#ref-1&lt;/a&gt; &lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Residential Power Usage dataset, &lt;a href=&#34;https://www.kaggle.com/srinuti/residential-power-usage-3years-data-timeseries&#34;&gt;https://www.kaggle.com/srinuti/residential-power-usage-3years-data-timeseries&lt;/a&gt; &lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Residential Power Usage Prediction script, &lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-314/blob/main/project/code/residential_power_usage_prediction.ipynb&#34;&gt;https://github.com/cybertraining-dsc/fa20-523-314/blob/main/project/code/residential_power_usage_prediction.ipynb&lt;/a&gt; &lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;seaborn: statistical data visualization, &lt;a href=&#34;https://seaborn.pydata.org/index.html&#34;&gt;https://seaborn.pydata.org/index.html&lt;/a&gt; &lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:8&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Group by: split-apply-combine, &lt;a href=&#34;https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html&#34;&gt;https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html&lt;/a&gt; &lt;a href=&#34;#fnref:8&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:9&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Mean Square Error &amp;amp; R2 Score Clearly Explained, [Online resource] &lt;a href=&#34;https://www.bmc.com/blogs/mean-squared-error-r2-and-variance-in-regression-analysis/&#34;&gt;https://www.bmc.com/blogs/mean-squared-error-r2-and-variance-in-regression-analysis/&lt;/a&gt; &lt;a href=&#34;#fnref:9&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:10&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Gregor von Laszewski, Cloudmesh StopWatch and Benchmark from the Cloudmesh Common Library, &lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-common&#34;&gt;https://github.com/cloudmesh/cloudmesh-common&lt;/a&gt; &lt;a href=&#34;#fnref:10&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;

      </description>
    </item>
    
  </channel>
</rss>
