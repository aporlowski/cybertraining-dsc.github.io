<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Cybertraining – Report</title>
    <link>/report/</link>
    <description>Recent content in Report on Cybertraining</description>
    <generator>Hugo -- gohugo.io</generator>
    
	  <atom:link href="/report/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Report: </title>
      <link>/report/cloudmesh-openapi/cloudmesh/openapi/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/cloudmesh-openapi/cloudmesh/openapi/readme/</guid>
      <description>
        
        
        &lt;h1 id=&#34;openapi-function-generator&#34;&gt;Openapi Function generator&lt;/h1&gt;
&lt;h2 id=&#34;activity-log&#34;&gt;Activity Log&lt;/h2&gt;
&lt;h2 id=&#34;week-of-mar-9---mar-16&#34;&gt;Week of Mar 9 - Mar 16&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Andrew Goldfarb&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Worked with Ishan and Jonathan to finalize the start stop
functionality.&lt;/li&gt;
&lt;li&gt;Added functionality to delete the process entry from the
registry upon stop command.&lt;/li&gt;
&lt;li&gt;Debugged weird start error for my personal machine where the
start functionality was running two bash terminals causing the
start function to fail.&lt;/li&gt;
&lt;li&gt;Met with Professor to discuss proper implementation of the
start/stop and how to tie into registry functionality.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;week-prior-to-mar-9th&#34;&gt;Week prior to Mar 9th&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;bkgerreis&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Jonathan Beckford&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Prateek&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Andrew Goldfarb&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;Edited the stop function to take process PID and use os.kill to
stop the process based on the name of the python file. However,
according to Ishan this is still not working.&lt;/li&gt;
&lt;li&gt;Resolved conflicts between master and our working branch&lt;/li&gt;
&lt;li&gt;Began work on assigning a default name if the user does not provide
one for server start. Potetially, a function to assign an alias
name to the whole process to amke it easier to reference.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;install-for-development&#34;&gt;Install for development&lt;/h2&gt;
&lt;p&gt;cloudmesh-installer git pull analytics&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd cloudmesh-openapi
pip install -e .
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;keep-up-to-date&#34;&gt;Keep up to date&lt;/h2&gt;
&lt;p&gt;explain how to set up and use upstream sync&lt;/p&gt;
&lt;h3 id=&#34;project-meeting&#34;&gt;Project Meeting&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://iu.zoom.us/rec/share/4dIpJZ-p8ztIHpH_q1HAZ6wzL6iiaaa8h3QX8_YMzRkn8tBfY_mRIe8z3j-3cZ_9?startTime=1581987567000&#34;&gt;Mon 17 Feb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://iu.zoom.us/rec/share/_8ZLKK7Z6zpLb53f73_UW4EFBY_iX6a8gydM_vVbzRu2MhrC_sUCKhChUkLzgEK8?startTime=1582591839000&#34;&gt;Mon 24 Feb&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;basic-function-generator&#34;&gt;Basic Function Generator&lt;/h3&gt;
&lt;h4 id=&#34;prateek-shaw----code-link&#34;&gt;Prateek Shaw -  code link.&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cloudmesh-community/sp20-516-229/tree/master/cloudmesh-openapi&#34;&gt;https://github.com/cloudmesh-community/sp20-516-229/tree/master/cloudmesh-openapi&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;created a basic function that will return the OpenAPI YAML file
of given python function including parameters.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;sp20-516-237----jonathan-beckford&#34;&gt;SP20-516-237 &amp;ndash; Jonathan Beckford&lt;/h4&gt;
&lt;p&gt;I created a class that generates the OpenAPI yaml file. I also created
a sample program that defines an example function, instantiates my
OpenAPI generator class and passes in the sample function as input. I
figured this would make things really easy to just paste any new
sample function for testing purposes. I also included the parameters
as was requested. I also ran my output yaml through the swagger
validator (&lt;a href=&#34;https://editor.swagger.io/&#34;&gt;https://editor.swagger.io/&lt;/a&gt;) to make sure it was compliant
and it was.&lt;br&gt;
&lt;a href=&#34;https://github.com/cloudmesh-community/sp20-516-237/tree/master/projectAI/generateOpenAPI&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;sp20-516-231---brian-kegerreis&#34;&gt;sp20-516-231 - Brian Kegerreis&lt;/h4&gt;
&lt;p&gt;I created a function to generate an OpenAPI spec including a rough
attempt at response types (only supports text/plain media types at
this point)
&lt;a href=&#34;https://github.com/cloudmesh-community/sp20-516-231/blob/master/openapi-exercises/example_echo.py&#34;&gt;https://github.com/cloudmesh-community/sp20-516-231/blob/master/openapi-exercises/example_echo.py&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;server-start&#34;&gt;Server Start&lt;/h3&gt;
&lt;h4 id=&#34;andrew-goldfarb---sp20-516-234&#34;&gt;Andrew Goldfarb - SP20-516-234&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cloudmesh-community/sp20-516-234/tree/open-api-exercise/openAPI&#34;&gt;https://github.com/cloudmesh-community/sp20-516-234/tree/open-api-exercise/openAPI&lt;/a&gt;
I have created a basic function that returns the IP address of the
server running the function to tell if it is running on the device
itself or connected to the internet running while running the
function.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/cloudmesh-openapi/cloudmesh/openapi/scikitlearn/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/cloudmesh-openapi/cloudmesh/openapi/scikitlearn/readme/</guid>
      <description>
        
        
        &lt;h1 id=&#34;sklearngeneratorfile-high-level-overview&#34;&gt;SKlearnGeneratorFile High level Overview&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The SklearnGeneratorFile.py is the generator function which outputs the python file for given
Sckit-learn Library.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The function takes two inputs&lt;/p&gt;
&lt;p&gt;1.input_sklibrary&lt;/p&gt;
&lt;p&gt;2.model_tag&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Examples of the inputs are&lt;/p&gt;
&lt;p&gt;input_sklibrary = sklearn.linear_model.LinearRegression(Full model specification)
model_tag = any name which you want the tag the model instance like LinReg1&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;This Version of Scikit-learn service accepts csv files in UTF-8 format only.It is the user responsibility to make
sure the files are in UTF-8 format.It is the user responsibility to split the data in to train and test datasets.
Split data functionality is not currently supported.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;scikit-learn uses numpydoc format in the docstring so the scraping of the parameters and docstrings
are done using docscrape from numpydoc.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;All the templates used in the code are based on X and y inputs scikit-learn takes and also based on the
return type&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;pytests-for-scikit-learn-tests&#34;&gt;Pytests for Scikit learn tests.&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The below pytest generates the .py file used by generator to do a OPENAPI specification.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/blob/master/tests/Scikitlearn-tests/test_06c_sklearngeneratortest.py&#34;&gt;Pytestcode&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt; pytest -v --capture&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;no tests/Scikitlearn_tests/test_06c_sklearngeneratortest.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The below pytest tests the methods generated .&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/blob/master/tests/Scikitlearn-tests/test_06d_sklearngeneratortest.py&#34;&gt;Pytestcode&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;pytest -v --capture&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;no tests/Scikitlearn_tests/test_06d_sklearngeneratortest.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/cloudmesh-openapi/deprecated/openapi/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/cloudmesh-openapi/deprecated/openapi/readme/</guid>
      <description>
        
        
        &lt;h1 id=&#34;openapi-function-generator&#34;&gt;Openapi Function generator&lt;/h1&gt;
&lt;h2 id=&#34;activity-log&#34;&gt;Activity Log&lt;/h2&gt;
&lt;h2 id=&#34;week-of-mar-9---mar-16&#34;&gt;Week of Mar 9 - Mar 16&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Andrew Goldfarb&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Worked with Ishan and Jonathan to finalize the start stop
functionality.&lt;/li&gt;
&lt;li&gt;Added functionality to delete the process entry from the
registry upon stop command.&lt;/li&gt;
&lt;li&gt;Debugged weird start error for my personal machine where the
start functionality was running two bash terminals causing the
start function to fail.&lt;/li&gt;
&lt;li&gt;Met with Professor to discuss proper implementation of the
start/stop and how to tie into registry functionality.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;week-prior-to-mar-9th&#34;&gt;Week prior to Mar 9th&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;bkgerreis&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Jonathan Beckford&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Prateek&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Andrew Goldfarb&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;Edited the stop function to take process PID and use os.kill to
stop the process based on the name of the python file. However,
according to Ishan this is still not working.&lt;/li&gt;
&lt;li&gt;Resolved conflicts between master and our working branch&lt;/li&gt;
&lt;li&gt;Began work on assigning a default name if the user does not provide
one for server start. Potetially, a function to assign an alias
name to the whole process to amke it easier to reference.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;install-for-development&#34;&gt;Install for development&lt;/h2&gt;
&lt;p&gt;cloudmesh-installer git pull analytics&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd cloudmesh-openapi
pip install -e .
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;keep-up-to-date&#34;&gt;Keep up to date&lt;/h2&gt;
&lt;p&gt;explain how to set up and use upstream sync&lt;/p&gt;
&lt;h3 id=&#34;project-meeting&#34;&gt;Project Meeting&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://iu.zoom.us/rec/share/4dIpJZ-p8ztIHpH_q1HAZ6wzL6iiaaa8h3QX8_YMzRkn8tBfY_mRIe8z3j-3cZ_9?startTime=1581987567000&#34;&gt;Mon 17 Feb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://iu.zoom.us/rec/share/_8ZLKK7Z6zpLb53f73_UW4EFBY_iX6a8gydM_vVbzRu2MhrC_sUCKhChUkLzgEK8?startTime=1582591839000&#34;&gt;Mon 24 Feb&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;basic-function-generator&#34;&gt;Basic Function Generator&lt;/h3&gt;
&lt;h4 id=&#34;prateek-shaw----code-link&#34;&gt;Prateek Shaw -  code link.&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cloudmesh-community/sp20-516-229/tree/master/cloudmesh-openapi&#34;&gt;https://github.com/cloudmesh-community/sp20-516-229/tree/master/cloudmesh-openapi&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;created a basic function that will return the OpenAPI YAML file
of given python function including parameters.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;sp20-516-237----jonathan-beckford&#34;&gt;SP20-516-237 &amp;ndash; Jonathan Beckford&lt;/h4&gt;
&lt;p&gt;I created a class that generates the OpenAPI yaml file. I also created
a sample program that defines an example function, instantiates my
OpenAPI generator class and passes in the sample function as input. I
figured this would make things really easy to just paste any new
sample function for testing purposes. I also included the parameters
as was requested. I also ran my output yaml through the swagger
validator (&lt;a href=&#34;https://editor.swagger.io/&#34;&gt;https://editor.swagger.io/&lt;/a&gt;) to make sure it was compliant
and it was.
&lt;a href=&#34;https://github.com/cloudmesh-community/sp20-516-237/tree/master/projectAI/generateOpenAPI&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;sp20-516-231---brian-kegerreis&#34;&gt;sp20-516-231 - Brian Kegerreis&lt;/h4&gt;
&lt;p&gt;I created a function to generate an OpenAPI spec including a rough
attempt at response types (only supports text/plain media types at
this point)
&lt;a href=&#34;https://github.com/cloudmesh-community/sp20-516-231/blob/master/openapi-exercises/example_echo.py&#34;&gt;https://github.com/cloudmesh-community/sp20-516-231/blob/master/openapi-exercises/example_echo.py&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;server-start&#34;&gt;Server Start&lt;/h3&gt;
&lt;h4 id=&#34;andrew-goldfarb---sp20-516-234&#34;&gt;Andrew Goldfarb - SP20-516-234&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cloudmesh-community/sp20-516-234/tree/open-api-exercise/openAPI&#34;&gt;https://github.com/cloudmesh-community/sp20-516-234/tree/open-api-exercise/openAPI&lt;/a&gt;
I have created a basic function that returns the IP address of the
server running the function to tell if it is running on the device
itself or connected to the internet running while running the
function.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/cloudmesh-openapi/docker/ubuntu19.10/todo/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/cloudmesh-openapi/docker/ubuntu19.10/todo/</guid>
      <description>
        
        
        &lt;ul&gt;
&lt;li&gt;
&lt;p&gt;make clean: only delete the artifacts created here, the clean wipes
currently everything&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;when using this in consecutive order, would cms init not wipe out the data&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;should we not mount the .cloudmesh and other data dire into the conatiner.
This way we can use the host system for developments. Maybe we need to&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;support both ways&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;leverage cmsd
we have told the class that we have cmsd taht starts up cloudmesh in
a container. Develop a new directory docker-cmsd and use that&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/cloudmesh-openapi/docker/ubuntu20.04-sklearn/todo/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/cloudmesh-openapi/docker/ubuntu20.04-sklearn/todo/</guid>
      <description>
        
        
        &lt;ul&gt;
&lt;li&gt;
&lt;p&gt;THIS IS JUST THE SKELETON AND HAS NOT BEEN RUN ONCE IT HAS BUGS&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;THIS HAS NOT YET THE SKLERAN START STOP&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;make clean: only delete the artifacts created here, the clean wipes
currently everything&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;when using this in consecutive order, would cms init not wipe out the data&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;should we not mount the .cloudmesh and other data dire into the conatiner.
This way we can use the host system for developments. Maybe we need to&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;support both ways&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;leverage cmsd
we have told the class that we have cmsd taht starts up cloudmesh in
a container. Develop a new directory docker-cmsd and use that&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/cloudmesh-openapi/docker/ubuntu20.04/todo/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/cloudmesh-openapi/docker/ubuntu20.04/todo/</guid>
      <description>
        
        
        &lt;ul&gt;
&lt;li&gt;
&lt;p&gt;THIS IS JUST THE SKELETON AND HAS NOT BEEN RUN ONCE IT HAS BUGS&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;make clean: only delete the artifacts created here, the clean wipes
currently everything&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;when using this in consecutive order, would cms init not wipe out the data&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;should we not mount the .cloudmesh and other data dire into the conatiner.
This way we can use the host system for developments. Maybe we need to&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;support both ways&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;leverage cmsd
we have told the class that we have cmsd taht starts up cloudmesh in
a container. Develop a new directory docker-cmsd and use that&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/cloudmesh-openapi/project_review/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/cloudmesh-openapi/project_review/</guid>
      <description>
        
        
        &lt;h1 id=&#34;project-review&#34;&gt;Project Review&lt;/h1&gt;
&lt;h2 id=&#34;team-members&#34;&gt;Team members:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Jonathan Beckford&lt;/li&gt;
&lt;li&gt;Brian Kegerreis&lt;/li&gt;
&lt;li&gt;Prateek Shaw&lt;/li&gt;
&lt;li&gt;Jagadeesh Kandimalla&lt;/li&gt;
&lt;li&gt;Ishan Mishra&lt;/li&gt;
&lt;li&gt;Andrew G&lt;/li&gt;
&lt;li&gt;Falconi&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;project-documentation&#34;&gt;Project Documentation:&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://cloudmesh.github.io/cloudmesh-openapi/index.html&#34;&gt;https://cloudmesh.github.io/cloudmesh-openapi/index.html&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;contributors-based-on-git-tracking&#34;&gt;Contributors based on Git tracking&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;NOTE:&lt;/strong&gt;&lt;/em&gt; This is not completely accurate because some did not have git config done correctly.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/graphs/contributors&#34;&gt;https://github.com/cloudmesh/cloudmesh-openapi/graphs/contributors&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-breakdown&#34;&gt;Code Breakdown&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;cms command:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/blob/master/cloudmesh/openapi/command/openapi.py&#34;&gt;https://github.com/cloudmesh/cloudmesh-openapi/blob/master/cloudmesh/openapi/command/openapi.py&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contributors:&lt;/strong&gt; all team&lt;/p&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;cms generate - to generate server yaml&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;executor&lt;/strong&gt; that parses parameters and calls generator:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/blob/master/cloudmesh/openapi/function/executor.py&#34;&gt;https://github.com/cloudmesh/cloudmesh-openapi/blob/master/cloudmesh/openapi/function/executor.py&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contributors:&lt;/strong&gt;  Brian, Professor&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;generator&lt;/strong&gt; that generates the server yaml:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/blob/master/cloudmesh/openapi/function/generator.py&#34;&gt;https://github.com/cloudmesh/cloudmesh-openapi/blob/master/cloudmesh/openapi/function/generator.py&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contributors:&lt;/strong&gt;  Brian, Jonathan, Prateek&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;cms server - to start and stop server&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/blob/master/cloudmesh/openapi/function/server.py&#34;&gt;https://github.com/cloudmesh/cloudmesh-openapi/blob/master/cloudmesh/openapi/function/server.py&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Contributors:&lt;/strong&gt;  Jonathan, Andrew, Prateek, Ishan&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;cms registry - register the server and cache model&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;registry&lt;/strong&gt; - registers server&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/blob/master/cloudmesh/openapi/registry/Registry.py&#34;&gt;https://github.com/cloudmesh/cloudmesh-openapi/blob/master/cloudmesh/openapi/registry/Registry.py&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contributors:&lt;/strong&gt; Falconi, Praful, Professor&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;cache&lt;/strong&gt; - cache serialized model locally&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/blob/master/cloudmesh/openapi/registry/cache.py&#34;&gt;https://github.com/cloudmesh/cloudmesh-openapi/blob/master/cloudmesh/openapi/registry/cache.py&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contributors:&lt;/strong&gt; Jonathan&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;fileoperation&lt;/strong&gt; - upload input files&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/blob/master/cloudmesh/openapi/registry/fileoperation.py&#34;&gt;https://github.com/cloudmesh/cloudmesh-openapi/blob/master/cloudmesh/openapi/registry/fileoperation.py&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contributors:&lt;/strong&gt; Prateek, Brian&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;cms scikitlearn - generate sklearn functions&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/blob/master/cloudmesh/openapi/scikitlearn/SklearnGeneratorFile.py&#34;&gt;https://github.com/cloudmesh/cloudmesh-openapi/blob/master/cloudmesh/openapi/scikitlearn/SklearnGeneratorFile.py&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contributors:&lt;/strong&gt; Jagadeesh&lt;/p&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;cms image processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contributors:&lt;/strong&gt; Falconi, Ishan&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;cms text analysis&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contirbutor:&lt;/strong&gt; Andrew Goldfarb&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/tree/master/tests/generator-natural-lang&#34;&gt;https://github.com/cloudmesh/cloudmesh-openapi/tree/master/tests/generator-natural-lang&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;deployment-steps&#34;&gt;Deployment steps&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://cloudmesh.github.io/cloudmesh-openapi/README.html#installation&#34;&gt;https://cloudmesh.github.io/cloudmesh-openapi/README.html#installation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;quick-start&#34;&gt;Quick Start&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://cloudmesh.github.io/cloudmesh-openapi/README.html#quick-steps-to-generate-start-and-stop-cpu-sample-example&#34;&gt;https://cloudmesh.github.io/cloudmesh-openapi/README.html#quick-steps-to-generate-start-and-stop-cpu-sample-example&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;pytests&#34;&gt;Pytests&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://cloudmesh.github.io/cloudmesh-openapi/README.html#pytests&#34;&gt;https://cloudmesh.github.io/cloudmesh-openapi/README.html#pytests&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;limitations&#34;&gt;Limitations&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Integration of openapi with cms allows for running locally only.  Cloud integration was not fully completed although team did create a way to setup openapi in a VM using a remote script for &lt;a href=&#34;https://github.com/cloudmesh/get/blob/master/openapi/ubuntu18.04/index.html&#34;&gt;openstack&lt;/a&gt; and &lt;a href=&#34;https://github.com/cloudmesh/get/blob/master/openapi/google/index.html&#34;&gt;google&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The generator only supports creating arrays of number data type.  This limitation is due to the bug documented below in &lt;em&gt;&lt;strong&gt;Bugs&lt;/strong&gt;&lt;/em&gt; section.  So manual changes are required to the output yaml to allow for other data types until another work around is found or the bug is resolved.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;bugs&#34;&gt;Bugs&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;reported a bug to Connexion and documented it in github for future reference:
&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/issues/60&#34;&gt;https://github.com/cloudmesh/cloudmesh-openapi/issues/60&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;additional-artifacts-produced&#34;&gt;Additional artifacts produced:&lt;/h2&gt;
&lt;h3 id=&#34;openstack-vm-set-up-script&#34;&gt;Openstack VM set up script&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cloudmesh/get/blob/master/openapi/ubuntu18.04/index.html&#34;&gt;OPENSTACK&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cloudmesh/get/blob/master/openapi/google/index.html&#34;&gt;GOOGLE&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Contributors:&lt;/strong&gt; Jonathan Beckford, Andrew Goldfarb&lt;/p&gt;
&lt;h3 id=&#34;openapi-project-readme-generator&#34;&gt;Openapi project readme generator&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/tree/master/sphinx&#34;&gt;https://github.com/cloudmesh/cloudmesh-openapi/tree/master/sphinx&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contributors:&lt;/strong&gt; Jonathan Beckford, Professor&lt;/p&gt;
&lt;h3 id=&#34;chapters&#34;&gt;Chapters&lt;/h3&gt;
&lt;h5 id=&#34;kubernetes&#34;&gt;Kubernetes&lt;/h5&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cloudmesh-community/sp20-516-231/blob/master/chapter/k8s-kubernetes-scheduler.md&#34;&gt;https://github.com/cloudmesh-community/sp20-516-231/blob/master/chapter/k8s-kubernetes-scheduler.md&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contributors:&lt;/strong&gt;  Jonathan Beckford, Brian Kegerreis, Ashok Singam&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/cloudmesh-openapi/readme-scikitlearn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/cloudmesh-openapi/readme-scikitlearn/</guid>
      <description>
        
        
        &lt;h1 id=&#34;cloudmesh-openapi-scikit-learn&#34;&gt;Cloudmesh OpenAPI Scikit-learn&lt;/h1&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We use recommend Python 3.8.2 Python or newer.&lt;/li&gt;
&lt;li&gt;We recommend pip version 20.0.2 or newer&lt;/li&gt;
&lt;li&gt;We recommend that you use a venv (see developer install)&lt;/li&gt;
&lt;li&gt;MongoDB installed as regular program not as service&lt;/li&gt;
&lt;li&gt;Please run cim init command to start mongodb server&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We have not checked if it works on older versions.&lt;/p&gt;
&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;
&lt;p&gt;Make sure to follow the instruction for &lt;code&gt;cms openapi&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;When getting started using the &lt;code&gt;openapi&lt;/code&gt;, please first call &lt;code&gt;cms help openapi&lt;/code&gt; to see the available functions and options. For your
convenience we include the manual page later on in this document.&lt;/p&gt;
&lt;h2 id=&#34;scikit-learn-documentation&#34;&gt;Scikit-learn Documentation&lt;/h2&gt;
&lt;p&gt;Scikit-learn is a Machine learning library in Python.We can choose a
ML algorithm like LinearRegression and cloudmesh will be able to spin
up OPENAPI specification for the library we choose.  We can interact
with the Scikit-learn library using either CURL commands or through
GUI.&lt;/p&gt;
&lt;p&gt;This Version of Scikit-learn service accepts csv files in UTF-8 format
only.It is the user responsibility to make sure the files are in UTF-8
format.It is the user responsiblity to split the data in to train and
test datasets.  Split data functionality is not currently supported.&lt;/p&gt;
&lt;h3 id=&#34;setting-up-scikit-learn-service&#34;&gt;Setting up Scikit-learn service&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Please complete the basic installation of
&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi&#34;&gt;cloudmesh-openapi&lt;/a&gt;,
To make set up easy the same steps are even referenced at the
Developer Installation section in the document.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You can find Scikit-learn documentation in
&lt;a href=&#34;https://scikit-learn.org/dev/modules/classes.html&#34;&gt;Scikit-learn&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The following packages needs to be installed to access Scikit-learn&lt;/p&gt;
&lt;p&gt;.. code:: bash&lt;/p&gt;
&lt;p&gt;pip install pandas
pip install Scikit-learn&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Navigate to the &lt;code&gt;./cloudmesh-openapi&lt;/code&gt; directory on your machine&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Utilize the Scikit-learn generate command to create the python file
which will used to generate OpenAPI spec&lt;/p&gt;
&lt;p&gt;.. code:: bash&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi sklearnreadfile sklearn.linear_model.LinearRegression Linregpytest
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The sample generated file can be viewed at
&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/tree/master/tests/generator&#34;&gt;tests/generator&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Utilize the generate command to generate OpenAPI spec with upload functionality enabled&lt;/p&gt;
&lt;p&gt;.. code:: bash&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi generate --filename=./tests/generator/LinearRegression.py --all_functions --enable_upload
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Start the server after the yaml file is generated ot the same directory as the .py file&lt;/p&gt;
&lt;p&gt;.. code:: bash&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi server start ./tests/generator/LinearRegression.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Access the REST service using
&lt;a href=&#34;http://localhost:8080/cloudmesh/ui/&#34;&gt;http://localhost:8080/cloudmesh/ui/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run a curl command against the newly running server to upload the
testfiles.&lt;/p&gt;
&lt;p&gt;Place your test files in
&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/tree/master/tests/Scikitlearn-data&#34;&gt;Scikitlearn-data&lt;/a&gt;
We are testing with X_SAT.csv(SAT Scores of students),y_GPA(GPA of
students)&lt;/p&gt;
&lt;p&gt;.. code:: bash&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -X POST &amp;quot;http://localhost:8080/cloudmesh/upload&amp;quot; \
     -H &amp;quot;accept: text/plain&amp;quot; \
     -H &amp;quot;Content-Type: multipart/form-data&amp;quot; \
     -F &amp;quot;upload=@tests/Scikitlearn-data/X_SAT.csv;type=text/csv&amp;quot;


curl -X POST &amp;quot;http://localhost:8080/cloudmesh/upload&amp;quot; \
     -H &amp;quot;accept: text/plain&amp;quot; \
     -H &amp;quot;Content-Type: multipart/form-data&amp;quot; \
     -F &amp;quot;upload=@tests/Scikitlearn-data/y_GPA.csv;type=text/csv&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run a curl command against the newly running server to verify fit
method in Scikit-learn using the uploaded files&lt;/p&gt;
&lt;p&gt;.. code:: bash&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -X GET &amp;quot;http://localhost:8080/cloudmesh/LinearRegression_upload-enabled/fit?X=X_SAT&amp;amp;y=y_GPA&amp;quot; -H &amp;quot;accept: */*&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run a curl command against the newly running server to run the
Predict method.&lt;/p&gt;
&lt;p&gt;.. code:: bash&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -X GET &amp;quot;http://localhost:8080/cloudmesh/LinearRegression_upload-enabled/predict?X=X_SAT&amp;quot; -H &amp;quot;accept: text/plain&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run a curl command against the newly running server to run the Score method.&lt;/p&gt;
&lt;p&gt;.. code:: bash&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -X GET &amp;quot;http://localhost:8080/cloudmesh/LinearRegression_upload-enabled/score?X=X_SAT&amp;amp;y=y_GPA&amp;quot; -H &amp;quot;accept: text/plain&amp;quot;   
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stop the server&lt;/p&gt;
&lt;p&gt;.. code:: bash&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi server stop LinearRegression
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/cloudmesh-openapi/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/cloudmesh-openapi/readme/</guid>
      <description>
        
        
        &lt;h1 id=&#34;cloudmesh-openapi-service-generator&#34;&gt;Cloudmesh OpenAPI Service Generator&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The README.md page is outomatically generated, do not edit it.
To modify  change the content in
&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/blob/master/README-source.md&#34;&gt;https://github.com/cloudmesh/cloudmesh-openapi/blob/master/README-source.md&lt;/a&gt;
Curley brackets must use two in README-source.md&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/cloudmesh-openapi/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/cloudmesh-openapi.svg&#34; alt=&#34;image&#34;&gt;&lt;/a&gt;
&lt;a href=&#34;https://pypi.python.org/pypi/cloudmesh-openapi&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/cloudmesh-openapi.svg&#34; alt=&#34;Python&#34;&gt;&lt;/a&gt;
&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/blob/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-blue.svg&#34; alt=&#34;License&#34;&gt;&lt;/a&gt;
&lt;a href=&#34;https://pypi.python.org/pypi/cloudmesh-openapi&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/format/cloudmesh-openapi.svg&#34; alt=&#34;Format&#34;&gt;&lt;/a&gt;
&lt;a href=&#34;https://pypi.python.org/pypi/cloudmesh-openapi&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/status/cloudmesh-openapi.svg&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;
&lt;a href=&#34;https://travis-ci.com/cloudmesh/cloudmesh-openapi&#34;&gt;&lt;img src=&#34;https://travis-ci.com/cloudmesh/cloudmesh-openapi.svg?branch=master&#34; alt=&#34;Travis&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We recommend Python 3.8.2 Python or newer.&lt;/li&gt;
&lt;li&gt;We recommend pip version 20.0.2 or newer&lt;/li&gt;
&lt;li&gt;We recommend that you use a venv (see developer install)&lt;/li&gt;
&lt;li&gt;MongoDB installed as regular program not as service, which can
easily be done with cloudmesh on macOS, Linux, and Windows.&lt;/li&gt;
&lt;li&gt;Please run &lt;code&gt;cms gui quick&lt;/code&gt; to initialize the password for the mongodb
server&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Note: On windows you can use &lt;a href=&#34;https://gitforwindows.org/&#34;&gt;gitbash&lt;/a&gt;
so you can use bash and can use the same commands as on Linux or
macOS. Otherwise, please use the appropriate backslashes to access
the path.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;
&lt;p&gt;The installation is rather simple  and is documented next.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python -m venv ~/ENV3
source ~/ENV3/bin/activate 
mkdir cm
cd cm
pip install cloudmesh-installer
cloudmesh-installer get openapi 
cms help
cms gui quick
# fill out mongo variables
# make sure autinstall is True
cms config set cloudmesh.data.mongo.MONGO_AUTOINSTALL=True
cms admin mongo install --force
# Restart a new terminal to make sure mongod is in your path
cms init
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If you like to know more about the installation of cloudmesh, please
visit the &lt;a href=&#34;https://cloudmesh.github.io/cloudmesh-manual/installation/install.html&#34;&gt;Cloudmesh
Manual&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;command-overview&#34;&gt;Command Overview&lt;/h2&gt;
&lt;p&gt;When getting started using cloudmes &lt;code&gt;openapi&lt;/code&gt;, please first call to
get familiar with the options you have:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms help openapi
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We include the manual page later on in this document.&lt;/p&gt;
&lt;h2 id=&#34;quick-start&#34;&gt;Quick Start&lt;/h2&gt;
&lt;p&gt;Next we provide a very simple quickstart guide to steps to generate a
simple microservice that returns the CPU information of your computer.
We demonstrate how to generate, start, and stop the servive.&lt;/p&gt;
&lt;p&gt;Navigate to &lt;code&gt;~/cm/cloudmesh-openapi&lt;/code&gt; folder. In this folder you will
have a file called &lt;code&gt;cpu.py&lt;/code&gt; from which we will generate the server.&lt;/p&gt;
&lt;p&gt;First, generate an OpenAPI YAML file with the convenient command&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi generate get_processor_name \
    --filename=./tests/server-cpu/cpu.py
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This will create the file &lt;code&gt;cpu.yaml&lt;/code&gt; that contains the OpenAPI
specification. To start the service from this specification simply use
the command&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi server start ./tests/server-cpu/cpu.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now that the service is up and running, you can issue a request for
example via the commandline with&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -X GET &amp;quot;http://localhost:8080/cloudmesh/get_processor_name&amp;quot; \
     -H &amp;quot;accept: text/plain&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To view the automatically generated documentation, you can go to your
browser and open the link&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:8080/cloudmesh/ui&#34;&gt;http://localhost:8080/cloudmesh/ui&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;images/openapi-ui.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;You can also look at the status of the server with the command&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi server list
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;images/openapi-info.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Once yo no longer need the service, you can stop it with&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi server stop cpu
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;quickstart-to-creating-your-own-microservice&#34;&gt;Quickstart to Creating your own Microservice&lt;/h2&gt;
&lt;p&gt;Cloudmesh uses introspection to generate an OpenAPI compliant YAML
specification that will allow your Python code to run as a web
service. For this reason, any code you write must conform to a set of
guidelines.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The parameters and return values of any functions you write must use
python typing&lt;/li&gt;
&lt;li&gt;Your functions must include docstrings&lt;/li&gt;
&lt;li&gt;If a function uses or returns a class, that class must be defined as
a dataclass in the same file&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Next we demonstrate how to create your own microservice.
We provide two examples. One in which we return a float,
te other one in which the return value is wrapped in a
json object.&lt;/p&gt;
&lt;h3 id=&#34;returning-a-float&#34;&gt;Returning a Float&lt;/h3&gt;
&lt;p&gt;We define a function that adds tow values.  Note how x,
y, and the return value are all typed. In this case they are all
&lt;code&gt;float&lt;/code&gt;, but other types are supported. The description in the
docstring will be added to your YAML specification to help describe
what the function does.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;def add(x: float, y: float) -&amp;gt; float:
    &amp;quot;&amp;quot;&amp;quot;
    adding float and float.
    :param x: x value
    :type x: float
    :param y: y value
    :type y: float
    :return: result
    :return type: float
    &amp;quot;&amp;quot;&amp;quot;
    result = x + y

    return result
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To generate, start, retrieve a result, and stop the service you can use the
following command sequence:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi generate add --filename=./tests/add-float/add.py
cms openapi server start ./tests/add-float/add.yaml 
curl -X GET &amp;quot;http://localhost:8080/cloudmesh/add?x=1&amp;amp;y=2&amp;quot; -H  &amp;quot;accept: text/plain&amp;quot;
# This command returns
&amp;gt; 3.0
cms openapi server stop add
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;returning-a-json-object&#34;&gt;Returning a Json Object&lt;/h3&gt;
&lt;p&gt;Often we like to wrap the return value into a json string object, which can easily be
done by modifying the previous example as showcased next.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from flask import jsonify

def add(x: float, y: float) -&amp;gt; str:
    &amp;quot;&amp;quot;&amp;quot;
    adding float and float.
    :param x: x value
    :type x: float
    :param y: y value
    :type y: float
    :return: result
    :return type: float
    &amp;quot;&amp;quot;&amp;quot;
    result = {&amp;quot;result&amp;quot;: x + y}

    return jsonify(result)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To generate, start, retrieve a result, and stop the service you can use the
following command sequence:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi generate add --filename=./tests/add-json/add.py
cms openapi server start ./tests/add-json/add.yaml 
curl -X GET &amp;quot;http://localhost:8080/cloudmesh/add?x=1&amp;amp;y=2&amp;quot; -H  &amp;quot;accept: text/plain&amp;quot;
# This command returns
&amp;gt; {&amp;quot;result&amp;quot;:3.0}
cms openapi server stop add
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;As usual in both cases the web browser can be used to inspect the documentation as well as to test running the
example, by filling out the form.&lt;/p&gt;
&lt;h2 id=&#34;details-of-the-cms-openapi-command&#34;&gt;Details of the &lt;code&gt;cms openapi&lt;/code&gt; command&lt;/h2&gt;
&lt;p&gt;The gaol as stated earlier is to transform a simple python function as a service&lt;/p&gt;
&lt;h3 id=&#34;generating-openapi-specification&#34;&gt;Generating OpenAPI specification&lt;/h3&gt;
&lt;p&gt;Once you have a Python function you would like to deploy as a web
service, you can generate the OpenAPI specification. Navigate to your
.py file&amp;rsquo;s directory and generate the YAML. This will print
information to your console about the YAML file that was generated.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi generate [function_name] --filename=[filename.py]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If you would like to include more than one function in your web
service, like addition and subtraction, use the &lt;code&gt;--all_functions&lt;/code&gt;
flag. This will ignore functions whose names start with &amp;lsquo;_&amp;rsquo;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi generate --filename=[filename.py] --all_functions
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You can even write a class like Calculator that contains functions for
addition, subtraction, etc. You can generate a specification for an
entire class by using the &lt;code&gt;--import_class&lt;/code&gt; flag.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi generate [ClassName] --filename=[filename.py] --import_class
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;starting-a-server&#34;&gt;Starting a server&lt;/h3&gt;
&lt;p&gt;Once you have generated a specification, you can start the web service
on your localhost by providing the path to the YAML file. This will
print information to your console about the server&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi server start ./[filename.yaml]

  Starting: [server name]
  PID:      [PID]
  Spec:     ./[filename.py]
  URL:      http://localhost:8080/cloudmesh
  Cloudmesh UI:      http://localhost:8080/cloudmesh/ui
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;sending-requests-to-the-server&#34;&gt;Sending requests to the server&lt;/h3&gt;
&lt;p&gt;Now you have two options to interact with the web service. The first
is to navigate the the Cloudmesh UI and click on each endpoint to test
the functionality. The second is to use curl commands to submit
requests.&lt;/p&gt;
&lt;p&gt;We have already shown you earlier in our quickstart how to apply this to a
service such as our add service&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ curl -X GET &amp;quot;http://localhost:8080/cloudmesh/add?x=1.2&amp;amp;y=1.5&amp;quot; -H &amp;quot;accept: text/plain&amp;quot;
&amp;gt;   2.7
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;stopping-the-server&#34;&gt;Stopping the server&lt;/h3&gt;
&lt;p&gt;Now you can stop the server using the name of the server. If you
forgot the name, use &lt;code&gt;cms openapi server ps&lt;/code&gt; to get a list of server
processes.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ cms openapi server stop [server name]
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;basic-auth&#34;&gt;Basic Auth&lt;/h3&gt;
&lt;p&gt;To use basic http authentication with a user password for the
generated API, add the following flag at the end of a &lt;code&gt;cms openapi generate&lt;/code&gt; command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--basic_auth=&amp;lt;username&amp;gt;:&amp;lt;password&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We plan on supporting more security features in the future. Example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi generate get_processor_name \
    --filename=./tests/server-cpu/cpu.py \
    --basic_auth=admin:secret
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;manual-page&#34;&gt;Manual Page&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;openapi generate [FUNCTION] --filename=FILENAME
                         [--serverurl=SERVERURL]
                         [--yamlfile=YAML]
                         [--import_class]
                         [--all_functions]
                         [--enable_upload]
                         [--verbose]
                         [--basic_auth=CREDENTIALS]
openapi server start YAML [NAME]
              [--directory=DIRECTORY]
              [--port=PORT]
              [--server=SERVER]
              [--host=HOST]
              [--verbose]
              [--debug]
              [--fg]
              [--os]
openapi server stop NAME
openapi server list [NAME] [--output=OUTPUT]
openapi server ps [NAME] [--output=OUTPUT]
openapi register add NAME ENDPOINT
openapi register filename NAME
openapi register delete NAME
openapi register list [NAME] [--output=OUTPUT]
openapi TODO merge [SERVICES...] [--dir=DIR] [--verbose]
openapi TODO doc FILE --format=(txt|md)[--indent=INDENT]
openapi TODO doc [SERVICES...] [--dir=DIR]
openapi sklearn FUNCTION MODELTAG
openapi sklearnreadfile FUNCTION MODELTAG
openapi sklearn upload --filename=FILENAME

Arguments:
  FUNCTION  The name for the function or class
  MODELTAG  The arbirtary name choosen by the user to store the Sklearn trained model as Pickle object
  FILENAME  Path to python file containing the function or class
  SERVERURL OpenAPI server URL Default: https://localhost:8080/cloudmesh
  YAML      Path to yaml file that will contain OpenAPI spec. Default: FILENAME with .py replaced by .yaml
  DIR       The directory of the specifications
  FILE      The specification

Options:
  --import_class         FUNCTION is a required class name instead of an optional function name
  --all_functions        Generate OpenAPI spec for all functions in FILENAME
  --debug                Use the server in debug mode
  --verbose              Specifies to run in debug mode
                         [default: False]
  --port=PORT            The port for the server [default: 8080]
  --directory=DIRECTORY  The directory in which the server is run
  --server=SERVER        The server [default: flask]
  --output=OUTPUT        The outputformat, table, csv, yaml, json
                         [default: table]
  --srcdir=SRCDIR        The directory of the specifications
  --destdir=DESTDIR      The directory where the generated code
                         is placed

Description:
This command does some useful things.

openapi TODO doc FILE --format=(txt|md|rst) [--indent=INDENT]
    Sometimes it is useful to generate teh openaopi documentation
    in another format. We provide fucntionality to generate the
    documentation from the yaml file in a different formt.

openapi TODO doc --format=(txt|md|rst) [SERVICES...]
    Creates a short documentation from services registered in the
    registry.

openapi TODO merge [SERVICES...] [--dir=DIR] [--verbose]
    Merges tow service specifications into a single servoce
    TODO: do we have a prototype of this?


openapi sklearn sklearn.linear_model.LogisticRegression
    Generates the .py file for the Model given for the generator

openapi sklearnreadfile sklearn.linear_model.LogisticRegression
Generates the .py file for the Model given for the generator which supports reading files

openapi generate [FUNCTION] --filename=FILENAME
                             [--serverurl=SERVERURL]
                             [--yamlfile=YAML]
                             [--import_class]
                             [--all_functions]
                             [--enable_upload]
                             [--verbose]
                             [--basic_auth=CREDENTIALS]
    Generates an OpenAPI specification for FUNCTION in FILENAME and
    writes the result to YAML. Use --import_class to import a class
    with its associated class methods, or use --all_functions to 
    import all functions in FILENAME. These options ignore functions
    whose names start with &#39;_&#39;. Use --enable_upload to add file
    upload functionality to a copy of your python file and the
    resulting yaml file.

    For optional basic authorization, we support (temporarily) a single user
    credential. CREDENTIALS should be formatted as follows:

    user:password

    Example: --basic_auth=admin:secret

openapi server start YAML [NAME]
                  [--directory=DIRECTORY]
                  [--port=PORT]
                  [--server=SERVER]
                  [--host=HOST]
                  [--verbose]
                  [--debug]
                  [--fg]
                  [--os]
    starts an openapi web service using YAML as a specification
    TODO: directory is hard coded as None, and in server.py it
      defaults to the directory where the yaml file lives. Can
      we just remove this argument?

openapi server stop NAME
    stops the openapi service with the given name
    TODO: where does this command has to be started from

openapi server list [NAME] [--output=OUTPUT]
    Provides a list of all OpenAPI services in the registry

openapi server ps [NAME] [--output=OUTPUT]
    list the running openapi service

openapi register add NAME ENDPOINT
    Openapi comes with a service registry in which we can register
    openapi services.

openapi register filename NAME
    In case you have a yaml file the openapi service can also be
    registerd from a yaml file

openapi register delete NAME
    Deletes the names service from the registry

openapi register list [NAME] [--output=OUTPUT]
    Provides a list of all registerd OpenAPI services
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;basic-examples&#34;&gt;Basic Examples&lt;/h2&gt;
&lt;p&gt;Please follow &lt;a href=&#34;tests/README.md&#34;&gt;Pytest Information&lt;/a&gt; document for
pytests related information&lt;/p&gt;
&lt;p&gt;We have included a significant number of tests that aso serve as examples&lt;/p&gt;
&lt;h3 id=&#34;example-one-function-in-a-python-file&#34;&gt;Example: One function in a python file&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Please check &lt;a href=&#34;tests/server-cpu/cpu.py&#34;&gt;Python file&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run below command to generate yaml file and start server&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi generate get_processor_name --filename=./tests/server-cpu/cpu.py
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;example-multiple-functions-in-python-file&#34;&gt;Example: Multiple functions in python file&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Please check &lt;a href=&#34;tests/generator-calculator/calculator.py&#34;&gt;Python file&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run below command to generate yaml file and start server&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi generate --filename=./tests/generator-calculator/calculator.py --all_functions
cms openapi generate server start ./tests/generator-calculator/calculator.py
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;example-functions-in-python-class-file&#34;&gt;Example: Function(s) in python class file&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Please check &lt;a href=&#34;tests/generator-testclass/calculator.py&#34;&gt;Python file&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run below command to generate yaml file and start server&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi generate Calculator \
    --filename=./tests/generator-testclass/calculator.py \
    --import_class&amp;quot;
cms openapi server start ./tests/generator-testclass/calculator.yaml
curl -X GET &amp;quot;http://localhost:8080/cloudmesh/Calculator/multiplyint?x=1&amp;amp;y=5&amp;quot;
cms openapi server stop Calculator
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;example-uploading-data&#34;&gt;Example: Uploading data&lt;/h3&gt;
&lt;p&gt;Code to handle uploads is located in
&lt;code&gt;cloudmesh-openapi/tests/generator-upload&lt;/code&gt;. The code snippet in
uploadexample.py and the specification in uploadexample.yaml can be
added to existing projects by adding the &lt;code&gt;--enable_upload&lt;/code&gt; flag to the
&lt;code&gt;cms openapi generate&lt;/code&gt; command. The web service will be able to
retrieve the uploaded file from &lt;code&gt;~/.cloudmesh/upload-file/&lt;/code&gt;.&lt;/p&gt;
&lt;h4 id=&#34;upload-example&#34;&gt;Upload example&lt;/h4&gt;
&lt;p&gt;This example shows how to upload a CSV file and how the web service
can retrieve it.&lt;/p&gt;
&lt;p&gt;First, generate the OpenAPI specification and start the server&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi generate print_csv2np \
    --filename=./tests/generator-upload/csv_reader.py \
    --enable_upload
cms openapi server start ./tests/generator-upload/csv_reader.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Next, navigate to localhost:8080/cloudmesh/ui. Click to open
the /upload endpoint, then click &amp;lsquo;Try it out.&amp;rsquo; Click to choose a file
to upload, then upload &lt;code&gt;tests/generator-upload/np_test.csv&lt;/code&gt;. Click
&amp;lsquo;Execute&amp;rsquo; to complete the upload.&lt;/p&gt;
&lt;p&gt;The uploaded file will be located at
&lt;code&gt;~/.cloudmesh/upload-file/[filename]&lt;/code&gt;. The file
&lt;code&gt;tests/generator-upload/csv_reader.py&lt;/code&gt; contains some example code to
retrieve the array in the uploaded file. To see this in action, click
to open the /print_csv2np endpoint, then click &amp;lsquo;Try it out.&amp;rsquo; Enter
&amp;ldquo;np_test.csv&amp;rdquo; in the field that prompts for a filename, and then click
Execute to view the numpy array defined by the CSV file.&lt;/p&gt;
&lt;h3 id=&#34;example-pipeline-anova-svm-example&#34;&gt;Example: Pipeline Anova SVM Example&lt;/h3&gt;
&lt;p&gt;This example is based on the sklearn example
&lt;a href=&#34;https://scikit-learn.org/stable/auto_examples/feature_selection/plot_feature_selection_pipeline.html#sphx-glr-auto-examples-feature-selection-plot-feature-selection-pipeline-py&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In this example, we will upload a data set to the server, tell the
server to train the model, and utilize said model for predictions.&lt;/p&gt;
&lt;p&gt;Firstly, ensure we are in the correct directory.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ pwd
~/cm/cloudmesh-openapi
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Let us generate the yaml file from our python file to generate the proper specs for our service.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ cms openapi generate PipelineAnovaSVM \
      --filename=./tests/Scikitlearn-experimental/sklearn_svm.py \
      --import_class --enable_upload
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now let us start the server&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ cms openapi server start ./tests/Scikitlearn-experimental/sklearn_svm.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The server should now be active. Navigate to
&lt;code&gt;http://localhost:8080/cloudmesh/ui&lt;/code&gt;. We now have a nice user inteface
to interact with our newly generated API. Let us upload the data
set. We are going to use the iris data set in this example. We have
provided it for you to use. Simply navigate to the &lt;code&gt;/upload&lt;/code&gt; endpoint
by clicking on it, then click &lt;code&gt;Try it out&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We can now upload the file. Click on &lt;code&gt;Choose File&lt;/code&gt; and upload the data
set located at &lt;code&gt;~./tests/Scikitlearn-experimental/iris.data&lt;/code&gt;.  Simply
hit &lt;code&gt;Execute&lt;/code&gt; after the file is uploaded. We should then get a return
code of 200 (telling us that everything went ok).&lt;/p&gt;
&lt;p&gt;The server now has our dataset. Let us now navigate to the &lt;code&gt;/train&lt;/code&gt;
endpoint by, again, clicking on it. Similarly, click &lt;code&gt;Try it out&lt;/code&gt;.
The parameter being asked for is the filename. The filename we are
interested in is &lt;code&gt;iris.data&lt;/code&gt;. Then click &lt;code&gt;execute&lt;/code&gt;.  We should get
another 200 return code with a Classification Report in the Response
Body.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-CLASSIFICATION_REPORT:&#34; data-lang=&#34;CLASSIFICATION_REPORT:&#34;&gt;
           0       1.00      1.00      1.00         8
           1       0.85      1.00      0.92        11
           2       1.00      0.89      0.94        19

    accuracy                           0.95        38
   macro avg       0.95      0.96      0.95        38
weighted avg       0.96      0.95      0.95        38
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Our model is now trained and stored on the server. Let us make a
prediction now. As we have done, navigate to the &lt;code&gt;/make_prediction&lt;/code&gt;
endpoint.  The information we need to provide is the name of the model
we have trained as well as some test data. The name of the model will
be the same as the name of the data-file (ie. iris). So type in &lt;code&gt;iris&lt;/code&gt;
into the &lt;code&gt;model_name&lt;/code&gt; field. Finally for params, let us use the
example &lt;code&gt;5.1, 3.5, 1.4, 0.2&lt;/code&gt; as the model expects 4 values
(attributes). After clicking execute, we should received a response
with the classification the model has made given the parameters.&lt;/p&gt;
&lt;p&gt;The response received should be as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;quot;Classification: [&#39;Iris-setosa&#39;]&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can make as many predictions as we like. When finished, we can shut down the server.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ cms openapi server stop sklearn_svm
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;example-to-run-ai-services-in-the-cloud&#34;&gt;Example to Run AI Services in the Cloud&lt;/h2&gt;
&lt;h3 id=&#34;google&#34;&gt;Google&lt;/h3&gt;
&lt;p&gt;After you create your google cloud account, it is recommended to
download and install Google&amp;rsquo;s &lt;a href=&#34;https://cloud.google.com/sdk/docs/quickstarts&#34;&gt;Cloud
SDK&lt;/a&gt;.  This will
enable CLI. Make sure you enable all the required services.&lt;/p&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;gcloud services enable servicemanagement.googleapis.com
gcloud services enable endpoints.googleapis.com
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;and any other services you might be using for your specific Cloud API
function.&lt;/p&gt;
&lt;p&gt;To begin using the tests for any of the Google Cloud Platform AI
services you must first set up a Google account (set up a free tier
account): &lt;a href=&#34;https://cloud.google.com/billing/docs/how-to/manage-billing-account&#34;&gt;Google Account
Setup&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;After you create your google cloud account, it is recommended to
download and install Google&amp;rsquo;s &lt;a href=&#34;https://cloud.google.com/sdk/docs/quickstarts&#34;&gt;Cloud
SDK&lt;/a&gt;.  This will
enable CLI. Make sure you enable all the required services.&lt;/p&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;gcloud services enable servicemanagement.googleapis.com
gcloud services enable servicecontrol.googleapis.com
gcloud services enable endpoints.googleapis.com
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;and any other services you might be using for your specific Cloud API
function.&lt;/p&gt;
&lt;p&gt;It is also required to install the cloudmesh-cloud package, if not
already installed:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cloudmesh-installer get cloud
cloudmesh-installer install cloud
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This will allow you automatically fill out the cloudmesh yaml file
with your credentials once you generate the servcie account JSON file.&lt;/p&gt;
&lt;p&gt;After you have verified your account is created you must then give your account access to the proper APIs and create a
project in the Google Cloud Platform(GCP) console.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Go to the &lt;a href=&#34;console.cloud.google.com/projectselector2/home/&#34;&gt;project
selector&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Follow directions from Google to create a project linked to your
account&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;quickstart-google-python-api&#34;&gt;Quickstart Google Python API&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;For quickstart in using Google API for Python visit &lt;a href=&#34;https://developers.google.com/docs/api/quickstart/python&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;setting-up-your-google-account&#34;&gt;Setting up your Google account&lt;/h3&gt;
&lt;p&gt;Before you generate the service account JSON file for your account you
will want to enable a number of services in the GCP console.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Google Compute&lt;/li&gt;
&lt;li&gt;Billing&lt;/li&gt;
&lt;li&gt;Cloud Natural Language API&lt;/li&gt;
&lt;li&gt;Translate API&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;To do this you will need to click the menu icon in the Dashboard
navigation bar. Ensure you are in the correct porject.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Once that menu is open hover over the &amp;ldquo;APIs and Services&amp;rdquo; menu item
and click on &amp;ldquo;Dashboard&amp;rdquo; in the submenu.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;At the dashboard click on the &amp;ldquo;+ Enable APIs and Services&amp;rdquo; button
at the top of the dashboard&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Search for &lt;strong&gt;cloud natural language&lt;/strong&gt; to find the API in the search
results and click the result&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Once the page opens click &amp;ldquo;Enable&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Do the same for the &lt;strong&gt;translate&lt;/strong&gt; API to enable that as well&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Do the same for the &lt;strong&gt;compute engine API&lt;/strong&gt; to enable that as well&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You must now properly set up the account roles to ensure you will have
access to the API. Follow the directions from Google to &lt;a href=&#34;https://cloud.google.com/natural-language/docs/setup#auth&#34;&gt;set up proper
authentication&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Make you account an owner for each of the APIs in the IAM tool as
directed in the authentication steps for the natural language API.
This makes your service account have proper access to the required
APIs and once the private key is downloaded those will be stored
there.&lt;/p&gt;
&lt;p&gt;It is VERY important that you create a service account and download
the private key as described in the directions from Google.  If you do
not the cms google commands will not work properly.&lt;/p&gt;
&lt;p&gt;Once you have properly set up your permissions please make sure you
download your JSON private key for the service account that has
permissions set up for the required API services. These steps to
download are found
&lt;a href=&#34;https://cloud.google.com/natural-language/docs/setup#sa-create&#34;&gt;here&lt;/a&gt;.
Please take note of where you store the downloaded JSON and copy the
path string to a easily accessible location.&lt;/p&gt;
&lt;p&gt;The client libraries for each API are included in teh requirements.txt file for the openapi proejct and should be isntalled when the
package is installed. If not, follow directions outlined by google install each package:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;google-cloud-translate
google-cloud-language
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To pass the information from your service account private key file ot
the cloudmesh yaml file run the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms register update --kind=google --service=compute --filename=GOOGLE_JSON_FILE
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;running-the-google-natural-language-and-translate-rest-services&#34;&gt;Running the Google Natural Language and Translate REST Services&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Navigate to the &lt;code&gt;~/.cloudmesh&lt;/code&gt; repo and create a cache directory
for your text examples you would like to analyze.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mkdir text-cache
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add any plain text files your would like to analyze to this
directory with a name that has no special characters or spaces.
You can copy the files at this location,
&lt;code&gt;./cloudmesh-openapi/tests/textanaysis-example-text/reviews/&lt;/code&gt; into
the text-cache if you want to use provided examples.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Navigate to the &lt;code&gt;./cloudmesh-openapi&lt;/code&gt; directory on your machine&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Utilize the generate command to create the OpenAPI spec&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi generate TextAnalysis --filename=./tests/generator-natural-lang/natural-lang-analysis.py --all_functions
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Start the server after the yaml file is generated ot the same
directory as the .py file&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapie start server ./tests/generator-natural-lang/natural-lang-analysis.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run a curl command against the newly running server to verify it
returns a result as expected.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Sample text file name is only meant to be the name of the file
not the full path.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -X GET &amp;quot;http://127.0.0.1:8080/cloudmesh/analyze?filename=SAMPLE_TEXT_FILENAME&amp;amp;cloud=google&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;This is currently only ready to translate a single word through
the API.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -X GET &amp;quot;http://127.0.0.1:8080/cloudmesh/translate_text?cloud=google&amp;amp;text=WORD_TO_TRANSLATE&amp;amp;lang=LANG_CODE&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stop the server&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi server stop natural-lang-analysis
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;example-using-aws&#34;&gt;Example using AWS&lt;/h3&gt;
&lt;p&gt;Sign up for AWS&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Go to &lt;a href=&#34;https://portal.aws.amazon.com/billing/signup&#34;&gt;https://portal.aws.amazon.com/billing/signup&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Follow online instructions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Create an IAM User&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For instructions, see
&lt;a href=&#34;https://docs.aws.amazon.com/IAM/latest/UserGuide/getting-started_create-admin-group.html&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Set up AWS CLI and AWS SDKs&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To download and instructions to install AWS CLI, see
&lt;a href=&#34;https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Install Boto 3&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install boto3
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;For quickstart, vist &lt;a href=&#34;https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As long as you enable all the services you need for using AWS AI APIs you should be able to write your functions for OpenAPI&lt;/p&gt;
&lt;h3 id=&#34;example-using-azure&#34;&gt;Example using Azure&lt;/h3&gt;
&lt;h4 id=&#34;setting-up-azure-sentiment-analysis-and-translation-services&#34;&gt;Setting up Azure Sentiment Analysis and Translation Services&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Create an Azure subscription. If you do not have one, create a
&lt;a href=&#34;https://azure.microsoft.com/try/cognitive-services/&#34;&gt;free account&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create a &lt;a href=&#34;https://portal.azure.com/#create/Microsoft.CognitiveServicesTextAnalytics&#34;&gt;Text Analysis resource&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This link will require you to be logged in to the Azure portal&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create a &lt;a href=&#34;https://docs.microsoft.com/en-us/azure/cognitive-services/cognitive-services-apis-create-account?tabs=multiservice%2Cwindows&#34;&gt;Translation Resource&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The microsoft packages are included in the openapi package
requirements file so they should be installed. If they are not,
install the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install msrest 
pip install azure-ai-textanalytics
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Navigate to the &lt;code&gt;~/.cloudmesh&lt;/code&gt; repo and create a cache directory for your text examples you would like to analyze.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mkdir text-cache
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add any plain text files your would like to analyze to this
directory with a name that has no special characters or spaces.
You can copy the files at this location,
&lt;code&gt;./cloudmesh-openapi/tests/textanaysis-example-text/reviews/&lt;/code&gt; into
the text-cache if you want to use provided examples.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Navigate to the &lt;code&gt;./cloudmesh-openapi&lt;/code&gt; directory on your machine&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Utilize the generate command to create the OpenAPI spec&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi generate TextAnalysis --filename=./tests/generator-natural-lang/natural-lang-analysis.py --all_functions
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Start the server after the yaml file is generated ot the same
directory as the .py file&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapie start server ./tests/generator-natural-lang/natural-lang-analysis.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run a curl command against the newly running server to verify it
returns a result as expected.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Sample text file name is only meant to be the name of the file not the full path.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -X GET &amp;quot;http://127.0.0.1:8080/cloudmesh/analyze?filename=&amp;lt;&amp;lt;sample text file name&amp;gt;&amp;gt;&amp;amp;cloud=azure&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;This is currently only ready to translate a single word through the API.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Available language tags are described in the &lt;a href=&#34;https://docs.microsoft.com/en-us/azure/cognitive-services/translator/reference/v3-0-languages&#34;&gt;Azure docs&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -X GET &amp;quot;http://127.0.0.1:8080/cloudmesh/translate_text?cloud=azure&amp;amp;text=&amp;lt;&amp;lt;word to translate&amp;gt;&amp;gt;&amp;amp;lang=&amp;lt;&amp;lt;lang code&amp;gt;&amp;gt;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stop the server&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi server stop natural-lang-analysis
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The natural langauge analysis API can be improved by allowing for full
phrase translation via the API. If you contribute to this API there is
room for improvement to add custom translation models as well if
preferred to pre-trained APIs.&lt;/p&gt;
&lt;h4 id=&#34;setting-up-azure-computervision-ai-services&#34;&gt;Setting up Azure ComputerVision AI services&lt;/h4&gt;
&lt;h5 id=&#34;prerequisite&#34;&gt;Prerequisite&lt;/h5&gt;
&lt;p&gt;Using the Azure Computer Vision AI service, you can describe, analyze
and/ or get tags for a locally stored image or you can read the text
from an image or hand-written file.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Azure subscription. If you do not have one, create a &lt;a href=&#34;https://azure.microsoft.com/try/cognitive-services/&#34;&gt;free
account&lt;/a&gt; before
you continue further.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create a Computer Vision resource and get the
&lt;code&gt;COMPUTER_VISION_SUBSCRIPTION_KEY&lt;/code&gt; and
&lt;code&gt;COMPUTER_VISION_ENDPOINT&lt;/code&gt;. Follow
&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/cognitive-services/cognitive-services-apis-create-account?tabs=singleservice%2Cunix&#34;&gt;instructions&lt;/a&gt;
to get the same.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install following Python packages in your virtual environment:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install requests
pip install Pillow
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install Computer Vision client library&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install --upgrade azure-cognitiveservices-vision-computervision
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;steps-to-implement-and-use-azure-ai-image-and-text-rest-services&#34;&gt;Steps to implement and use Azure AI image and text &lt;em&gt;REST-services&lt;/em&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Go to &lt;code&gt;./cloudmesh-openapi&lt;/code&gt; directory&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run following command to generate the YAML files&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi generate AzureAiImage --filename=./tests/generator-azureai/azure-ai-image-function.py --all_functions --enable_upload
cms openapi generate AzureAiText --filename=./tests/generator-azureai/azure-ai-text-function.py --all_functions --enable_upload
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Verify the &lt;em&gt;YAML&lt;/em&gt; files created in &lt;code&gt;./tests/generator-azureai&lt;/code&gt; directory&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;azure-ai-image-function.yaml
azure-ai-text-function.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Start the REST service by running following command in &lt;code&gt;./cloudmesh-openapi&lt;/code&gt; directory&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi server start ./tests/generator-azureai/azure-ai-image-function.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The default port used for starting the service is 8080. In case you
want to start more than one REST service, use a different port in
following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi server start ./tests/generator-azureai/azure-ai-text-function.yaml --port=&amp;lt;**Use a different port than 8080**&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Access the REST service using &lt;a href=&#34;http://localhost:8080/cloudmesh/ui/&#34;&gt;http://localhost:8080/cloudmesh/ui/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;After you have started the azure-ai-image-function or azure-ai-text-function on default port 8080, run following command to upload the image or text_image file&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -X POST &amp;quot;http://localhost:8080/cloudmesh/upload&amp;quot; -H  &amp;quot;accept: text/plain&amp;quot; -H  &amp;quot;Content-Type: multipart/form-data&amp;quot; -F &amp;quot;upload=@tests/generator-azureai/&amp;lt;image_name_with_extension&amp;gt;;type=image/jpeg&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Keep your test image files at &lt;code&gt;./tests/generator-azureai/&lt;/code&gt; directory&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;With &lt;em&gt;azure-ai-text-function&lt;/em&gt; started on port=8080, in order to test the azure ai function for text detection in an image, run following command&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -X GET &amp;quot;http://localhost:8080/cloudmesh/azure-ai-text-function_upload-enabled/get_text_results?image_name=&amp;lt;image_name_with_extension_uploaded_earlier&amp;gt;&amp;quot; -H &amp;quot;accept: text/plain&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;With &lt;em&gt;azure-ai-image-function&lt;/em&gt; started on port=8080, in order to
test the azure ai function for describing an image, run following
command&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -X GET &amp;quot;http://localhost:8080/cloudmesh/azure-ai-image-function_upload-enabled/get_image_desc?image_name=&amp;lt;image_name_with_extension_uploaded_earlier&amp;gt;&amp;quot; -H &amp;quot;accept: text/plain&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;With &lt;em&gt;azure-ai-image-function&lt;/em&gt; started on port=8080, in order to
test the azure ai function for analyzing an image, run following
command&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -X GET &amp;quot;http://localhost:8080/cloudmesh/azure-ai-image-function_upload-enabled/get_image_analysis?image_name=&amp;lt;image_name_with_extension_uploaded_earlier&amp;gt;&amp;quot; -H &amp;quot;accept: text/plain&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;With &lt;em&gt;azure-ai-image-function&lt;/em&gt; started on port=8080, in order to
test the azure ai function for identifying tags in an image, run
following command&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -X GET &amp;quot;http://localhost:8080/cloudmesh/azure-ai-image-function_upload-enabled/get_image_tags?image_name=&amp;lt;image_name_with_extension_uploaded_earlier&amp;gt;&amp;quot; -H &amp;quot;accept: text/plain&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Check the running REST services using following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi server ps
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stop the REST service using following command(s):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapi server stop azure-ai-image-function
cms openapi server stop azure-ai-text-function
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;list-of-tests&#34;&gt;List of Tests&lt;/h2&gt;
&lt;p&gt;The following table lists the different test we have, we provide additional
information for the tests in the test directory in a README file. Summaries
are provided below the table&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Test&lt;/th&gt;
&lt;th&gt;Short Description&lt;/th&gt;
&lt;th&gt;Link&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Generator-calculator&lt;/td&gt;
&lt;td&gt;Test to check if calculator api is generated correctly. This is to test multiple function in one python file&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/blob/master/tests/generator-calculator/test_01_generator.py&#34;&gt;test_01_generator.py&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Generator-testclass&lt;/td&gt;
&lt;td&gt;Test to check if calculator api is generated correctly. This is to test multiple function in one python class file&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/blob/master/tests/generator-testclass/test_02_generator.py&#34;&gt;test_02_generator.py&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Server-cpu&lt;/td&gt;
&lt;td&gt;Test to check if cpu api is generated correctly. This is to test single function in one python file and function name is different than file name&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/blob/master/tests/server-cpu/test_03_generator.py&#34;&gt;test_03_generator.py&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Server-cms&lt;/td&gt;
&lt;td&gt;Test to check if cms api is generated correctly. This is to test multiple function in one python file.&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/blob/master/tests/server-cms/test_04_generator.py&#34;&gt;test_04_generator.py&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Registry&lt;/td&gt;
&lt;td&gt;test_001_registry.py - Runs tests for registry. Description is in tests/README.md&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/blob/master/tests/README.md&#34;&gt;Link&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Image-Analysis&lt;/td&gt;
&lt;td&gt;image_test.py - Runs benchmark for text detection for Google Vision API and AWS Rekognition. Description in image-analysis/README.md&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/blob/master/tests/image-analysis/README.md&#34;&gt;image&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;For more information about test cases ,please check &lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-openapi/blob/master/tests/README.md&#34;&gt;tests info&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;tests/test_001_registry.py&#34;&gt;test_001_registry&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;tests/test_003_server_manage_cpu.py&#34;&gt;test_003_server_manage_cpu&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;tests/test_010_generator.py&#34;&gt;test_010_generator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;tests/test_011_generator_cpu.py&#34;&gt;test_011_generator_cpu&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;tests/test_012_generator_calculator.py&#34;&gt;test_012_generator_calculator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;tests/test_015_generator_azureai.py&#34;&gt;test_015_generator_azureai&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;tests/test_020_server_manage.py&#34;&gt;test_020_server_manage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;tests/test_server_cms_cpu.py&#34;&gt;test_server_cms_cpu&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that there a many more tests that you can explore.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/cloudmesh-openapi/tests/add-float/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/cloudmesh-openapi/tests/add-float/readme/</guid>
      <description>
        
        
        &lt;h1 id=&#34;readme&#34;&gt;README&lt;/h1&gt;
&lt;p&gt;please see the README in the root dir of this repository&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/cloudmesh-openapi/tests/add-json/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/cloudmesh-openapi/tests/add-json/readme/</guid>
      <description>
        
        
        &lt;h1 id=&#34;readme&#34;&gt;README&lt;/h1&gt;
&lt;p&gt;please see the README in the root dir of this repository&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/cloudmesh-openapi/tests/generator-natural-lang/googlecloudvmset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/cloudmesh-openapi/tests/generator-natural-lang/googlecloudvmset/</guid>
      <description>
        
        
        &lt;h1 id=&#34;steps&#34;&gt;Steps:&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Setup a google account with Google Cloud&lt;/li&gt;
&lt;li&gt;Create a project&lt;/li&gt;
&lt;li&gt;Set permission for create on compute engine in the project&lt;/li&gt;
&lt;li&gt;create a service account file and link to json in cloudmesh yaml file
&lt;a href=&#34;https://cloud.google.com/docs/authentication/production?hl=en_US&#34;&gt;https://cloud.google.com/docs/authentication/production?hl=en_US&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Create a storage location using google storage
&lt;a href=&#34;https://cloud.google.com/storage/docs/creating-buckets#storage-create-bucket-code_samples&#34;&gt;https://cloud.google.com/storage/docs/creating-buckets#storage-create-bucket-code_samples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Install the google cloud sdk
&lt;a href=&#34;https://cloud.google.com/compute/docs/tutorials/python-guide&#34;&gt;https://cloud.google.com/compute/docs/tutorials/python-guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Install the google cloud api client library
&lt;a href=&#34;https://cloud.google.com/apis/docs/client-libraries-explained&#34;&gt;https://cloud.google.com/apis/docs/client-libraries-explained&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Write a startup script for your vm&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;azure&#34;&gt;Azure&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal&#34;&gt;https://docs.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal&lt;/a&gt;
Credentials:
app-name: vm-creation-example
auth url:https://andyvmcreateexample.com/auth&lt;/p&gt;
&lt;p&gt;app (client) ID: 8db85342-7efd-433c-aeba-d175ae4d4404
directory (tenant) id: 398e5475-e850-4239-ba0d-62ddc3e644ff
object ID: 38224a7e-79e0-4642-b765-2bf731d296ad
client-secret: w[f7o=[dKKeSn?VxF3iNoZDW3ctMmd3G
subscription id:4513afc9-4159-49d0-aa1d-0a2a0ab9933c&lt;/p&gt;
&lt;p&gt;when creating a vm in the portal the network interface is set up for you
but if you do it programmatically you have to set it up.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/cloudmesh-openapi/tests/gregor/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/cloudmesh-openapi/tests/gregor/readme/</guid>
      <description>
        
        
        &lt;h1 id=&#34;test-it-yourself&#34;&gt;Test it yourself&lt;/h1&gt;
&lt;p&gt;cd to &lt;code&gt;cloudmesh-openapi&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Start the service&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapii server start ./tests/server-cpu.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Stop the service&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapii3 server stop cpu
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;urls&lt;/p&gt;
&lt;p&gt;cloudmesh/ui&lt;/p&gt;
&lt;p&gt;cloudmesh/cpu&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/cloudmesh-openapi/tests/image-analysis/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/cloudmesh-openapi/tests/image-analysis/readme/</guid>
      <description>
        
        
        &lt;h1 id=&#34;test-it-yourself&#34;&gt;Test it yourself&lt;/h1&gt;
&lt;h2 id=&#34;in-cloudmesh-openapi&#34;&gt;In cloudmesh-openapi&lt;/h2&gt;
&lt;p&gt;Start server&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cms openapi server start ./tests/image-analysis/image.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Get Response Google Vision&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;curl -sL http://127.0.0.1:8080/cloudmesh/image/detect_text_google
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Get Response AWS Rekognition&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;curl -sL http://127.0.0.1:8080/cloudmesh/image/detect_text_aws
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Stop server&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cms openapi server stop image
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;urls&lt;/p&gt;
&lt;p&gt;cloudmesh/image/detect_text_google&lt;/p&gt;
&lt;p&gt;cloudmesh/image/detect_text_aws&lt;/p&gt;
&lt;h2 id=&#34;image_testpy&#34;&gt;image_test.py&lt;/h2&gt;
&lt;p&gt;How to run test&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;pytest -v --capture&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;no tests/image-analysis/image_test.py 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;image_test.py has 7 tests&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Uses generate command to generate new yaml file&lt;/li&gt;
&lt;li&gt;Check yaml syntax&lt;/li&gt;
&lt;li&gt;Starts server&lt;/li&gt;
&lt;li&gt;Does a curl call for google vision api response&lt;/li&gt;
&lt;li&gt;Does a curl call for aws rekognition api response&lt;/li&gt;
&lt;li&gt;Stops the server&lt;/li&gt;
&lt;li&gt;Prints benchmark&lt;/li&gt;
&lt;/ol&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/cloudmesh-openapi/tests/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/cloudmesh-openapi/tests/readme/</guid>
      <description>
        
        
        &lt;h1 id=&#34;how-to-write-and-run-test-case-for-openapi&#34;&gt;How to write and run test case for OpenAPI&lt;/h1&gt;
&lt;h2 id=&#34;this-document-will-explain-how-to-validate-if-openapi-is-generated-correctly-and-server-start-and-stop-working-correctly&#34;&gt;This document will explain how to validate if openapi is generated correctly and server start and stop working correctly&lt;/h2&gt;
&lt;h3 id=&#34;we-have-create-a-framework-class-which-has-below-basic-test-case-functions&#34;&gt;We have create a framework class which has below basic test case functions&lt;/h3&gt;
&lt;p&gt;Framework file is present under tests/lib named as generator_test.py&lt;/p&gt;
&lt;h4 id=&#34;below-test-cases-are-related-to-generator-api&#34;&gt;Below test cases are related to generator API&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Create a build folder and copy py file into it. Build sub folder will created where test py file present.&lt;/li&gt;
&lt;li&gt;It will call generator generate function to generate Yaml file inside build folder&lt;/li&gt;
&lt;li&gt;It will check if generated YMAL file syntax is correct or not.&lt;/li&gt;
&lt;li&gt;It will check if number of function generated in YMAL is same as py file.&lt;/li&gt;
&lt;li&gt;Delete the build folder.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;two-test-cases-are-related-to-server-api&#34;&gt;Two test cases are related to server API&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;It will start server&lt;/li&gt;
&lt;li&gt;It will stop server&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;how-to-create-test-case&#34;&gt;How to create test case&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;If you creating new Open API , then inside tests folder you have to commit your working py and yaml files.&lt;/li&gt;
&lt;li&gt;Create new function for test case where py and yaml located. Example (test_01_generator)&lt;/li&gt;
&lt;li&gt;We have already created test cases function file for generator-calculator name as test_01_generator.py. Please check this file.&lt;/li&gt;
&lt;li&gt;Copy the contains of test_01_generator.py and paste inside your test py file.&lt;/li&gt;
&lt;li&gt;Change startservercommand and filename variables value accordingly to your use case.&lt;/li&gt;
&lt;li&gt;Change some of parameters of constructor of GeneratorBaseTest class.&lt;/li&gt;
&lt;li&gt;if your py file has a class then.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt; &lt;span style=&#34;color:#000&#34;&gt;gen&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; GeneratorBaseTest&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;(&lt;/span&gt;filename,False,True&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol&gt;
&lt;li&gt;if your py file has functions then&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt; &lt;span style=&#34;color:#000&#34;&gt;gen&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; GeneratorBaseTest&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;(&lt;/span&gt;filename,True,False&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol&gt;
&lt;li&gt;First boolean flag in GeneratorBaseTest for &amp;ndash;all_functions and second flag is for &amp;ndash;import_class&lt;/li&gt;
&lt;li&gt;If you need to write more test cases based on your requirement, check order of test case and write accordingly.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;how-to-run-test-case&#34;&gt;How to run test case&lt;/h3&gt;
&lt;p&gt;Below command can use to run your case. Make sure your current directory is cloudmesh-openapi.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ how &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;do&lt;/span&gt; you call this you can add -x to stop pytest when first &lt;span style=&#34;color:#204a87&#34;&gt;test&lt;/span&gt; failed
pytest -v  --capture&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;no tests/generator-calculator/test_01_generator.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;run-test-case-with-csv-command-enabled&#34;&gt;Run test case with CSV command enabled&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ how &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;do&lt;/span&gt; you call this , you can add -x to stop pytest when first &lt;span style=&#34;color:#204a87&#34;&gt;test&lt;/span&gt; failed
pytest -v  --capture&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;no tests/generator-calculator/test_01_generator.py  &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;|&lt;/span&gt; fgrep &lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#39;# cvs&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;below-are-test-case-files&#34;&gt;Below are test case files&lt;/h2&gt;
&lt;p&gt;Generator-calculator and file name is test_01_generator.py&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pytest -v  --capture=no tests/generator-calculator/test_01_generator.py
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Generator-testclass and file name is test_02_generator.py&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pytest -v --capture=no tests/generator-testclass/test_02_generator
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Server-cpu and file name is test_03_generator.py&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pytest -v  --capture=no tests/server-cpu/test_03_generator
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Server-cms and file name is test_04_generator.py&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pytest -v  --capture=no tests/server-cms/test_04_generator
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Generator and file name is test_05_generator.py&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pytest -v --capture=no tests/generator/test_05_generator
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Azure AI Image Function is test_06_generator.py&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;pytest -v --capture&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;no tests/generator_azureai/test_06_generator
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Azure AI Text Function is test_07_generator.py&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;pytest -v --capture&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;no tests/generator_azureai/test_07_generator
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Natural Language Analysis Generator Tests are run from test_generator_natural_language.py&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;pytest -v --capture&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;no  ./tests/test_generator_natural_language.py::TestGenerator
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This test will generate an OpenAPI spec for the natural-lang-analysis.py file located in the generator-natural-lang
directory. If the above command is copied and pasted to run in the terminal it will do the following.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Generate a yaml file&lt;/li&gt;
&lt;li&gt;Verify the spec has all the functions that are available in the natural-lang-analysis.py file&lt;/li&gt;
&lt;li&gt;Start a server hosting the openAPI spec&lt;/li&gt;
&lt;li&gt;Run a call against the sentiment analysis and translation endpoint for each available cloud service (Google/Azure) and verify it was successful.&lt;/li&gt;
&lt;li&gt;Stop the service&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Results for Natural Language Tests&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Attribute&lt;/th&gt;
&lt;th&gt;Value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;cpu_count&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mem.active&lt;/td&gt;
&lt;td&gt;2.0 GiB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mem.available&lt;/td&gt;
&lt;td&gt;2.1 GiB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mem.free&lt;/td&gt;
&lt;td&gt;148.8 MiB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mem.inactive&lt;/td&gt;
&lt;td&gt;2.0 GiB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mem.percent&lt;/td&gt;
&lt;td&gt;73.2 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mem.total&lt;/td&gt;
&lt;td&gt;8.0 GiB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mem.used&lt;/td&gt;
&lt;td&gt;4.8 GiB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mem.wired&lt;/td&gt;
&lt;td&gt;2.8 GiB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;platform.version&lt;/td&gt;
&lt;td&gt;10.14.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;python&lt;/td&gt;
&lt;td&gt;3.8.1 (v3.8.1:1b293b6006, Dec 18 2019, 14:08:53)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;[Clang 6.0 (clang-600.0.57)]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;python.pip&lt;/td&gt;
&lt;td&gt;20.0.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;python.version&lt;/td&gt;
&lt;td&gt;3.8.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;sys.platform&lt;/td&gt;
&lt;td&gt;darwin&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;uname.machine&lt;/td&gt;
&lt;td&gt;x86_64&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;uname.node&lt;/td&gt;
&lt;td&gt;Andrews-MacBook-Pro.local&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;uname.processor&lt;/td&gt;
&lt;td&gt;i386&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;uname.release&lt;/td&gt;
&lt;td&gt;18.2.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;uname.system&lt;/td&gt;
&lt;td&gt;Darwin&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;uname.version&lt;/td&gt;
&lt;td&gt;Darwin Kernel Version 18.2.0: Fri Oct  5 19:41:49 PDT 2018; root:xnu-4903.221.2~2/RELEASE_X86_64&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;user&lt;/td&gt;
&lt;td&gt;andrewgoldfarb&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;Status&lt;/th&gt;
&lt;th&gt;Time&lt;/th&gt;
&lt;th&gt;Start&lt;/th&gt;
&lt;th&gt;tag&lt;/th&gt;
&lt;th&gt;Node&lt;/th&gt;
&lt;th&gt;User&lt;/th&gt;
&lt;th&gt;OS&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;generator_test/copy_py_file&lt;/td&gt;
&lt;td&gt;ok&lt;/td&gt;
&lt;td&gt;0.003&lt;/td&gt;
&lt;td&gt;2020-05-09 06:33:47&lt;/td&gt;
&lt;td&gt;openapi&lt;/td&gt;
&lt;td&gt;Andrews-MacBook-Pro.local&lt;/td&gt;
&lt;td&gt;andrewgoldfarb&lt;/td&gt;
&lt;td&gt;Darwin&lt;/td&gt;
&lt;td&gt;10.14.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;generator_test/generate&lt;/td&gt;
&lt;td&gt;ok&lt;/td&gt;
&lt;td&gt;2.601&lt;/td&gt;
&lt;td&gt;2020-05-09 06:33:47&lt;/td&gt;
&lt;td&gt;openapi&lt;/td&gt;
&lt;td&gt;Andrews-MacBook-Pro.local&lt;/td&gt;
&lt;td&gt;andrewgoldfarb&lt;/td&gt;
&lt;td&gt;Darwin&lt;/td&gt;
&lt;td&gt;10.14.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;generator_test/read_spec&lt;/td&gt;
&lt;td&gt;ok&lt;/td&gt;
&lt;td&gt;0.012&lt;/td&gt;
&lt;td&gt;2020-05-09 06:33:49&lt;/td&gt;
&lt;td&gt;openapi&lt;/td&gt;
&lt;td&gt;Andrews-MacBook-Pro.local&lt;/td&gt;
&lt;td&gt;andrewgoldfarb&lt;/td&gt;
&lt;td&gt;Darwin&lt;/td&gt;
&lt;td&gt;10.14.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;generator_test/start_service&lt;/td&gt;
&lt;td&gt;ok&lt;/td&gt;
&lt;td&gt;1.864&lt;/td&gt;
&lt;td&gt;2020-05-09 06:33:49&lt;/td&gt;
&lt;td&gt;openapi&lt;/td&gt;
&lt;td&gt;Andrews-MacBook-Pro.local&lt;/td&gt;
&lt;td&gt;andrewgoldfarb&lt;/td&gt;
&lt;td&gt;Darwin&lt;/td&gt;
&lt;td&gt;10.14.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;test_generator_natural_language/test_run_analyze_google&lt;/td&gt;
&lt;td&gt;ok&lt;/td&gt;
&lt;td&gt;0.67&lt;/td&gt;
&lt;td&gt;2020-05-09 06:33:51&lt;/td&gt;
&lt;td&gt;openapi&lt;/td&gt;
&lt;td&gt;Andrews-MacBook-Pro.local&lt;/td&gt;
&lt;td&gt;andrewgoldfarb&lt;/td&gt;
&lt;td&gt;Darwin&lt;/td&gt;
&lt;td&gt;10.14.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;test_generator_natural_language/test_run_analyze_azure&lt;/td&gt;
&lt;td&gt;ok&lt;/td&gt;
&lt;td&gt;0.58&lt;/td&gt;
&lt;td&gt;2020-05-09 06:33:52&lt;/td&gt;
&lt;td&gt;openapi&lt;/td&gt;
&lt;td&gt;Andrews-MacBook-Pro.local&lt;/td&gt;
&lt;td&gt;andrewgoldfarb&lt;/td&gt;
&lt;td&gt;Darwin&lt;/td&gt;
&lt;td&gt;10.14.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;generator_test/stop_server&lt;/td&gt;
&lt;td&gt;ok&lt;/td&gt;
&lt;td&gt;2.095&lt;/td&gt;
&lt;td&gt;2020-05-09 06:33:52&lt;/td&gt;
&lt;td&gt;openapi&lt;/td&gt;
&lt;td&gt;Andrews-MacBook-Pro.local&lt;/td&gt;
&lt;td&gt;andrewgoldfarb&lt;/td&gt;
&lt;td&gt;Darwin&lt;/td&gt;
&lt;td&gt;10.14.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;generator_test/delete_file&lt;/td&gt;
&lt;td&gt;ok&lt;/td&gt;
&lt;td&gt;0.002&lt;/td&gt;
&lt;td&gt;2020-05-09 06:33:54&lt;/td&gt;
&lt;td&gt;openapi&lt;/td&gt;
&lt;td&gt;Andrews-MacBook-Pro.local&lt;/td&gt;
&lt;td&gt;andrewgoldfarb&lt;/td&gt;
&lt;td&gt;Darwin&lt;/td&gt;
&lt;td&gt;10.14.1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;todo-describe-what-they-do&#34;&gt;TODO DESCRIBE WHAT THEY DO&lt;/h2&gt;
&lt;p&gt;cache-scikitlearn
deprecated
examples
generator
generator-azureai
generator-calculator
generator-printerclass
generator-testclass
generator-upload
image-analysis
lib
Scikit-learntestfiles
Scikitlearn_tests
server-cms
server-cms-simple
server-cpu
server-sample
server-sampleFunction
test_mlperf
textanalysis-example-text
&lt;strong&gt;init&lt;/strong&gt;.py
README.md
test_001_registry.py
test_03_generator.py
test_010_generator.py
test_011_generator_cpu.py
test_012_generator_calculator.py
test_015_generator_azureai.py
test_020_server_manage.py
test_server_cms_cpu.py
util.py&lt;/p&gt;
&lt;p&gt;THIS WAS HERE BEFORE&lt;/p&gt;
&lt;h2 id=&#34;test_001_registrypy&#34;&gt;test_001_registry.py&lt;/h2&gt;
&lt;p&gt;This test has 5 test functions&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;test_registry_add&lt;/li&gt;
&lt;li&gt;test_registry_list_name&lt;/li&gt;
&lt;li&gt;test_registry_list&lt;/li&gt;
&lt;li&gt;test_registry_delete&lt;/li&gt;
&lt;li&gt;test_benchmark&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Test 1 calls registry and adds to the registry. If successful prints &amp;lsquo;PASSED&amp;rsquo;&lt;/p&gt;
&lt;p&gt;Test 2 calls registry and prints ONLY the server specified in filename.&lt;/p&gt;
&lt;p&gt;Test 3 calls registry and print list for ALL servers in registry.&lt;/p&gt;
&lt;p&gt;Test 4 calls registry and deletes entry for filename.&lt;/p&gt;
&lt;p&gt;Test 5 runs benchmark test on registry.&lt;/p&gt;
&lt;h3 id=&#34;how-to-call-this&#34;&gt;How to call this&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cms &lt;span style=&#34;color:#204a87&#34;&gt;set&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;filename&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#34;./tests/server-cpu/cpu.yaml&amp;#34;&lt;/span&gt;
pytest -v --capture&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;no tests/test_001_registry.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;deprecated
examples
generator
generator-calculator
generator-printerclass
generator-testclass
server-class
server-cms
server-cms-simple
server-cpu
server-sample
server-sampleFunction
textanalysis-example-text
&lt;strong&gt;init&lt;/strong&gt;.py
README.md
test_001_registry.py  Falconi
test_03_generator.py  jonthan
test_010_generator.py jonthan
test_011_generator_cpu.py prateek
test_012_generator_calculator.py prateek
test_020_server_manage.py ishan
test_server_cms_cpu.py andrew&amp;ndash;&amp;gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/cloudmesh-openapi/tests/server-cpu/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/cloudmesh-openapi/tests/server-cpu/readme/</guid>
      <description>
        
        
        &lt;h1 id=&#34;test-it-yourself&#34;&gt;Test it yourself&lt;/h1&gt;
&lt;p&gt;cd to &lt;code&gt;cloudmesh-openapi&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Start the service&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapii server start ./tests/server-cpu.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Stop the service&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms openapii3 server stop cpu
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;urls&lt;/p&gt;
&lt;p&gt;cloudmesh/ui&lt;/p&gt;
&lt;p&gt;cloudmesh/cpu&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/cloudmesh-openapi/tests/test_mlperf/readme-source/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/cloudmesh-openapi/tests/test_mlperf/readme-source/</guid>
      <description>
        
        
        &lt;h1 id=&#34;mlperf-tests&#34;&gt;MLPerf Tests&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;http://mlperf.org&#34;&gt;MLperf&lt;/a&gt; [@www-mlperf] provides &amp;ldquo;fair and useful benchmarks for measuring
training and inference performance of ML hardware, software, and
services&amp;rdquo;&lt;/p&gt;
&lt;p&gt;In this benchmark we will&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Deploy MLPerf on the system&lt;/li&gt;
&lt;li&gt;Use functions that run a number of tests as inout to the OpenAPI Gnerator&lt;/li&gt;
&lt;li&gt;From these functions we run our OpenAPI generator to create a service
that allows to run the MLperf examples through a Web service with
http calls&lt;/li&gt;
&lt;li&gt;Test out the created functions by running selected example invocations&lt;/li&gt;
&lt;li&gt;Report the time it takes to run these examples&lt;/li&gt;
&lt;li&gt;Provide a Makefile or python script that allows us to conveniently
cun these tests&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;deployment&#34;&gt;Deployment&lt;/h2&gt;
&lt;p&gt;Describe how to deploy&lt;/p&gt;
&lt;h3 id=&#34;reports-for-running-the-tests-on-machines&#34;&gt;Reports for running the tests on Machines&lt;/h3&gt;
&lt;p&gt;Provide summary information about teh runtime
Provide details do checked in results in the &lt;a href=&#34;results&#34;&gt;results&lt;/a&gt; directory&lt;/p&gt;
&lt;h3 id=&#34;local-output&#34;&gt;Local Output&lt;/h3&gt;
&lt;p&gt;All output is written into a &lt;code&gt;~/.cloudmesh/dest/benchmark/mlperf&lt;/code&gt; folder
which can be removed after the test is completed. In the results folder
we also find a copy of the OpenAPI YAML file that is generated with the
cenerator. This file can also be used to compare the generated output.&lt;/p&gt;
&lt;h2 id=&#34;selected-benchmarks&#34;&gt;Selected Benchmarks&lt;/h2&gt;
&lt;p&gt;Describe which benchmarks were selected&lt;/p&gt;
&lt;h2 id=&#34;functions&#34;&gt;Functions&lt;/h2&gt;
&lt;p&gt;Short description aboutthe functions that have been defined&lt;/p&gt;
&lt;h2 id=&#34;opeanapi&#34;&gt;OpeanAPI&lt;/h2&gt;
&lt;p&gt;Describe where to find the generated functions
Link th=o wher ethe open api is created in the&lt;/p&gt;
&lt;h2 id=&#34;how-to-run-individual-tests&#34;&gt;How to run individual tests&lt;/h2&gt;
&lt;p&gt;Describe how to run indific=dual Tests&lt;/p&gt;
&lt;h2 id=&#34;benchmarks&#34;&gt;Benchmarks&lt;/h2&gt;
&lt;p&gt;Links to benchmarks that are listed in the &lt;a href=&#34;results&#34;&gt;results&lt;/a&gt; directory&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/cloudmesh-openapi/tests/test_mlperf/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/cloudmesh-openapi/tests/test_mlperf/readme/</guid>
      <description>
        
        
        &lt;h1 id=&#34;mlperf-tests&#34;&gt;MLPerf Tests&lt;/h1&gt;
&lt;p&gt;According to &lt;a href=&#34;https://mlperf.org/&#34;&gt;https://mlperf.org/&lt;/a&gt; MLPerf provides &amp;quot; Fair and useful
benchmarks for measuring training and inference performance of ML
hardware, software, and services&amp;rdquo; [@www-mlperf]&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/cloudmesh-openapi/tests/test_mlperf/results/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/cloudmesh-openapi/tests/test_mlperf/results/readme/</guid>
      <description>
        
        
        &lt;p&gt;put your result files here&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/cloudmesh-openapi/tests/timeseries-example/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/cloudmesh-openapi/tests/timeseries-example/readme/</guid>
      <description>
        
        
        &lt;h1 id=&#34;time-series-forecast-using-multi-cloud-ai-services&#34;&gt;Time Series Forecast using Multi Cloud AI Services&lt;/h1&gt;
&lt;p&gt;Prafull Porwal, &lt;a href=&#34;https://github.com/cloudmesh-community/sp20-516-255/blob/master/Cloudmesh-OpenAPI/Readme.md&#34;&gt;sp20-516-255&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/cloudmesh-community/sp20-516-255/graphs/contributors&#34;&gt;Contributors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/cloudmesh-community/fa19-516-147/pulse&#34;&gt;Insights&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/cloudmesh-community/sp20-516-255/tree/master/Cloudmesh-OpenAPI/AWSForecast&#34;&gt;Project Code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;objective&#34;&gt;Objective&lt;/h2&gt;
&lt;p&gt;Develop Open API for time series forecasting on multiple clouds&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Many cloud providers have introduced machine learning capabilities on their infrastructure. The project aims to provide an open API for timeseries forecasting for AWS using Forecast Services and S3&lt;/p&gt;
&lt;h3 id=&#34;aws-ai-service--forecast-open-api-service-features&#34;&gt;AWS AI Service : Forecast Open API Service Features&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Upload the data file to ./cloudmesh/upload-file location&lt;/li&gt;
&lt;li&gt;Upload the json schema file to ./cloudmesh/upload-file location&lt;/li&gt;
&lt;li&gt;Validate the data for missing and less than 0 values&lt;/li&gt;
&lt;li&gt;Split the dataset into Train and test by specifying split percentge.&lt;/li&gt;
&lt;li&gt;Provide list of Multi Cloud supported for Timeseries Forecasting&lt;/li&gt;
&lt;li&gt;Initialize the cloud service&lt;/li&gt;
&lt;li&gt;Create a Dataset Group&lt;/li&gt;
&lt;li&gt;Create a Target Time Series Dataset&lt;/li&gt;
&lt;li&gt;Import data into Forecast from AWS Storage S3&lt;/li&gt;
&lt;li&gt;Create a Predictor&lt;/li&gt;
&lt;li&gt;Generate Forecast&lt;/li&gt;
&lt;li&gt;Query the Forecast&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;additional-features&#34;&gt;Additional Features&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Multiple instance of the process supported&lt;/li&gt;
&lt;li&gt;Data Validation and missing values checks&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;environment-configuration&#34;&gt;Environment Configuration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Python 3.8.2 Python or newer.&lt;/li&gt;
&lt;li&gt;Use a venv (see developer install)&lt;/li&gt;
&lt;li&gt;MongoDB installed as regular program not as service&lt;/li&gt;
&lt;li&gt;AWS boto3 library&lt;/li&gt;
&lt;li&gt;Open API package installed&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Make sure that cloudmesh is properly installed on your machine and you have mongodb setup to work with cloudmesh.
More details can be found in the &lt;a href=&#34;https://cloudmesh.github.io/cloudmesh-manual/installation/install.html&#34;&gt;Cloudmesh Manual&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;openapi-package-installation&#34;&gt;OpenAPI package installation&lt;/h3&gt;
&lt;p&gt;Make sure you use a python venv before installing. Users can install the code with&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ pip install cloudmesh-openapi
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;pre-requisites-&#34;&gt;Pre Requisites :&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;add below parameter to cloudmesh.yaml for forecast service to work&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;bucket_name : awsforecastassignnment&lt;/li&gt;
&lt;li&gt;region_name : us-east-1&lt;/li&gt;
&lt;li&gt;forecast_srv : forecast&lt;/li&gt;
&lt;li&gt;forecastquery_srv : forecastquery&lt;/li&gt;
&lt;li&gt;s3_srv : s3&lt;/li&gt;
&lt;li&gt;iam_role_arn: XXXXXX&lt;/li&gt;
&lt;li&gt;algorithmArn: arn:aws:forecast:::algorithm/Deep_AR_Plus&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Data Format : The data should be in csv file format and must have&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;item_id : reference column for which time series forecast is required&lt;/li&gt;
&lt;li&gt;target_value : the column which need to be predicted, data type integer&lt;/li&gt;
&lt;li&gt;timestamp : timestamp of data samples&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.aws.amazon.com/forecast/latest/dg/API_CreateDataset.html&#34;&gt;AWS Time Series Forecast&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Json Schema : Json Schema file with name schema.json&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;quick-forecast-api-reference-commands&#34;&gt;Quick Forecast API reference Commands&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Start the open API server for the forecast service&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cms openapi server start .//forecast.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Check for supported AI services&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;curl http://localhost:8080/cloudmesh/forecast
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;e.g. output:
&amp;ldquo;model&amp;rdquo;: &amp;ldquo;Supported Time Series Forecast Services AWS : Forecast &amp;quot;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Upload file to the server from location (
File path should be the location on server where file is located.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;curl &lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#34;http://localhost:8080/cloudmesh/forecast/upload&amp;#34;&lt;/span&gt; -F &lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#34;upload=@&amp;lt;file_path&amp;gt;\countries-aggregated.csv&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;e.g. output:
countries-aggregated.csv uploaded successfully&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Validate data file&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;curl &lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#34;http://localhost:8080/cloudmesh/forecast/validate_data&amp;#34;&lt;/span&gt; -F &lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#34;upload=@&amp;lt;file_path&amp;gt;\countries-aggregated.csv&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;e.g. output:
countries-aggregated.csv validated successfully&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Split the data into test and train. Data should be validated first before splitting&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;curl http://localhost:8080/cloudmesh/forecast/split_data?split_pct&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;20&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;output: &amp;ldquo;Please validate the data first&amp;rdquo;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;curl http://localhost:8080/cloudmesh/forecast/split_data?split_pct&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;20&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;output: &amp;ldquo;Data split successfully&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Initialize aws parameters&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;curl &lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#34;http://localhost:8080/cloudmesh/forecast/aws&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;e.g. output:
{&amp;ldquo;model&amp;rdquo;:&amp;ldquo;AWS AI Service initialized successfully&amp;rdquo;}&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create Forecast, this is a multistep process, it cretes datasetgroup, dataset, import job, predictor and forecast&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;curl http://localhost:8080/cloudmesh/forecast/create_forecast?country&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;Austrailia
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This api expects cloud services to be already initialized if not it will request to initialize
output:&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Please initialize cloud service&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;output: &amp;ldquo;Forecast generated successfully&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lookup a Forecast&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;curl http://localhost:8080/cloudmesh/forecast/lookupForecast?countryName&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;Austrailia
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;output :
shows &lt;a href=&#34;https://github.com/cloudmesh-community/sp20-516-255/blob/master/Cloudmesh-OpenAPI/AWSForecast/sampleOutput&#34;&gt;ouput&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Delete Data Stack for the current project
This API should be executed at the end of the session to delete all the resources created for the analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;curl &lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#34;http://localhost:8080/cloudmesh/forecast/deletestack&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;algorithm-details&#34;&gt;Algorithm details&lt;/h2&gt;
&lt;p&gt;The AWS Forecast service supports following pre-defined algortithms&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Autoregressive Integrated Moving Average (ARIMA) Algorithm - arn:aws:forecast:::algorithm/ARIMA&lt;/li&gt;
&lt;li&gt;DeepAR+ Algorithm - arn:aws:forecast:::algorithm/Deep_AR_Plus&lt;/li&gt;
&lt;li&gt;Exponential Smoothing (ETS) - arn:aws:forecast:::algorithm/ETS&lt;/li&gt;
&lt;li&gt;Non-Parametric Time Series (NPTS) Algorithm - arn:aws:forecast:::algorithm/NPTS&lt;/li&gt;
&lt;li&gt;Prophet Algorithm - arn:aws:forecast:::algorithm/Prophet&lt;/li&gt;
&lt;li&gt;Supports hyperparameter optimization (HPO)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.aws.amazon.com/forecast/latest/dg/forecast.dg.pdf&#34;&gt;AWS Time Series Forecast&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;limitations&#34;&gt;Limitations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Requires data file with mandatory colums item_id, target_value and timestamp&lt;/li&gt;
&lt;li&gt;Requires a schema file schema.json to be provided by the user&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.aws.amazon.com/forecast/latest/dg/forecast.dg.pdf&#34;&gt;https://docs.aws.amazon.com/forecast/latest/dg/forecast.dg.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/aws-samples/amazon-forecast-samples&#34;&gt;https://github.com/aws-samples/amazon-forecast-samples&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-301/assignment6/assignment6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-301/assignment6/assignment6/</guid>
      <description>
        
        
        &lt;h1 id=&#34;assignment-6&#34;&gt;Assignment 6&lt;/h1&gt;
&lt;h1 id=&#34;health-and-medicine--artificial-intelligence-influence-on-ischemic-stroke-imaging&#34;&gt;Health and Medicine – Artificial Intelligence Influence on Ischemic Stroke Imaging&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Gavin Hemmerlein, fa20-523-301&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-301/blob/master/assignment6/assignment6.md&#34;&gt;Edit&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Keywords:&lt;/strong&gt; convolutional neural network, random forest learning, Computer Tomography Scan, CT Scan, stroke, artificial intelligence, deep learning, machine learning, large vessel occlusions&lt;/p&gt;
&lt;h2 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;The Computer Tomography Scan (CT Scan) is a medical procedure that involves multiple x-rays analyzed using computer aided techniques. The CT Scan’s creation was credited to Allan M. Cormack and Godfrey N. Hounsfield for which both individuals were awarded the 1979 Nobel Prize in Physiology or Medicine &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. The OECD estimates that there are a total of 42.64 million scanners located in the United States; the fourth most of any country &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. This prevalence is extremely important when discussing a diagnosis for stroke victims. In the 1980s, the identification techniques were generally done through a process called computer-aided diagnosis (CAD). “CAD usually relies on a combination of interpretation of medical images through computational algorithms and the physicians’ evaluation of the medical images &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;”.&lt;/p&gt;
&lt;p&gt;The goal of researchers and medical practitioners is to improve upon detection rates to ensure that more lives are saved by early detection. According to Johns Hopkins Medical Department the faster medical precautions can be given to a victim, the better the prognosis is for the individual &lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;. The brain requires a constant supply of blood and oxygen. When it is starved of these nutrients, the brain tissue begins to die.&lt;/p&gt;
&lt;h2 id=&#34;2-assisting-researchers-with-artificial-intelligence&#34;&gt;2. Assisting Researchers with Artificial Intelligence&lt;/h2&gt;
&lt;p&gt;According to an article in Radiology Business, automated detection of stroke anomalies is improving. As stated in a review in the article, “the team found convolutional neural networks beat random forest learning (RFL) on sensitivity, 85% to 68% &lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;.” This improvement is an excellent improvement by switching the algorithm that is used to train the model. A convolutional neural network (CNN) is a deep learning technique while a random forest is a modified decision tree. By modifying approaches from a decision tree to a deep learning technique, there is a very high likelihood that more lives could be saved. Strokes account for nearly 140,000 deaths a year and are one of the leading causes of permanent disability in the United States &lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;A RFL algorithm is a form of decision tree supervised learning. Decision trees are unique because they can also be used to solve regression and classification problems; which is unique to supervised learning methods. The RFL uses many decision trees that build upon one another. Where the CNN algorithm differs is that it is a form of deep learning that performs unsupervised learning. Each layer in the CNN understands its inputs and outputs while passing the output on to the next layer. A CNN can pass this information forward through a number of layers, but there is also a diminishing return given the amount of processing needed for each layer.&lt;/p&gt;
&lt;p&gt;After reviewing the literature from the Radiology Business article, the most common avenue for early detection appears to be the RFL as stated above. A meta analysis reviewing PubMed articles from January 2014 to February 2019 found that the RFL was the highest performer for predictive measures &lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;. For large vessel occlusions (LVO), the best approach was to use a CNN. CNN’s use little pre-processing and rely moreso on the filters with the data. This results in a more dynamic approach to the data as opposed to the harder developed structure of a decision tree.&lt;/p&gt;
&lt;h2 id=&#34;3-future-work&#34;&gt;3. Future Work&lt;/h2&gt;
&lt;p&gt;Upon examining the cited sources, there are some future areas to look research. To improve on current understanding, a standardization of metrics for to evaluate the fidelity of the models &lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;, continued development of automative image analysis software &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;, and leveraging emerging techniques to develop even more effective algorithms to detect large vessel occlusion &lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;. As of 2019, the advantage of CNN’s over conventional detection methods was only 7.6% &lt;sup id=&#34;fnref:9&#34;&gt;&lt;a href=&#34;#fn:9&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;. The percentage may seem marginal, but when expanded out to the 140,000 strokes per year the amount of strokes identified could be as much as 10,000 individuals.&lt;/p&gt;
&lt;p&gt;These areas are only a few of the many improvements that could be made in the world of stroke detection. It is not a far stretch to imagine detecting vessels that are becoming clogged or brittle. If detection of these medical issues could become prevalent, even more lives could be saved by predicting strokes before they even occur.&lt;/p&gt;
&lt;h2 id=&#34;4-references&#34;&gt;4. References&lt;/h2&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Nobel Prizes &amp;amp; Laureates, &amp;ldquo;The Nobel Prize in Physiology or Medicine 1979,&amp;rdquo; &lt;em&gt;The Nobel Prize,&lt;/em&gt; [Online]. Available:
&lt;a href=&#34;https://www.nobelprize.org/prizes/medicine/1979/summary/&#34;&gt;https://www.nobelprize.org/prizes/medicine/1979/summary/&lt;/a&gt; [Accessed Oct. 16, 2020]. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;OECD, &amp;ldquo;Computed tomography (CT) scanners,&amp;rdquo; &lt;em&gt;OECD Data,&lt;/em&gt; [Online]. Available:
&lt;a href=&#34;https://data.oecd.org/healtheqt/computed-tomography-ct-scanners.htm&#34;&gt;https://data.oecd.org/healtheqt/computed-tomography-ct-scanners.htm&lt;/a&gt; [Accessed Oct. 16, 2020]. &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Y. Mokli, J. Pfaff, D. Pinto dos Santos, C. Herweh, and S. Nagel &amp;ldquo;Computer-aided imaging analysis in acute ischemic stroke – background and clinical applications&amp;rdquo;, &lt;em&gt;Neurological Research and Practice&lt;/em&gt;, p. 1-13. 2020 [Online serial]. Available:  &lt;a href=&#34;https://neurolrespract.biomedcentral.com/track/pdf/10.1186/s42466-019-0028-y&#34;&gt;https://neurolrespract.biomedcentral.com/track/pdf/10.1186/s42466-019-0028-y&lt;/a&gt; [Accessed Oct. 13, 2020]. &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;A. Pruski, “Stroke Recovery Timeline,” &lt;em&gt;John Hopkins Medical,&lt;/em&gt; [Online]. Available: &lt;a href=&#34;https://www.hopkinsmedicine.org/health/conditions-and-diseases/stroke/stroke-recovery-timeline&#34;&gt;https://www.hopkinsmedicine.org/health/conditions-and-diseases/stroke/stroke-recovery-timeline&lt;/a&gt; [Accessed Oct. 16, 2020]. &lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;D. Pearson, &amp;ldquo;AI helps bust stroke, identify occlusions,&amp;rdquo; &lt;em&gt;Radiology Business,&lt;/em&gt; [Online]. Available:
&lt;a href=&#34;https://www.radiologybusiness.com/topics/artificial-intelligence/ai-helps-bust-stroke-identify-occlusions&#34;&gt;https://www.radiologybusiness.com/topics/artificial-intelligence/ai-helps-bust-stroke-identify-occlusions&lt;/a&gt; [Accessed Oct. 13, 2020]. &lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;The Internet Stroke Center, &amp;ldquo;About Strokes,&amp;rdquo; &lt;em&gt;Stroke Statistics,&lt;/em&gt; [Online]. Available:
&lt;a href=&#34;http://www.strokecenter.org/patients/about-stroke/stroke-statistics/#:~:text=More%20than%20140%2C000%20people%20die,and%20185%2C000%20are%20recurrent%20attacks&#34;&gt;http://www.strokecenter.org/patients/about-stroke/stroke-statistics/#:~:text=More%20than%20140%2C000%20people%20die,and%20185%2C000%20are%20recurrent%20attacks&lt;/a&gt; [Accessed Oct. 16, 2020]. &lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;N. Murray, &amp;ldquo;Artificial intelligence to diagnose ischemic stroke and identify large vessel occlusions: a systematic review,&amp;rdquo; &lt;em&gt;Journal of NeuroInterventional Surgery&lt;/em&gt;, vol. 12, no. 2, p. 156-164. 2020 [Online serial]. Available: &lt;a href=&#34;https://jnis.bmj.com/content/12/2/156&#34;&gt;https://jnis.bmj.com/content/12/2/156&lt;/a&gt; [Accessed Oct. 13, 2020]. &lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:8&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;M. Stib, J. Vasquez, M. Dong, Y. Kim, S. Subzwari, H. Triedman, A. Wang, H. Wang, A. Yao, M. Jayaraman, J. Boxerman, C. Eickhoff, U. Cetintemel, G. Baird, and R. McTaggart, &amp;ldquo;Detecting Large Vessel Occlusion at Multiphase CT Angiography by Using a Deep Convolutional Neural Network&amp;rdquo;, &lt;em&gt;Original Research Neuroradiology&lt;/em&gt;, Sep 29, 2020. [Online serial]. Available: &lt;a href=&#34;https://pubs.rsna.org/doi/full/10.1148/radiol.2020200334&#34;&gt;https://pubs.rsna.org/doi/full/10.1148/radiol.2020200334&lt;/a&gt; [Accessed Oct. 13, 2020]. &lt;a href=&#34;#fnref:8&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:9&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;J. Tuan, &amp;ldquo;How AI is able to Predict and Detect a Stroke&amp;rdquo;, &lt;em&gt;Referral MD&lt;/em&gt;. [Online]. Available: &lt;a href=&#34;https://getreferralmd.com/2019/10/how-ai-is-able-to-predict-and-detect-a-stroke/&#34;&gt;https://getreferralmd.com/2019/10/how-ai-is-able-to-predict-and-detect-a-stroke/&lt;/a&gt; [Accessed Oct. 13, 2020]. &lt;a href=&#34;#fnref:9&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-301/project/misc_files/blank/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-301/project/misc_files/blank/</guid>
      <description>
        
        
        &lt;h1 id=&#34;blank&#34;&gt;Blank&lt;/h1&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-301/project/project/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-301/project/project/</guid>
      <description>
        
        
        &lt;h1 id=&#34;nba-performance-and-injury&#34;&gt;NBA Performance and Injury&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Gavin Hemmerlein, fa20-523-301&lt;/li&gt;
&lt;li&gt;Chelsea Gorius, fa20-523-344&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-301/blob/master/project/project.md&#34;&gt;Edit&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;div class=&#34;pageinfo pageinfo-primary&#34;&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;please add abstract&lt;/p&gt;
&lt;p&gt;Contents&lt;/p&gt;
&lt;div class=&#34;toc&#34;&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#1-introduction&#34;&gt;1. Introduction&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#2-objective&#34;&gt;2. Objective&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#3-dataset&#34;&gt;3. Dataset&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#4-work-breakdown&#34;&gt;4. Work Breakdown&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#5-references&#34;&gt;5. References&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Keywords:&lt;/strong&gt; please add keywords&lt;/p&gt;
&lt;h2 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;The topic to be investigated is basketball player performance as it relates to injury. The topic of injury and recovery is a multi-billion dollar industry.  The Sports Medicine field is expected to reach $7.2 billion dollars by 2025 &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.  The scope of this effort is to explore National Basketball Association(NBA) teams, but the additional uses of a topic such as this could expand into other realms such as the National Football League, Major League Baseball, the Olympic Committees, and many other avenues.  For leagues with salaries, projecting an expected return on the investment can assist in contract negotiations and cater expectations.  Competing at such a high level of intensity puts these players at a greater risk to injury than the average athlete because of the intense and constant strain on their bodies.  The overall valuation of the NBA in recent years is over 2 billion dollars, meaning each team is spending millions of dollars in the pursuit of a championship every season.  Injuries to players can cost teams not only wins but also significant profits.  Ticket sales alone for a single NBA finals game have reported greater than 10 million dollars in profit for the home team, if a team&amp;rsquo;s star player gets injured just before the playoffs and the team does not succeed, that is a lot of money lost.  These injuries can have an effect no matter the time of year, regular season ticket sales have been known to fluctuate with injuries from the team&amp;rsquo;s top performers.  Besides ticket sales these injuries can also influence viewrship, TV or streaming, and potentially lead to a greater loss in profits.  With the health of the players and so much money at stake NBA team organizations as a whole do their best to take care of their players and keep them injury free.&lt;/p&gt;
&lt;h2 id=&#34;2-objective&#34;&gt;2. Objective&lt;/h2&gt;
&lt;p&gt;The objective of this project is to develop performance indicators for injured players returning to basketball in the NBA.  It is unreasonable to expect a player to return to the same level of play post injury immediately upon starting back up after recovery.  It often takes a player months if not years to return to the same level of play as pre-injury, especially considering the severity of the injuries.  In order to successfully analyse this information from the datasets, a predictive model will need to be created using a large set of the data to train.&lt;/p&gt;
&lt;p&gt;From this point, a test run will be used to gauge the validity and accuracy of the model compared to some of the data set aside.  The model created will be able to provide feature importance to give a better understanding of which specific features are the most crucial when it comes to determining how bad the effects of an injury may or may not be on player performance.  Feature engineering will be performed prior to training the model in order to improve the chances of higher accuracy from the predictions.  This model could be used to keep an eye out for how a player&amp;rsquo;s performance intensity and the engineered features could affect how long a player takes to recover from injury, if there are any warning signs prior to an injury, and even how well they perform when returning.&lt;/p&gt;
&lt;h2 id=&#34;3-dataset&#34;&gt;3. Dataset&lt;/h2&gt;
&lt;p&gt;To compare performance and injury, a minimum of two datasets will be needed. The first is a dataset of injuries for players &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. This dataset will create the samples necessary for review.&lt;/p&gt;
&lt;p&gt;Once the controls for injuries are established, the next requirement will be to establish  pre-injury performance parameters and post-injury parameters.  These areas will be where the feature engineering will take place.  The datasets needed must dive into appropriate basketball performance stats to establish a metric to encompass a player’s performance. One example that ESPN has tried in the past is the Player Efficiency Rating (PER).  To accomplish this, it will be important to review player performance within games such as in the “NBA games data” &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; dataset.  There is a potential to pull more data from other datasets such as the “NBA Enhanced Box Score and Standings (2012 - 2018)” &lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;.  It is important to use the in depth data from the “NBA games data” &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. dataset because of how it will allow us to see how the player was performing throughout the season, and not just their average stats across the year.  With in depth information about each game of the season, and not just the teams and players aggregated stats, added to the data provided from the injury dataset &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; we will be able to compose new metrics to understand how these injuries are actually affecting the players performance.&lt;/p&gt;
&lt;p&gt;Along the way we look forward to discovering if there is also a causal relationship to the severity of some of the injuries, based on how the player was performing just before the injury.  The term “load management” has become popular in recent years to describe players taking rest periodically throughout the season in order to prevent injury from overplaying.  This new practice has received both support for the player safety it provides and also criticism around players taking too much time off.  Of course not all injuries are entirely based on the recent strain under the players body, but a better understanding about how that affects the injury as a whole could give better insight into avoiding more injuries.  It is important to remember though that any pattern identification would not lead to an elimination of all injuries, any contact sport will continue to have injuries, especially one as high impact as the NBA.  There is value to learn from why some players are able to return from certain injuries more quickly and why some return to almost equivalent or better playing performance than before the injury.  This comparison of performance will be made by deriving metrics based on varying ranges of games immediately leading up to injury and then immediately after returning from injury.  In addition to that we will perform comparisons to the players known peak performance to better understand how the injury affected them.  Another factor it will be important to include is the length of time recovering from the injury. Different players take differing amounts of time off, sometimes even with similar injuries.  Something will be said about the player’s dedication to recovery and determination to remain at peak performance, even through injury, when looking at how severe their injury was, how much time was taken for recovery, and how they performed upon returning.&lt;/p&gt;
&lt;p&gt;These datasets were chosen because they allow for a review of individual game performance, for each team, throughout each season in the recent decade.  Aggregate statistics such as points per game (ppg) can be deceptive because duration of the metric is such a large period of time.  The large sample of 82 games can lead to a perception issue when reviewing the data.  These datasets include more variables to help us determine effects to player injury, such as minutes per game (mpg) to understand how strenuous the pre-injury performance or how fatigue may have played a factor in the injury.  Understanding more of the variables such as fouls given or drawn can help determine if the player or other team seemed to be the primary aggressor before any injury.&lt;/p&gt;
&lt;h2 id=&#34;4-work-breakdown&#34;&gt;4. Work Breakdown&lt;/h2&gt;
&lt;p&gt;Initial Project Report - Gorius, Hemmerlein
Predictive Model - breakdown udetermined at this time&lt;/p&gt;
&lt;h2 id=&#34;5-references&#34;&gt;5. References&lt;/h2&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;A. Mehra, &lt;em&gt;Sports Medicine Market worth $7.2 billion by 2025&lt;/em&gt;, Markets and Markets.
&lt;a href=&#34;https://www.marketsandmarkets.com/PressReleases/sports-medicine-devices.asp&#34;&gt;https://www.marketsandmarkets.com/PressReleases/sports-medicine-devices.asp&lt;/a&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;R. Hopkins, &lt;em&gt;NBA Injuries from 2010-2020&lt;/em&gt;, Kaggle. &lt;a href=&#34;https://www.kaggle.com/ghopkins/nba-injuries-2010-2018&#34;&gt;https://www.kaggle.com/ghopkins/nba-injuries-2010-2018&lt;/a&gt; &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;N. Lauga, &lt;em&gt;NBA games data&lt;/em&gt;, Kaggle.  &lt;a href=&#34;https://www.kaggle.com/nathanlauga/nba-games?select=games_details.csv&#34;&gt;https://www.kaggle.com/nathanlauga/nba-games?select=games_details.csv&lt;/a&gt; &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;P. Rossotti, &lt;em&gt;NBA Enhanced Box Score and Standings (2012 - 2018)&lt;/em&gt;, Kaggle. &lt;a href=&#34;https://www.kaggle.com/pablote/nba-enhanced-stats&#34;&gt;https://www.kaggle.com/pablote/nba-enhanced-stats&lt;/a&gt; &lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-301/test/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-301/test/</guid>
      <description>
        
        
        &lt;h2 id=&#34;gavin-hemmerlein&#34;&gt;Gavin Hemmerlein&lt;/h2&gt;
&lt;h2 id=&#34;ghemmer&#34;&gt;ghemmer&lt;/h2&gt;
&lt;h2 id=&#34;engr-e-534&#34;&gt;ENGR-E 534&lt;/h2&gt;
&lt;p&gt;This is a test MarkDown file to ensure I have write privileges.&lt;/p&gt;
&lt;h1 id=&#34;test-typing&#34;&gt;Test Typing&lt;/h1&gt;
&lt;p&gt;This appears to be &lt;em&gt;working.&lt;/em&gt;&lt;/p&gt;
&lt;h1 id=&#34;table&#34;&gt;Table&lt;/h1&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Col1&lt;/th&gt;
&lt;th&gt;Col2&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Row 1&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Row 2&lt;/td&gt;
&lt;td&gt;21&lt;/td&gt;
&lt;td&gt;22&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id=&#34;images&#34;&gt;Images&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://assets.iu.edu/brand/3.2.x/trident-large.png&#34; alt=&#34;Image of IU Logo&#34;&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-304/test/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-304/test/</guid>
      <description>
        
        
        &lt;h1 id=&#34;header&#34;&gt;Header&lt;/h1&gt;
&lt;h2 id=&#34;sub-header-with&#34;&gt;Sub Header with&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Bulleted&lt;/li&gt;
&lt;li&gt;lists&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sub-header-with-1&#34;&gt;Sub Header with&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Numbered&lt;/li&gt;
&lt;li&gt;Lists&lt;/li&gt;
&lt;/ol&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-305/homework3/cody_harris_hw3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-305/homework3/cody_harris_hw3/</guid>
      <description>
        
        
        &lt;h1 id=&#34;square-kilometer-array-ska-use-case&#34;&gt;Square Kilometer Array (SKA) Use Case&lt;/h1&gt;
&lt;p&gt;The SKA is an unprecedented, international, engineering endeavor to create the largest radio telescope in the world. Completion of this project requires the use of state-of-the-art technologies to facilitate the massive amount of data that will be captured [1]. Once this data is captured, it will require advanced high-performance computing centers to make sense of the data and gain valuable insight. While there are many innovative ideas involved with the SKA, this use case will only examine the technologies and processes involved with the solutions directly related to the SKA’s big data needs.&lt;/p&gt;
&lt;h1 id=&#34;what-is-a-radio-telescope&#34;&gt;What is a radio telescope?&lt;/h1&gt;
&lt;p&gt;Before understanding the data needs of the SKA, it is important to understand what a radio telescope is. Many people are familiar with a regular telescope that uses a series of lenses to amplify light waves from distant places to create an image. A radio telescope is similar in the fact that it collects weak electromagnetic radiation from far distances, and then amplifies it so that it can be analyzed. Another application could be to send radio waves towards a direction and then record the reflection off celestial bodies. In any case, the signal’s that astronomers are interested in are extremely weak. Many earthly sources of electro-magnetic radiation are many times greater in strength. There are multiple ways to combat this noise from earth-based radiation, and some of it could be done using hardware, or software, but there are also other ways to combat this that the SKA is utilizing.
Modern radio telescopes accept a wide range of radio frequencies, and then computationally split the frequencies into up to many thousands of channels. To further complicate things, while increasing the efficacy of the radio telescopes, generally more than one telescope is used. This allows multiple positions on the ground to receive the same radio signal, but at slightly different times and slightly different phases of the waveform. This variation allows for more complex analysis of the radio signal. Obviously, this adds another step in the computational work, but having a large array of radio telescopes is imperative to accomplish most modern astronomical research goals [2].&lt;/p&gt;
&lt;h1 id=&#34;science-goals&#34;&gt;Science Goals&lt;/h1&gt;
&lt;p&gt;The vast size of the SKA project allows the exploration of a variety of burning questions that not only intrigue astrophysicists, but nearly everyone on the planet. One overreaching design goal of the SKA is to have a design flexible enough that it can be used as a “discovery machine” for the “exploration of the unknown”. With that said, there are five broad research goals of the SKA [3].&lt;/p&gt;
&lt;h2 id=&#34;galaxy-evolution-and-dark-energy&#34;&gt;Galaxy Evolution and Dark Energy&lt;/h2&gt;
&lt;p&gt;As a central goal of the SKA, this is quite a broad question that requires a great deal of study to fully understand. With the data gathered, researchers how to understand fundamental questions about how galaxies change over the course of their lifetimes. One problem with studying this, is that most galaxies nearest to us are so far along in their evolution that it is hard to know what happens in the early years of the galaxy. We can overcome this challenge with SKA, due to its “sensitivity and resolution”. The SKA will be able to focus on younger galaxies that are much earlier in their evolution to study what our galaxy was like shortly after the big bang.
To gain an understanding of the creation and evolution of galaxies, a study of dark energy must be done. While this mysterious energy has made headlines in the past decade, it is still the subject of a lot of speculation. As gravity is a main driving factor in the evolution of cosmic objects, understanding dark energy is needed to gain a full picture of what is happening in galactical evolution. Currently our fundamental physical theories, derived by Einstein, suggest that universal expansion should be slowing, but it is not. This is where dark energy plays a part in the formation of our universe [4].&lt;/p&gt;
&lt;h2 id=&#34;was-einsteins-theory-of-relativity-correct&#34;&gt;Was Einstein’s theory of relativity, correct?&lt;/h2&gt;
&lt;p&gt;It is a tall order to question the most influential physicist in history. Technology is catching up with our theoretical understanding of physics so that we can test fundamental theories that we have held true for many years. The SKA hopes to use its incredible sensitivity to investigate gravitational waves from extremely powerful sources of gravity such as black holes. While Einstein’s theories are very likely to be mostly true, they might not be fully complete and that is what SKA hopes to find out [1].&lt;/p&gt;
&lt;h2 id=&#34;what-are-the-sources-of-large-magnetic-fields-in-space&#34;&gt;What are the sources of large magnetic fields in space?&lt;/h2&gt;
&lt;p&gt;We know that our earth creates a magnetic field that is imperative for life to exist. For the most part we understand that this is due to the composition and actions of the core of the planet. When it comes to the origin of magnetic fields in space, we are not completely sure what creates all the fields. The study of these magnetic fields will allow further study of the evolution of galaxies and our universe [5].&lt;/p&gt;
&lt;h2 id=&#34;what-are-the-origins-of-our-universe&#34;&gt;What are the origins of our universe?&lt;/h2&gt;
&lt;p&gt;This is a burning question that we have some theories about, but still have a great deal of exploration to do on the topic. The prevailing theory relies on the big bang, but the SKA hopes to further study the eras shortly after the big bang to gain insight into the origins of our universe. The SKA hopes to do this by once again using its sensitivity to give the most accurate measurements of the initial light sources in our universe [6]. As long this question remains unsolved, humans will always want to understand where we all came from.&lt;/p&gt;
&lt;h2 id=&#34;as-living-beings-are-we-alone-in-the-universe&#34;&gt;As living beings, are we alone in the universe?&lt;/h2&gt;
&lt;p&gt;Using Drake’s equation, and new exoplanet information, scientists are extremely optimistic that life exists somewhere in our universe. In some estimates, what has happened on our planet, could have happened about “10 billion other times over in cosmic history!” [7].  One way that SKA can look for extraterrestrial life is by searching for radio signals sent out by advanced civilizations such as ours. Another way that SKA could look for extraterrestrial life is by looking for signs of the building blocks of life. One of these building blocks are amino acids, which can be identified by the SKA.&lt;/p&gt;
&lt;h1 id=&#34;current-progress&#34;&gt;Current Progress&lt;/h1&gt;
&lt;p&gt;The SKA telescopes reside in two separate locations. One location is in Western Australia and will be focused on low frequencies. The second location is in South Africa and will have two arrays, one for mid frequencies, and one for mid to high frequency [8].&lt;/p&gt;
&lt;h2 id=&#34;south-africa&#34;&gt;South Africa&lt;/h2&gt;
&lt;p&gt;Design and preparations for the final SKA implementation are still on-going. Currently there are two arrays named KAT7 and MeerKAT that are installed and functioning and will be the precursor to the SKA arrays in South Africa.&lt;/p&gt;
&lt;h2 id=&#34;australia&#34;&gt;Australia&lt;/h2&gt;
&lt;p&gt;This site also has a precursor to SKA already operating named ASKAP. It is currently located in the same location that the SKA’s major components will eventually occupy, so this will give insights into the performance of this location for radio telescopes. Also, in Australia, as recent as in the past year, prototype antennas are being setup in smaller arrays to capture data and run tests before the design is used in the final array [10].&lt;/p&gt;
&lt;h1 id=&#34;big-data-challenges-and-solutions&#34;&gt;Big Data Challenges and Solutions&lt;/h1&gt;
&lt;p&gt;The SKA presents many big data challenges, from preprocessing to long-term storage of data. The estimated output of all the telescopes is around 700 PB per year [12].&lt;/p&gt;
&lt;h2 id=&#34;raw-data-and-preprocessing&#34;&gt;Raw Data and Preprocessing&lt;/h2&gt;
&lt;p&gt;The data comes in the form of an analog radio signals that are collected over a vast geographical area. At some point, to do analytics on the data, the data needs to be converted from analog to digital. While this is usually done via hardware, and is not on computational machines, this is still a data processing step that must be done at scale.
There is also some preprocessing of the data, that must happen constantly as data is collected. While this could be done once reaching the supercomputer, it is a repetitive task that could be done using FPGAs. The benefit of using a FGPA is that it can parallel process in many more threads and do repetitive algorithms faster and with less power as normal CPUs [12].&lt;/p&gt;
&lt;h2 id=&#34;storage-and-access&#34;&gt;Storage and Access&lt;/h2&gt;
&lt;p&gt;As mentioned previously, the estimated data output of the telescope at peak is 700 PB. The initiative also hopes to save all data for the lifetime of the project which is around 50 years. This ends up being in the realm of needing to eventually store 35 EB of data. For more immediate storage, the SKA team plans to use a buffer system. The way this works is by having a large array of fast read and write storage devices such as SSDs and NVMe (a specialized SSD). This buffer will immediately take in the data as it is coming in at rates that require write speeds that are not as prevalent with traditional spinning disks. After being written to this buffer, they will slowly move the data onto more affordable solutions, that have slower read/write speeds.
While the team could use SSDs for the entire storage, the cost would be enormous. It is much more cost effective to have most of the data stored on hard disk. When it comes to long-term storage of data, even cheaper sources of data such as tape drives could be utilized. After a certain time from data collection, the data will be opened up to the public, this means that the data will likely not end up in a cold storage system [12].&lt;/p&gt;
&lt;h2 id=&#34;processing-of-data&#34;&gt;Processing of data&lt;/h2&gt;
&lt;p&gt;Currently, the processing of data will be done at a large network of sites that will be made up of a variety of technologies. Mostly, no new high-performance computing centers will be created. Existing infrastructures, including public clouds will be used for the processing of data. Along with using FPGAs for pre-processing and possibly more processing afterwards, the SKA team plans to use GPU accelerators to allow for efficient processing.
Each team of researchers will have various goals that they will want from the data. This means that they will have a variety of processing needs, which will be carried out in SKA Regional Centers (SRCs). This might mean machine learning programs to get insights from the data, all the way to other mathematical operations to make the data ready for study. In any case, it is the expectation that this additional data is preserved as well, leading to even more data needing to be managed [12].&lt;/p&gt;
&lt;h2 id=&#34;other-challenges&#34;&gt;Other Challenges&lt;/h2&gt;
&lt;p&gt;While this data is not the most sensitive data on the planet, it is important that security is considered. The SKA team is planning on creating a sort of firewall between users and the actual HPC centers by using an AAAI (authorization, access, authentication, and identification) system. Security of proprietary data will be a concern that will have to be addressed. As there is a large team working on the project, as well as many external actors, security becomes extremely complex, especially the more access points there are to the data [12].
A project this large and versatile requires the use of many software tools. These software tools generally need some level or automatic communication if they are used together in a project. With a large number of tools, there becomes a complex IT infrastructure that needs to be managed, and constantly monitored. It is possible for one tool to receive a critical update, and then cause issues with integration of other software systems.&lt;/p&gt;
&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;
&lt;p&gt;[1] &amp;ldquo;Square Kilometre Array - ICRAR&amp;rdquo;, ICRAR, 2020. [Online]. Available: &lt;a href=&#34;https://www.icrar.org/our-research/ska/&#34;&gt;https://www.icrar.org/our-research/ska/&lt;/a&gt;. [Accessed: 23- Sep- 2020].&lt;br&gt;
[2] &amp;ldquo;What are Radio Telescopes? - National Radio Astronomy Observatory&amp;rdquo;, National Radio Astronomy Observatory, 2020. [Online]. Available:                                              &lt;a href=&#34;https://public.nrao.edu/telescopes/radio-telescopes/&#34;&gt;https://public.nrao.edu/telescopes/radio-telescopes/&lt;/a&gt;. [Accessed: 23- Sep- 2020].&lt;br&gt;
[3] &amp;ldquo;SKA Science - Public Website&amp;rdquo;, SQUARE KILOMETRE ARRAY, 2020. [Online]. Available: &lt;a href=&#34;https://www.skatelescope.org/science/&#34;&gt;https://www.skatelescope.org/science/&lt;/a&gt;. [Accessed: 24-      Sep- 2020].&lt;br&gt;
[4] &amp;ldquo;Galaxy Evolution, Cosmology and Dark Energy - Public Website&amp;rdquo;, SQUARE KILOMETRE ARRAY, 2020. [Online]. Available:      &lt;a href=&#34;https://www.skatelescope.org/galaxyevolution/&#34;&gt;https://www.skatelescope.org/galaxyevolution/&lt;/a&gt;. [Accessed:      24- Sep- 2020].&lt;br&gt;
[5] &amp;ldquo;Cosmic Magnetism - Public Website&amp;rdquo;, SQUARE KILOMETRE ARRAY, 2020. [Online]. Available: &lt;a href=&#34;https://www.skatelescope.org/magnetism/&#34;&gt;https://www.skatelescope.org/magnetism/&lt;/a&gt;. [Accessed: 24- Sep- 2020].&lt;br&gt;
[6] &amp;ldquo;Probing the Cosmic Dawn - Public Website&amp;rdquo;, SQUARE KILOMETRE ARRAY, 2020. [Online]. Available: &lt;a href=&#34;https://www.skatelescope.org/cosmicdawn/&#34;&gt;https://www.skatelescope.org/cosmicdawn/&lt;/a&gt;. [Accessed: 24- Sep- 2020].&lt;br&gt;
[7] L. Sierra, &amp;ldquo;Are we alone in the universe? Revisiting the Drake equation&amp;rdquo;, Exoplanet Exploration: Planets Beyond our Solar System, 2020. [Online]. Available: &lt;a href=&#34;https://exoplanets.nasa.gov/news/1350/are-we-alone-in-the-universe-revisiting-the-drake-equation/&#34;&gt;https://exoplanets.nasa.gov/news/1350/are-we-alone-in-the-universe-revisiting-the-drake-equation/&lt;/a&gt;. [Accessed: 24- Sep- 2020].&lt;br&gt;
[8] &amp;ldquo;Design - ICRAR&amp;rdquo;, ICRAR, 2020. [Online]. Available: &lt;a href=&#34;https://www.icrar.org/our-research/ska/design/&#34;&gt;https://www.icrar.org/our-research/ska/design/&lt;/a&gt;. [Accessed: 24- Sep- 2020].&lt;br&gt;
[9] &amp;ldquo;Africa - Public Website&amp;rdquo;, SQUARE KILOMETRE ARRAY, 2020. [Online]. Available: &lt;a href=&#34;https://www.skatelescope.org/africa/&#34;&gt;https://www.skatelescope.org/africa/&lt;/a&gt;. [Accessed: 24- Sep- 2020].&lt;br&gt;
[10] Square Kilometre Array, Building a giant telescope in the outback - part 2. 2020.&lt;br&gt;
[11] &amp;ldquo;Australia - Public Website&amp;rdquo;, SQUARE KILOMETRE ARRAY, 2020. [Online]. Available: &lt;a href=&#34;https://www.skatelescope.org/australia/&#34;&gt;https://www.skatelescope.org/australia/&lt;/a&gt;. [Accessed: 24- Sep- 2020].&lt;br&gt;
[12] Filled in Use Case Survey for SKA&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-305/project/old_project/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-305/project/old_project/</guid>
      <description>
        
        
        &lt;h1 id=&#34;how-big-data-technologies-can-improve-indoor-agriculture&#34;&gt;How Big Data Technologies Can Improve Indoor Agriculture&lt;/h1&gt;
&lt;h2 id=&#34;team&#34;&gt;Team&lt;/h2&gt;
&lt;h1 id=&#34;harrcodyiuedu&#34;&gt;Cody Harris
&lt;a href=&#34;mailto:harrcody@iu.edu&#34;&gt;harrcody@iu.edu&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Cody Harris, &lt;a href=&#34;mailto:harrcody@iu.edu&#34;&gt;harrcody@iu.edu&lt;/a&gt;, fa20-523-305&lt;/p&gt;


&lt;div class=&#34;pageinfo pageinfo-primary&#34;&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;p&gt;Contents&lt;/p&gt;
&lt;div class=&#34;toc&#34;&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#team&#34;&gt;Team&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;

  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#topic-discussion&#34;&gt;Topic Discussion&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#dataset&#34;&gt;Dataset&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#getting-a-good-grade&#34;&gt;Getting a Good Grade&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#data-storage-and-streaming&#34;&gt;Data Storage and Streaming&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#analytics&#34;&gt;Analytics&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#hardware&#34;&gt;Hardware&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#novel-ideas&#34;&gt;Novel Ideas&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#extensions&#34;&gt;Extensions&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Keywords:&lt;/strong&gt; agriculture&lt;/p&gt;
&lt;h2 id=&#34;topic-discussion&#34;&gt;Topic Discussion&lt;/h2&gt;
&lt;p&gt;The overall topic of the project will include investigating how a host of Big Data technologies could be leveraged to improve multiple facets of the growing and distribution process for indoor farmers. One of the biggest benefits of growing indoors is the ability to precisely control the growing environment. From the light intensity and spectrum, to the nutrients given to the plant, there is an optimal combination of variables that produce the best results. Each farmer has priorities, whether those are yield, produce quality or a combination of various factors, it is a complex system that requires experimentation and robust tools to see the best results. There are a host of IoT sensors and controllers that can be employed to help monitor and control the growing environment, these all produce vast amounts of data that must be sifted through to extract insights. For sizeable farms, this produces big data problems that must be overcome.&lt;/p&gt;
&lt;p&gt;While some insights from the data can come during or directly after the growing “season”, some requires the produce to hit the shelves or to be used to create various food products. This means monitoring continues through the logistics process, and this data is integral when it comes to the end result of the produce. All this data allows for traceability in the food supply chain as well, in which big data technologies are perfect to handle.&lt;/p&gt;
&lt;p&gt;The end goal is to investigate and implement a scalable solution that follows the farmers crops from seed to consumers tables and optimizes the process along the way. Although indoor farms allow for great control, it is important to understand that there are many costs that are not associated with traditional farms. This means that to make the farming endeavor sustainable, optimization is important.&lt;/p&gt;
&lt;h2 id=&#34;dataset&#34;&gt;Dataset&lt;/h2&gt;
&lt;p&gt;There are not any publicly available data sources that meet the needs of this project. In order to accomplish the goal of building an analytics platform for an indoor farm and the related logistics, simulation data will be created. The simulation data will encompass many different situations that could be encountered and labeled by these issues. Along with possible issues, the data will include mostly satisfactory situations, as this is what the farmer is most likely to encounter.&lt;/p&gt;
&lt;p&gt;The majority of the data being worked with will be streaming sensor data. The sensors will include: PAR (Photosynthetically Active Radiation), temperature, humidity, pH of grow medium, and CO2. Along with these streaming measurements, other data points about the specific grow area will be taken. A grow area could be as small as a row of ten plants, all the way to thousands of plants. These datapoints could be things such as phenotype, light spectrum, light cycle, feeding schedule, or any other labels that could be beneficial in determining the end attributes of the given produce.&lt;/p&gt;
&lt;p&gt;Variance in the data will come in a variety of forms. As completely identical data is not analogous to real life, it can be for certain measurements. For example, in an indoor growing environment equipped with HVAC, the temperature might only fluctuate a few percent all day, and then have a rapid change as the lights are turned off. With this in mind, certain simulation measurements should not have a great deal of noise in them unless trying to simulate an adverse event.&lt;/p&gt;
&lt;p&gt;As this data is streaming, the dataset will be a time series that needs to be handled in the sense of streaming as well as in a postmortem capacity. The size of the data should be large enough to properly simulate a large farm over the course of a grow cycle. This data set will simulate an experiment, in which a variety of conditions or phenotypes will be compared to a control. Within the realm of the experiment the simulation data might include adverse events, such as a power outage, that could actually occur during an experiment and these adverse events must be accounted for in the end conclusion.&lt;/p&gt;
&lt;h2 id=&#34;getting-a-good-grade&#34;&gt;Getting a Good Grade&lt;/h2&gt;
&lt;p&gt;As I am a graduate student, I am expected to not only write a report, but also create a software component. For the software, it will be a proof of concept to show that a scalable solution could be built to use open source big data technologies. The report will detail the work done in the solution being built, as well as exploring ideas that cannot be built but are required for the full solution to be implemented.&lt;/p&gt;
&lt;h3 id=&#34;data-storage-and-streaming&#34;&gt;Data Storage and Streaming&lt;/h3&gt;
&lt;p&gt;With a focus on open source platforms, Apache has solutions that can be leveraged to handle many aspects of big data streaming and storage in a distributed computing environment. While more investigation needs to be done on the exact software that will be used, Hadoop, Spark, or the combination of the two will be used to handle the large amount of data, whether that is for longer term storage or real time streaming. Another Apache system that will be evaluated is Kafka, but again, there are many possibly solutions to be used. The goal is to stay within the Apache environment as it is widely used in industry as well as is an open source platform.&lt;/p&gt;
&lt;h3 id=&#34;analytics&#34;&gt;Analytics&lt;/h3&gt;
&lt;p&gt;The analytics component of this project is diverse. While all goals might not be able to be achieved in the proof of work, all of the data needs that are required for the growing and logistics processes of an indoor farm will be evaluated and explained in the final report. There are two main components to the analytics: real time analytics and historic data analysis. While some models are being fine tuned for the specific farm, the real time analytics will likely be mostly monitoring at the beginning. As grow seasons go by and metrics are collected on the harvest, the real time analytics will be informed by the historic data using some sort of machine learning processes. These analytic goals can likely be completed using tools within Spark, using MLlib. If this cannot be accomplished, then another library will be used such as sci-kit learn.&lt;/p&gt;
&lt;h3 id=&#34;hardware&#34;&gt;Hardware&lt;/h3&gt;
&lt;p&gt;It is important that the proof of concept is designed for a distributed computing environment. The goal is to create open source software that can be used by small farms that sell solely at farmers markets, all the way to large commercial operations. When designing in this way, growing pains in the future can be minimized. For the hardware being used, multiple solutions are being evaluated. The first possible idea is using a commercial cloud application such as AWS, Azure, Google Cloud, etc. Secondly, personal local hardware could be used to create a virtual distributed computing environment. There are two options for local hardware. Either a personal computer with multiple virtual machines, or an array of Raspberry Pi’s will be used.&lt;/p&gt;
&lt;h3 id=&#34;novel-ideas&#34;&gt;Novel Ideas&lt;/h3&gt;
&lt;p&gt;Everything that has already been explained has more or less been attempted or implemented successfully. The innovation comes by trying to implement some ideas that are fresh by borrowing ideas and implementing them in the context of an indoor produce farm. The first big deviation from the norm is using a blockchain backbone to store immutable data. This idea is used in some niche farming scenarios but is yet to be adopted by produce farmers. Blockchain could be used to hold the logistical data to establish immutable custody data, but also to store the data from the growing process, pesticides tests, chemical makeup tests, genetic markers and more. Next, in the spirit of providing transparency there will be a public blockchain that could be explored by consumers or businesses that buy the farmers produce. In today’s world, we always wonder if we are paying some premium for products in order to just have a special label on that product. For this example, the label is: organic, GMO free, pesticide free, etc. Transparency goes a long way to prove to consumers that you are doing more for them to provide a good product, which allows for a greater amount that people are willing to spend. Some data might be proprietary, such as the exact genome of the phenotypes being used, or the specific growing protocols, so this information must stay off the blockchain.&lt;/p&gt;
&lt;h3 id=&#34;extensions&#34;&gt;Extensions&lt;/h3&gt;
&lt;p&gt;Not everything can be built or examined completely within the time constraints. Part of the project will be planning future updates or technologies that could improve the solution. One immediate future plan would be to incorporate cameras and computer vision to monitor the crops. Using images of plants, certain diseases, pests, or nutrient deficiencies can be seen as soon as they start to develop, giving the farmer the best odds at reversing the issue without effecting the harvest. Many of these issues cannot be greatly noticed with sensor data alone, which requires a farmer to constantly visually inspect crops. While this might not be terribly hard in some cases, some vertical grows might require large ladders to see all levels of the crop. This improvement could lead to less staff being required, which can allow more farmers to grow more for less money.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-305/project/project/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-305/project/project/</guid>
      <description>
        
        
        &lt;h1 id=&#34;estimating-soil-moisture-content-using-sensor-and-weather-data&#34;&gt;Estimating Soil Moisture Content Using Sensor and Weather Data&lt;/h1&gt;
&lt;p&gt;Cody Harris, &lt;a href=&#34;mailto:harrcody@iu.edu&#34;&gt;harrcody@iu.edu&lt;/a&gt;, &lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-305&#34;&gt;fa20-523-305&lt;/a&gt;, &lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-305/edit/master/project/project.md&#34;&gt;Edit&lt;/a&gt;&lt;/p&gt;


&lt;div class=&#34;pageinfo pageinfo-primary&#34;&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;As the world is gripped with finding solutions to problems such as food and water shortages, the study of agriculture could improve where we stand with both of these problems. By integrating weather and sensor data, a model could be created to estimate soil moisture based on on weather data. While some farmers could afford to have many moisture sensors and monitor them, many would not have the funds or resources to keep track of the soil moisture long term. A solution would be to allow farmers to contract out a limited study of their land using sensors and then this model would be able to predict soil moistures from weather data.&lt;/p&gt;
&lt;p&gt;Contents&lt;/p&gt;
&lt;div class=&#34;toc&#34;&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#topic-discussion&#34;&gt;Topic Discussion&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#datasets&#34;&gt;Datasets&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#data-cleaning-and-aggregation&#34;&gt;Data Cleaning and Aggregation&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#preliminary-analysis-and-eda&#34;&gt;Preliminary Analysis and EDA&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#plan-for-the-rest-of-the-semseter&#34;&gt;Plan for the rest of the Semseter&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#october-26&#34;&gt;October 26&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#november-2&#34;&gt;November 2&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#november-9&#34;&gt;November 9&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#november-16&#34;&gt;November 16&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Keywords:&lt;/strong&gt; agriculture, soil moisture, IoT&lt;/p&gt;
&lt;h2 id=&#34;topic-discussion&#34;&gt;Topic Discussion&lt;/h2&gt;
&lt;p&gt;Maintaining correct soil moisture throughout the plant growing process can result in better yeilds, and less overall problems with the crop. Water deficiencies or surplus at various stages of growth have different effects, or even negligable effects&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. It is important to have an idea of how your land consumes and stores water, which could be very different based on the plants being used, and variation of elevation and geography.&lt;/p&gt;
&lt;p&gt;For hundreds of years, farmers have done something similar to this model. The difference is the precision that we can gain by using real data. For the past few hundred years, farmers had to rely on mostly experience and touch to know the moisture of their soil. While many farmers were successful, in the sense that they produced crops, there were ways they could have better optimized their crops to produce better. The water avaliable to the plants is not the only variable that effects yeilds, but this project seeks to create an accessible model to which farmers can have predicted values of soil moisture without needing to buy and deploy expensive sensors.&lt;/p&gt;
&lt;p&gt;The model created could be used in various ways. The first main use is to be able to monitor what is currently happening in the soil so that changes can be made to correct the issue if there is one. Secondly, a farmer could evaluate historical data and compare it to yeilds or other results of the harvest and use this analytical information to inform future descions.&lt;/p&gt;
&lt;p&gt;This project specifically seeks to see the effect of weather on a particular piece of land in Washington state. This process could be done all over the world to obtain benchmarks. These benchmarks could be a cheap option for a farmer that does not have the funds to support a full study of water usage on their land. Instead, they could look for a model that has land that has similar soil and or geographical features, and then use their own weather data to estimate their soil moisture content.&lt;/p&gt;
&lt;h2 id=&#34;datasets&#34;&gt;Datasets&lt;/h2&gt;
&lt;p&gt;The first data set comes from NOAA and contains daily summary data in regards to various measurments such as temperature, percipitation, wind speed, etc. For this project, only data that came from the closest station to the field will be used &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. In this case, that is the Pullman station at the Pullman-Moscow airport. Below is an image showing the weather data collection location, and the red pin is at the longitude and lattitude of one of the sensors in the field. This data is in csv format (see Figure 1).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-305/master/project/resources/distance_map.png&#34; alt=&#34;Figure 1&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Caption missing&lt;/p&gt;
&lt;p&gt;The second dataset comes from the USDA. This dataset consits of &amp;ldquo;hourly and daily measurements of volumetric water content, soil temperature, and bulk electrical conductivity, collected at 42 monitoring locations and 5 depths (30, 60, 90, 120, and 150 cm)&amp;rdquo; at a farm in Washington state &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;. Mainly, the daily temperature and water conent are the measurements of interest. There are multiple files that have data that corresponds to what plants are being grown in specific places, and the make up of the soil at each sensor cite. This auxilary information could be used in later models once the base model has been completed. This data is in tab delimited files.&lt;/p&gt;
&lt;p&gt;Within the data, there are GIS file types that can be imported into Google Maps desktop to visualize the locations of the sensors and other geographical information. Below is an example of the sensor locations plotted on the sattelite image (see Figure 2).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-305/master/project/resources/sensor_locations.png&#34; alt=&#34;Figure 2&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 2:&lt;/strong&gt; Caption missing&lt;/p&gt;
&lt;h2 id=&#34;data-cleaning-and-aggregation&#34;&gt;Data Cleaning and Aggregation&lt;/h2&gt;
&lt;p&gt;The first step is to get the soil moisture data into a combined format, currently it is in one file per sensor, and there are 42 sensors. See the data_cleaning_aggregation.ipynb file to see how this was done. After aggregation, some basic information can be checked about the data. For instance, there is quite a bit of NAs in the data. These NAs are just instances where there was no measurement on that day. There is about 45% NAs in the measurement columns. To further clean the data, any row that has only NAs for the measurements will be removed.&lt;/p&gt;
&lt;p&gt;Next, the weather data needs some small adjustments. This is mostly in the form of removing columns that either are empty or have redundant data such as elevation.&lt;/p&gt;
&lt;p&gt;Once the data is sufficiently clean, some choices have to be made on joining the data. The simplist route would be to join the weather measurements directly with the same day the soil measurement, however, the previous days weather is likely to also have an impact on the moisture. In the same fashion, the weather for the 5 previous days might all have a large impact on the moisture. So simply joining the two data sets right now is likely not the correct course of action until further analysis is made.&lt;/p&gt;
&lt;h2 id=&#34;preliminary-analysis-and-eda&#34;&gt;Preliminary Analysis and EDA&lt;/h2&gt;
&lt;p&gt;Before building a machine learning model, it is important to get a general idea of how the data looks, to see if any insights can be made right away.&lt;/p&gt;
&lt;p&gt;The first two visualizations are grids that show the entire distribution of measurements across each sensor. The first grid is the volume of water at 30 cm, and the second grid is the water volume at 150 cm. Each chart could be looked at and examined on it&amp;rsquo;s own, but what is most important to note is the variability of the measures from location to location. These different sensors are not that far away, but show that different areas of the farm do retain water in different ways.&lt;/p&gt;
&lt;p&gt;The third and fourth grid shows the temperature at 150 cm, the results are what would logically be expected. The different sensors do not show much variance from location to location.&lt;/p&gt;
&lt;p&gt;A simple bar chart is used to get a quick overview of the percipitation values over the same time period to see the overall trends. The most interesting part of this analysis is from the end of 2009 to nearly 2012. There is very little percipitation in this time period. Which initially looks like an issue with the data, but when it is compared to the water volume charts, a correlation can be seen. It isn&amp;rsquo;t perfect, but in many of the sensors that have data in this period, the moisture seems rather constant.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: After doing this analysis using the Altair library in python, the notebook became way too big due to the size of the data. As a quick remedy for this, I saved the visualizations as PNG and saved them in the resources folder. They are named: one, two, three, and four, for the order that they are mentioned in the above section. I will remove the eda notebook from the repo.&lt;/p&gt;
&lt;h2 id=&#34;plan-for-the-rest-of-the-semseter&#34;&gt;Plan for the rest of the Semseter&lt;/h2&gt;
&lt;p&gt;The following is a plan for the rest of the semester, using the due dates for Assignments 8-11 as milestone dates&lt;/p&gt;
&lt;h3 id=&#34;october-26&#34;&gt;October 26&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Further EDA (make charts more presentable)&lt;/li&gt;
&lt;li&gt;Create multiple data sets that allow for the training data to include the past N days of weather&lt;/li&gt;
&lt;li&gt;Brainstorm ideas for feature engineering and build features&lt;/li&gt;
&lt;li&gt;Test various ML methods using scikit-learn to get an early idea of possible models, and to have a baseline for comparison&lt;/li&gt;
&lt;li&gt;Update report&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;november-2&#34;&gt;November 2&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Tune hyperparameters for selected ML model&lt;/li&gt;
&lt;li&gt;Evalutate deep learning frameworks (Keras, PyTorch, scikit-learn also has NN functionalities)&lt;/li&gt;
&lt;li&gt;Try to implement simple deep learning model&lt;/li&gt;
&lt;li&gt;Update report&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;november-9&#34;&gt;November 9&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Further tune and tweak models&lt;/li&gt;
&lt;li&gt;Start analyzing various models and techniques to find the most accurate model&lt;/li&gt;
&lt;li&gt;Update report&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;november-16&#34;&gt;November 16&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Build a library/program that can pull weather data from NOAA and takes sensor input and outputs predictions of soil moisture by area&lt;/li&gt;
&lt;li&gt;Finalize report and findings&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;O. Denmead and R. Shaw, &amp;ldquo;The Effects of Soil Moisture Stress at Different Stages of Growth on the Development and Yield of Corn 1&amp;rdquo;, Agronomy Journal, vol. 52, no. 5, pp. 272-274, 1960. Available: 10.2134/agronj1960.00021962005200050010x. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;N. (NCEI), &amp;ldquo;Climate Data Online (CDO) - The National Climatic Data Center&amp;rsquo;s (NCDC) Climate Data Online (CDO) provides free access to NCDC&amp;rsquo;s archive of historical weather and climate data in addition to station history information. | National Climatic Data Center (NCDC)&amp;rdquo;, Ncdc.noaa.gov, 2020. [Online]. Available: &lt;a href=&#34;https://www.ncdc.noaa.gov/cdo-web/&#34;&gt;https://www.ncdc.noaa.gov/cdo-web/&lt;/a&gt;. [Accessed: 19- Oct- 2020]. &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&amp;ldquo;Data from: A field-scale sensor network data set for monitoring and modeling the spatial and temporal variation of soil moisture in a dryland agricultural field&amp;rdquo;, USDA: Ag Data Commons, 2020. [Online]. Available: &lt;a href=&#34;https://data.nal.usda.gov/dataset/data-field-scale-sensor-network-data-set-monitoring-and-modeling-spatial-and-temporal-variation-soil-moisture-dryland-agricultural-field&#34;&gt;https://data.nal.usda.gov/dataset/data-field-scale-sensor-network-data-set-monitoring-and-modeling-spatial-and-temporal-variation-soil-moisture-dryland-agricultural-field&lt;/a&gt;. [Accessed: 19- Oct- 2020]. &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-305/test/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-305/test/</guid>
      <description>
        
        
        &lt;p&gt;Testing if I have write access to this repo.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-307/assignment6/assignment6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-307/assignment6/assignment6/</guid>
      <description>
        
        
        &lt;h1 id=&#34;ai-in-precision-medicine&#34;&gt;AI in Precision Medicine&lt;/h1&gt;
&lt;p&gt;In recent years, precision medicine has started to become the new standard when it comes to healthcare. This is moving us from a one size fits all approach to a more personal, data-driven approach that allows hospitals and treatment centers to spend more efficiently and have a higher patient outcome. Precision medicine is using knowledge that is specific to one patient, such as biomarkers, rather than the generic approach to an issue. The overall goal is to &amp;quot;design and optimize the pathway for diagnosis and prognosis through the use of large multidimensional biological datasets that capture different variables such as genes&amp;quot; [1].&lt;/p&gt;
&lt;p&gt;Artificial intelligence (AI) has been increasingly growing in business, society and now is emerging in healthcare. The potential that AI has can completely transform patient care. These technologies can perform to or exceed human capability when it comes to different medical tasks such as cancer diagnosis or disease diagnosis as well as patient engagement and administration tasks. AI has the potential to offer automated care to individuals by providing precision medicine.&lt;/p&gt;
&lt;p&gt;Precision medicine enables patients to not only recover from illnesses faster but to also stay healthy longer. However, with the increased use of precision medicine new challenges arise such as the increasing amount of data, a lack of specialists and ever increasing drug development costs. &amp;quot;Healthcare data is projected to grow by 43 percent by 2020, to roughly 2.3 zettabytes. The size of the data is not the only problem; it&#39;s the kind of data as well. Eighty percent of it is unstructured and mostly unlabeled, making it hard to extract value from the datasets&amp;quot; [2].&lt;/p&gt;
&lt;p&gt;Artificial intelligence (AI) has helped reshape how precision medicine is distributed. AI is able to solve many of the problems that have arisen. For big data challenges, AI methods are able to clear up obstacles that large and unstructured data present. In medical imaging, machine learning can be introduced to help classify what type of issue is present by training a model over thousands of images and predicting on the patient&#39;s image. Neural networks have also been able to make predictions when it comes to precision medicine.&lt;/p&gt;
&lt;p&gt;Neural networks are a more advanced form of AI. The uses in precision medicine is for categorisation applications such as the likelihood of a patient developing a disease. Neural networks look at problems from inputs, outputs, and weights of features to try and associate inputs with the corresponding outputs. &amp;quot;It has been likened to the way that neurons process signals, but the analogy to the brain&#39;s function is relatively weak&amp;quot; [3].&lt;/p&gt;
&lt;p&gt;Deep learning is one of the most complex forms of AI. This involves hundreds or thousands of models with numerous levels of features that are needed to predict the outcomes. Precision medicine takes advantage of this technology through the &amp;quot;recognition of potentially cancerous lesions in radiology images&amp;quot; [4]. Deep learning is able to be applied to fields such as radiomics. This is the practice of detecting features in image data that cannot be detected with the human eye. &amp;quot;Their combination appears to promise greater accuracy in diagnosis than the previous generation of automated tools for image analysis, known as computer-aided detection or CAD&amp;quot; [4].&lt;/p&gt;
&lt;p&gt;AI plays a pivotal role in the future of healthcare. In the development of precision medicine, it is one of the primary components in order to advance care for patients. Efforts to help classify medical imagery more quickly and accurately have proven more effective with the amount of data used to train such models. A big challenge that AI is facing in precision medicine is whether or not this technology will be widely adopted. These systems will need to have some regulations in order to have a universal standard. This will allow doctors and medical personnel to train with this technology so they will be able to provide the care their patients deserve. AI will never replace the human aspect of precision medicine but over time AI will be able to make the jobs and lives of the doctors and patients better and healthier.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] M. Uddin, Y. Wang, and M. Woodbury-Smith, &amp;quot;Artificial intelligence for precision medicine in neurodevelopmental disorders,&amp;quot; &lt;em&gt;Nature News&lt;/em&gt;, 21-Nov-2019. [Online]. Available: &lt;a href=&#34;https://www.nature.com/articles/s41746-019-0191-0&#34;&gt;https://www.nature.com/articles/s41746-019-0191-0&lt;/a&gt;. [Accessed: 11-Oct-2020].&lt;/p&gt;
&lt;p&gt;[2] H. Chamraj, &amp;quot;Powering Precision Medicine with Artificial Intelligence,&amp;quot; &lt;em&gt;Intel&lt;/em&gt;. [Online]. Available: &lt;a href=&#34;https://www.intel.com/content/www/us/en/artificial-intelligence/posts/powering-precision-medicine-artificial-intelligence.html&#34;&gt;https://www.intel.com/content/www/us/en/artificial-intelligence/posts/powering-precision-medicine-artificial-intelligence.html&lt;/a&gt;. [Accessed: 12-Oct-2020].&lt;/p&gt;
&lt;p&gt;[3] T. Davenport and R. Kalakota, &amp;quot;The potential for artificial intelligence in healthcare,&amp;quot; &lt;em&gt;Future healthcare journal&lt;/em&gt;, Jun-2019. [Online]. Available: &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6616181/&#34;&gt;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6616181/&lt;/a&gt;. [Accessed: 12-Oct-2020].&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-307/project/project/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-307/project/project/</guid>
      <description>
        
        
        &lt;h1 id=&#34;analysis-of-financial-markets-based-on-president-trumps-tweets&#34;&gt;Analysis of Financial Markets based on President Trump&amp;rsquo;s Tweets&lt;/h1&gt;
&lt;p&gt;Alex Baker, fa20-523-307, &lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-307/blob/master/project/project.md&#34;&gt;Edit&lt;/a&gt;&lt;/p&gt;


&lt;div class=&#34;pageinfo pageinfo-primary&#34;&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Financial markets can be unpredictable as is but this unpredictability is increased by one man&amp;rsquo;s Twitter account, President Trump. My goal is to use Twitter and finance datasets to see how these tweets affect the market.&lt;/p&gt;
&lt;p&gt;Contents&lt;/p&gt;
&lt;div class=&#34;toc&#34;&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#1-introduction&#34;&gt;1. Introduction&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#2-datasets&#34;&gt;2. DataSets&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#3-methodologyprocess&#34;&gt;3. Methodology/Process&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#outline-of-plan&#34;&gt;Outline of plan&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#4-technologies-used&#34;&gt;4. Technologies used&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#5-references&#34;&gt;5. References&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Keywords:&lt;/strong&gt; analysis, finance, stock markets, politics&lt;/p&gt;
&lt;h2 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;For the final project, my focus will be on financial market reactions through the President&amp;rsquo;s tweets. The plan is to utilize President Trump&amp;rsquo;s tweets and stock market data to predict the market reaction based on what is going to be published. A feature that is being introduced is a way to craft tweets based on historical data to see how the markets will react if a tweet such as that is published. This can be useful to see how news from the president can cause an increase or decline in markets.&lt;/p&gt;
&lt;h2 id=&#34;2-datasets&#34;&gt;2. DataSets&lt;/h2&gt;
&lt;p&gt;The datasets that will be used are the tweets from President Trump&amp;rsquo;s personal account as well as Yahoo fiance data. These will be gathered from their respected APIs. If needed, the following dataset from Kaggle (&lt;a href=&#34;https://www.kaggle.com/austinreese/trump-tweets?select=trumptweets.csv&#34;&gt;https://www.kaggle.com/austinreese/trump-tweets?select=trumptweets.csv&lt;/a&gt;) can be used in replace of Twitter&amp;rsquo;s API for President Trump&amp;rsquo;s tweets but are only available up to June 2020. Which leads to the objective for the project, based on the data collected, the program should be able to visualize and predict how the market will react when President Trump send out a tweet.&lt;/p&gt;
&lt;p&gt;The data will span from President Trumps inauguration to the current day. To strengthen the prediction, even more, some code from the 2016 election’s analysis of markets may be utilized but the focus will be on the markets during the Trump administration. Rally data maybe introduced in order to have a deeper sense of some of the tweets when it comes to important news that is announces at President Trump&amp;rsquo;s rallies. In order to have a realistic and strong prediction, the financial data needs to be aligned with the timing of tweets but news that has already started to affect the markets before a tweet has been sent out needs to be taken into account.&lt;/p&gt;
&lt;h2 id=&#34;3-methodologyprocess&#34;&gt;3. Methodology/Process&lt;/h2&gt;
&lt;p&gt;The collection of finance and Twitter data will be used to visualize and predict the results. Some of Twitter or dataset data will need to be cleaned and classified to build the model. The methodology is composed of the following steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use data from President Trump&amp;rsquo;s personal twitter to help visualize and create the model&lt;/li&gt;
&lt;li&gt;Use data from Yahoo finance API to help visualize and create the model&lt;/li&gt;
&lt;li&gt;Data cleaning and extraction.&lt;/li&gt;
&lt;li&gt;New data will be updated to keep up with the current time.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;outline-of-plan&#34;&gt;Outline of plan&lt;/h2&gt;
&lt;p&gt;In the next seven weeks, these are the tasks that need to be accomplished.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Week 9 - Week 10: Clean and preprocess the data needed for the project&lt;/li&gt;
&lt;li&gt;Week 10 - Week 11: Research what methods that should be used&lt;/li&gt;
&lt;li&gt;Week 11 - Week 13: Preform sentiment analysis to find features and see if any correlation exists&lt;/li&gt;
&lt;li&gt;Week 13 - Week 16: Build out NLP model&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;4-technologies-used&#34;&gt;4. Technologies used&lt;/h2&gt;
&lt;p&gt;Python, Jupyter notebook or collab, Pandas, Scikit-learn, Tensorflow/PyTorch&lt;/p&gt;
&lt;h2 id=&#34;5-references&#34;&gt;5. References&lt;/h2&gt;
&lt;p&gt;TBD&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-308/hw7/task_3_next_steps/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-308/hw7/task_3_next_steps/</guid>
      <description>
        
        
        &lt;h1 id=&#34;next-steps&#34;&gt;Next Steps&lt;/h1&gt;
&lt;p&gt;I am still not 100% that this is the project I want to complete. As the instructions for HW 5 laid out, we do not have to fully commit at this point to the project. I may try to work on a basic deep learning project that can introduce me to that type of work. Next steps for the project I have started here would be to complete the basic descriptive data modeling in charts. Then would be to model the data and pull in the playoff teams from 2009-2019 to compare the model output with the playoff team that qualified for the playoffs. I want to work on this over the next month before moving onto the writing portion of the project.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-308/project/project/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-308/project/project/</guid>
      <description>
        
        
        &lt;h1 id=&#34;nfl-regular-season-skilled-position-player-performance-as-a-predictor-of-playoff-appearance&#34;&gt;NFL Regular Season Skilled Position Player Performance as a Predictor of Playoff Appearance&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; please use pageinfo compare template carefully&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; do not use sup&lt;/li&gt;
&lt;li&gt;[I have attempted to match the markdwon of the template ] improve markdown skills&lt;/li&gt;
&lt;li&gt;[again attempted to match the template ] use footnotes as refernces, learn how to do this, read our piaza posts&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Travis Whitaker &lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-308&#34;&gt;fa20-523-308&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;(To be written)&lt;/p&gt;
&lt;p&gt;Contents
&lt;div class=&#34;toc&#34;&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#abstract&#34;&gt;Abstract&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#1-introduction&#34;&gt;1. Introduction&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#2-background-research-and-previous-work&#34;&gt;2. Background Research and Previous Work&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#3-choice-of-data-sets&#34;&gt;3. Choice of Data-sets&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#4-methodology&#34;&gt;4. Methodology&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#5-inference&#34;&gt;5. Inference&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#6-conclusion&#34;&gt;6. Conclusion&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#7-acknowledgements&#34;&gt;7. Acknowledgements&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#8-references&#34;&gt;8. References&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/div&gt;&lt;/p&gt;
&lt;h2 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;(Content to be entered from proposal and additional material written)&lt;/p&gt;
&lt;h2 id=&#34;2-background-research-and-previous-work&#34;&gt;2. Background Research and Previous Work&lt;/h2&gt;
&lt;p&gt;&amp;hellip;&lt;/p&gt;
&lt;h2 id=&#34;3-choice-of-data-sets&#34;&gt;3. Choice of Data-sets&lt;/h2&gt;
&lt;p&gt;The dataset we will be using is in a github folder that holds nflscrapR-data that originates from NFL.com [&lt;sup&gt;1&lt;/sup&gt;]. The folder includes play-by-play data, including performance measures, for all regular season games from 2009 to 2019. This file will be paired with week-by-week regular season roster data for each team in the NFL. This will allow me to track skilled position player performance during the regular season and then compare this regular season file with the files that contain playoff teams for each year from 2009-2019. Supplemental data may be pulled from Pro-Football-Reference.com or other sources depending on what preliminary data analysis presents [&lt;sup&gt;2&lt;/sup&gt;].&lt;/p&gt;
&lt;h2 id=&#34;4-methodology&#34;&gt;4. Methodology&lt;/h2&gt;
&lt;p&gt;The first step we am planning to take in understanding the data will be to use various slices of the data put into scatterplots and bar charts to find correlations and trends, as well as various time series charts. This will be an exploratory step in understanding the data. we may also deploy area charts to observe any interesting trends or segments of our data that may warrant additional analysis.&lt;/p&gt;
&lt;p&gt;Then each metric from player performance during the regular season will be included in the analysis or algorithm that will be built to predict playoff appearance. Playoff appearance is a designation for a team qualifying for the post-season or playoffs. We am thinking it may be important to engineer some new features to potentially provide insights.  For instance, it is possible to determine whether a play was during the final two minutes of a half and if a play was in the red zone. During these critical points of a game a win or lose is often determined. So my thought is by weighing these moments and performance metrics with more importance in an algorithm the machine will better predict a team’s likelihood of making the playoffs. Another secondary metric that may strengthen the predictive ability of the algorithm would be to use Football Outsider’s Success Rate, which is a determination of a play’s success rate for the offense that is on the field [2]. This can also provide me with the down and distance to go for the offense and players that are on the field. We will also use college position designations as way to normalize the positions performance across teams. Many NFL teams utilize different player sets. Thus, it is important to use a standard, which college football uses across all teams. Since we am only interested in skill position players this will include Wide Receiver (WR), Running Back (RB), Full Back (FB), Quarterback (QB), and Tight End (TE). These designations will allow the algorithm to compare, if needed, the skill position performance across teams and by designation of players on that team for each metric utilized.&lt;/p&gt;
&lt;p&gt;After breaking down the data into key categorical variables to see if there was an impact for these performance variables in making the playoffs for the NFL teams. These individual position statistics will need to be paired with their teammate’s performance metrics so that each team is represented by all of their skill position players. It is often debated as to which position is most important in football and whether a QB is critical to a successful post-season appearance. My hope is that by combining each skill position player onto their respective team we will be able to better determine the importance of success at each position by whether or not that team made the playoffs. Further, it will be important to see whether roster makeup across teams varies by position. If roster makeup is stable than we will not need to combining positions. However, if roster makeup varies, we will need to combine positions into a larger group instead of two subgroups. The concern here is the deployment of WR over TE. Some teams may carry more TE than others and fewer WR. Therefor we would need to combine WR and TE into a group called Designated Receiver (DR).&lt;/p&gt;
&lt;p&gt;Metric measurement needs to be consistent across years. A comparison of year-to-year metrics will need to be done comparing each years measurements from 2009-2019 in order to make sure that the measurement techniques are stable and do not vary across time. If there are changes in the way metrics are measured than either that year will need to be dropped from the model or adjustments will need to be made to the metric to balance it with the other years included in the model.&lt;/p&gt;
&lt;p&gt;Finally, once all metrics have been balanced and the team performance metrics have been aggregated. The algorithm will need to be implemented to identify the eight best teams from each NFL division and the two best other teams in the conference that would constitute the wild card playoff teams from each conference. Once this analysis is run, we will be able to look at retroactive NFL post-season designated teams from 2009-2019 to see how accurate the playoff prediction machine was at identifying NFL post-season teams for each year.&lt;/p&gt;
&lt;h2 id=&#34;5-inference&#34;&gt;5. Inference&lt;/h2&gt;
&lt;p&gt;This section will be addressed upon project completion.&lt;/p&gt;
&lt;h2 id=&#34;6-conclusion&#34;&gt;6. Conclusion&lt;/h2&gt;
&lt;p&gt;This section will be addressed upon project completion.&lt;/p&gt;
&lt;h2 id=&#34;7-acknowledgements&#34;&gt;7. Acknowledgements&lt;/h2&gt;
&lt;h2 id=&#34;8-references&#34;&gt;8. References&lt;/h2&gt;
&lt;p&gt;[&lt;sup&gt;1&lt;/sup&gt;] Ryurko. Ryurko/NflscrapR-Data. 2 Mar. 2020, github.com/ryurko/nflscrapR-data.&lt;/p&gt;
&lt;p&gt;[&lt;sup&gt;2&lt;/sup&gt;] Sports Reference, LLC. “Pro Football Statistics and History.”  Retrieved October 09, 2020. &lt;a href=&#34;https://www.pro-football-reference.com/&#34;&gt;https://www.pro-football-reference.com/&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-309/project/project/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-309/project/project/</guid>
      <description>
        
        
        &lt;h1 id=&#34;detecting-heart-disease-using-machine-learning-classification-techniques&#34;&gt;Detecting Heart Disease using Machine Learning Classification Techniques&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; please follow our template&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ethan Nguyen&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Since cardiovascular diseases are the number 1 cause of death globally, early prevention could help in extending one’s life span and possibly quality of life. Since there are cases where patients do not show any signs of cardiovascular trouble until an event occurs, having an algorithm predict from their medical history would help in picking up on early warning signs a physician may overlook. Or could also reveal additional risk factors and patterns for research on prevention and treatment.&lt;/p&gt;
&lt;p&gt;It has been decided for this project to take a high-level overview of the common, widely available classification algorithms and analyze their effectiveness for this specific use case. Notable ones include, Gaussian Naive Bayes, Logistic Regression, K-Nearest Neighbors, and Support Vector Machines.&lt;/p&gt;
&lt;p&gt;Additionally, a variety of data sets that contain common information types will be used to increase the training and test pool for evaluation. As it is known that a large set of data is required to reduce the possibility of the algorithm’s overfitting.&lt;/p&gt;
&lt;h2 id=&#34;dataset&#34;&gt;Dataset&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/andrewmvd/heart-failure-clinical-data&#34;&gt;https://www.kaggle.com/andrewmvd/heart-failure-clinical-data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/johnsmith88/heart-disease-dataset&#34;&gt;https://www.kaggle.com/johnsmith88/heart-disease-dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/ronitf/heart-disease-uci&#34;&gt;https://www.kaggle.com/ronitf/heart-disease-uci&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The range of creation dates vary from as late as 2 years to as recent of 4 months. This does bring up a small hiccup in preprocessing to consider. Namely the possibility of changing diet and culture trends resulting in significantly different trends/patterns within the same age group.&lt;/p&gt;
&lt;p&gt;This possible phenomenon may be of interest to explore closely if time allows. Whether a trend itself is even present or there is an overarching trend across different cultures and time periods. Or to consider if this difference is significant enough that the data from the various sets needs to be adjusted to normalize the ages to present day.&lt;/p&gt;
&lt;h2 id=&#34;what-needs-to-be-done-to-achieve-a-great-grade&#34;&gt;What needs to be done to achieve a great grade&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;A report analyzing the results and outlining the procedure of the project.&lt;/li&gt;
&lt;li&gt;The report satisfies the minimum length requirement and complies with the additional requirements outlined in the project pdf&lt;/li&gt;
&lt;li&gt;Searching and curating a large data set on heart disease&lt;/li&gt;
&lt;li&gt;A software component that preprocesses and trains various machine learning algorithms on the chosen dataset&lt;/li&gt;
&lt;li&gt;The trained algorithms are tuned to a reasonable degree to obtain the maximum performance possible within the project timeframe&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;strech-goals-for-the-project&#34;&gt;Strech goals for the project&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Further analysis of the datasets to see if the age between the creation/release date reveal any shifts over time&lt;/li&gt;
&lt;li&gt;What occurs when the data set is normalized based on creation/release date to the newest set and if they reveal any other interesting trends&lt;/li&gt;
&lt;/ol&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-312/assignment6/assignment6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-312/assignment6/assignment6/</guid>
      <description>
        
        
        &lt;h1 id=&#34;engr-e-534-assignment-6-ai-in-health-and-medicine&#34;&gt;ENGR-E 534 Assignment 6: AI in Health and Medicine&lt;/h1&gt;
&lt;h1 id=&#34;ai-enabled-covid-19-diagnostic-framework-utilizing-smartphone-based-embedded-sensors&#34;&gt;AI-enabled COVID-19 diagnostic framework utilizing Smartphone-based Embedded Sensors&lt;/h1&gt;
&lt;p&gt;Saptarshi Sinha, &lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-312/&#34;&gt;fa20-523-312&lt;/a&gt;, &lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-312/blob/master/project/project.md&#34;&gt;Edit&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Keywords:&lt;/strong&gt; smartphone, neural networks, CNN, RNN, embedded sensors, symptom detection, cloud computing&lt;/p&gt;
&lt;h2 id=&#34;1-background-the-need-for-smarter-and-more-pervasive-covid-19-monitoring&#34;&gt;1. Background: The need for smarter and more pervasive COVID-19 monitoring&lt;/h2&gt;
&lt;p&gt;As mankind grapples with the menacing threat of an ongoing pandemic involving the novel COVID-19 (coronavirus infection), researchers and clinicians across the board have tirelessly involved themselves in myriad efforts for controlling the relentless proliferation of this virus so as to check the viral-driven casualties across the globe. It might seem that for the very first time, science and technology have been put to its greatest test ever. It seems that only time can tell if our scientific valor is indeed powerful enough to succeed in such a test, or if the virus would instead claim a major portion of the world’s population as its unfortunate casualty.&lt;/p&gt;
&lt;p&gt;Numerous scientific approaches have been fielded in a relatively short amount of time to deal with the current problem. Many approaches involve novel technologies such as remote video surveillance using assistive robots that monitor virus-inflicted patients, while also protecting those healthcare workers by not involving them in such in-person diagnostic processes. Other approaches involve using machine learning based methodologies for sorting out patents with the virus from those without it simply by using an efficient algorithmic procedure of analyzing different aspects of patients’ CT scans. Major companies have also stepped in to assist in a war-footing format. As an example, Amazon Care is providing pick-up and delivery-based services of test-kits in particular virus-prone locations. Apple’s Siri is now able to provide symptom-based guidance in relation to COVID-19. Microsoft helped creating the Adaptive Biotechnologies platform that studies how our immune system responds to the virus which can provide insights for establishing drug development procedures. Finally, various biotechnological companies all across the world have started conducting extensive research into vaccine development and drug development procedures to combat this novel strain of the virus.&lt;/p&gt;
&lt;p&gt;As amazing these techniques might seem at a superficial glance, the major setback the world is suffering from is with the extent of the viral spread that is amplified due to the lack of testing capabilities. They are either inadequate or cannot handle an entire nation’s population. Although proactive actions have been employed in many nations, testing kits are still being produced slowly. This gives the virus an unfair advantage as time is of the essence. People (esp. asymptomatic individuals) with the virus remain undiagnosed for a greater length of time during which they can inadvertently aid with the proliferation of the viral disease. In this particular context, a very novel strategy for COVID-19 testing and diagnosis will be discussed that utilizes something that we all possess – a smartphone device.&lt;/p&gt;
&lt;h2 id=&#34;2-design--working-principle-ai-based-diagnostic-framework-for-covid-19-utilizing-smartphone-based-embedded-sensors-and-artificial-neural-networks&#34;&gt;2. Design &amp;amp; Working Principle: AI-based diagnostic framework for COVID-19 utilizing Smartphone-based Embedded Sensors and Artificial Neural Networks&lt;/h2&gt;
&lt;p&gt;Cornell University’s archive on Human Computer Interaction (HCI) features a recent article that discusses a strategy involving COVID-19 diagnosis with smartphone-based sensors. In its simplest form, the framework includes the smartphone, and its accompanying sensors and algorithms. External hardware accessories with high-power consumption, or access to specialized equipment is not required for this design &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. Since the application framework involves something that common people use on a daily basis, no tutorials or expert assistance is required to work with such an application. To understand the framework better, we must first note the various symptom types that are exhibited by COVID-19 patients which include high fever, tiredness, dry cough, intense headache, shortness of breath, nausea, etc. To efficiently capture the symptoms, an essential piece of information to keep in mind here is that modern smartphone devices come equipped with various in-built sensors viz. camera sensor, inertial sensor, temperature sensor, accelerometer sensor, microphones, etc. Many previous endeavors utilized such sensors to detect symptoms for other diseases &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. For instance, temperature-fingerprint sensor was used previously for measuring fever-levels; camera sensors (with accelerometers) were utilized earlier to analyze fatigue levels via pattern-recognition algorithms for human-gait analysis; camera sensor (with inertial sensor) were also used for analyzing neck posture to evaluate the headache severities; and, even the microphone was utilized previously for analyzing a patient’s cough-noise in a diagnostic process &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;The research article describes a strategy which uses these various smartphone sensors and their respective algorithms. This is followed up by creating a dataset record comprising predicted levels of the different symptoms which are collected from different patients and studied using deep learning approaches &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. Chiefly, it uses Convolutional Neural Networks (CNN) to analyze spatial data (viz. imaging data from the camera sensor), and Recurrent Neural Networks (RNN) for temporal data (viz. signal or text-based measurements) &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. The entire prediction-based framework can be summarized as follows:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;images/Picture1.png&#34; alt=&#34;Smartphone-based framework for COVID-19 testing&#34;&gt;&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;path_to_image&#34; alt&gt;
    &lt;em&gt;Figure 1: Smartphone-based framework for COVID-19 testing; Source: Adapted from [^1]&lt;/em&gt;
&lt;/p&gt;
&lt;p&gt;The above framework can be sub-divided into four important layers which provides further insights into the different procedures going on in the background while the system makes the disease predictions.&lt;/p&gt;
&lt;h3 id=&#34;i-reading&#34;&gt;i. Reading&lt;/h3&gt;
&lt;p&gt;The first layer involves reading based functionalities for the data coming from different smartphone sensors. This could refer to arrays of different types of data coming from different sources (viz. CT scan imageries, accelerometer readings, microphone sound signals, etc.).&lt;/p&gt;
&lt;h3 id=&#34;ii-configurations&#34;&gt;ii. Configurations&lt;/h3&gt;
&lt;p&gt;The second layer deals with configuring onboard sensors for varied metrics such as time intervals, image resolution, etc. Readings from these first two steps are fed as inputs for the “symptoms algorithm” that can be executed as a smartphone application.&lt;/p&gt;
&lt;h3 id=&#34;iii-symptoms-prediction&#34;&gt;iii. Symptoms Prediction&lt;/h3&gt;
&lt;p&gt;The third layer deals with symptoms-level evaluation. The result is stored as a record that can be fed as an input for the next layer.&lt;/p&gt;
&lt;h3 id=&#34;iv-covid-19-prediction&#34;&gt;iv. COVID-19 Prediction&lt;/h3&gt;
&lt;p&gt;Finally, the last layer involves the application of deep learning (DL) based algorithms to the input data for predicting whether the patient has been afflicted with the virus. A CNN and RNN based combined process is utilized here such that the system can analyze both the spatial data (viz. image pixels) as well as the temporal data (viz. text/signal information) [1].&lt;/p&gt;
&lt;h2 id=&#34;3-discussions-augmentation-with-cloud-comuputing-capabilities&#34;&gt;3. Discussions: Augmentation with Cloud-Comuputing capabilities&lt;/h2&gt;
&lt;p&gt;To enhance the performance of this framework, the recorded data and predicted results can be uploaded to cloud-computing servers. This can help researchers and medical professionals from all around the globe in exchanging information and insights involving accurate patient diagnosis. Such applications are already developing. For instance, IBM recently launched the COVID-19 High Performance Computing Consortium &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. As the name suggests, this consortium has been explicitly designed to tackle the threat of COVID-19 by harnessing enormous computing power for streamlining the search for more information, aiding the hunt of possible treatment paths, and creating drug-and-disease based informational repositories that are made available to appropriate and eligible researchers and institutions strewn all across the globe &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;All in all, it is indeed very commendable on the part of these researchers to facilitate the design of such a low-cost yet effective method of diagnosing COVID-19 when testing capacities are severely limited. If used appropriately, it can stem the spread of this virus by making it possible to diagnose patients sooner and quarantining them. Of course, the strategy does not focus on the treatment itself. But in the current scenario, where we have arrived at a breaking point with this disease, it would greatly assist healthcare personnel with locating and quarantining patients, that would indirectly help saving scores of other lives.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References:&lt;/h2&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Maghdid, Halgurd S., et al. “A Novel AI-Enabled Framework to Diagnose Coronavirus COVID 19 Using Smartphone Embedded Sensors: Design Study.” ArXiv:2003.07434 [Cs, q-Bio], May 2020. arXiv.org, &lt;a href=&#34;http://arxiv.org/abs/2003.07434&#34;&gt;http://arxiv.org/abs/2003.07434&lt;/a&gt; &lt;/br&gt; &lt;/br&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;D. Gil, “IBM Releases Novel AI-Powered Technologies to Help Health and Research Community Accelerate the Discovery of Medical Insights and Treatments for COVID-19”, ibm.com, Apr. 3, 2020. [Online]. Available: &lt;a href=&#34;https://www.ibm.com/blogs/research/2020/04/ai-powered-technologies-accelerate-discovery-covid-19/&#34;&gt;https://www.ibm.com/blogs/research/2020/04/ai-powered-technologies-accelerate-discovery-covid-19/&lt;/a&gt; [Accessed Oct. 17, 2020] &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-312/project/project/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-312/project/project/</guid>
      <description>
        
        
        &lt;h1 id=&#34;aquatic-toxicity-analysis-with-the-aid-of-autonomous-surface-vehicle-asv&#34;&gt;Aquatic Toxicity Analysis with the aid of Autonomous Surface Vehicle (ASV)&lt;/h1&gt;
&lt;p&gt;Saptarshi Sinha, &lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-312/&#34;&gt;fa20-523-312&lt;/a&gt;, &lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-312/blob/master/project/project.md&#34;&gt;Edit&lt;/a&gt;&lt;/p&gt;


&lt;div class=&#34;pageinfo pageinfo-primary&#34;&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;With the passage of time, human activities have created and contributed much to the aggrandizing problems of various forms of environmental pollution. Massive amounts of industrial effluents and agricultural waste wash-offs, that often comprise pesticides and other forms of agricultural chemicals, find their way to fresh water bodies, to lakes, and eventually to the oceanic systems. Such events start producing a gradual increase in the toxicity levels of marine ecosystems thereby perturbing the natural balance of such water-bodies. In this endeavor, an attempt will be made to measure the various water quality metrics (viz. temperature, pH, dissolved-oxygen level, and conductivity) with the help of an autonomous surface vehicle (ASV). This collected data will then be analyzed to ascertain if these values exhibit aberration from the established values that are found from USGS and EPA databases for water-quality standards. In the event, the collected data significantly deviates from the standard values of unpolluted sources in nearby geographical areas, that are obtained from the above databases, it can be concluded that the aquatic system in question has been degraded and may no longer be utilized for any form of human usage, such as being sourced for drinking water.&lt;/p&gt;
&lt;p&gt;Contents&lt;/p&gt;
&lt;div class=&#34;toc&#34;&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#1-introduction&#34;&gt;1. Introduction&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#2-background-research-and-previous-work&#34;&gt;2. Background Research and Previous Work&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#3-choice-of-data-sets&#34;&gt;3. Choice of Data-sets&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#4-methodology&#34;&gt;4. Methodology&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#41-hardware-component&#34;&gt;4.1 Hardware Component&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#42-software-component&#34;&gt;4.2 Software Component&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#5-inference&#34;&gt;5. Inference&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#6-conclusion&#34;&gt;6. Conclusion&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#7-acknowledgements&#34;&gt;7. Acknowledgements&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#8-references&#34;&gt;8. References&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Keywords:&lt;/strong&gt; toxicology, pollution, autonomous systems, surface vehicle, sensors, arduino, water quality, data analysis, environment, big data, ecosystem&lt;/p&gt;
&lt;h2 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;When it comes to revolutionizing our qualities of life and improving standards, there is not another branch of science and technology that has made more impact than the myriad technological capabilities offered by the areas of Artificial Intelligence (AI) and its sub-fields involving Computer Vision, Robotics, Machine Learning, Deep Learning, Reinforcement Learning, etc. It should be borne in mind that AI was developed to allow machines/computer processors to work in the same way as the human brain works and which could make intelligent decisions at every conscious level. It was meant to help with tasks for rendering scientific applications more smarter and efficient. There are many tasks that can be performed in a far more dexterous fashion by employing smart-machines and algorithms than by involving human beings. But even more importantly, AI has also been designed to perform tasks that cannot be successfully completed by employing human beings. This could either be due to the prolonged boredom of the task itself, or a task that involves hazardous environments that cannot sustain life-forms for a long time. Some examples in this regard would involve exploring deep mines or volcanic trenches for mineral deposits, exploring the vast expanse of the universe and heavenly bodies, etc. And this is where the concept employing AI/Robotics based technology fits in perfectly for aquatic monitoring and oceanographical surveillance based applications.&lt;/p&gt;
&lt;p&gt;Toxicity analysis of ecologically vulnerable water-bodies, or any other marine ecosystem for that matter, could give us a treasure trove of information regarding biodiversity, mineral deposits, unknown biophysical phenomenon, but most importantly, it could also provide meaningful and scientific information related to the biodegradation of the ecosystem itself. In this research project, an attempt will be made to design a simple foundation of an aquatic Autonomous Surface Vehicle (ASV) that will be deployed in marine ecosystems. Such a vehicle would be embedded with different kind of electronic sensors, that are capable of measuring physical quantities such as temperature, pH, conductance, dissolved oxygen level, etc. The data collected by such a system can either be over a period of time (temporal data), or it could cover a vast aquatic geographical region (spatial data). This data will then be compared with existing datasets that are made publicly available by various environmental organizations in the United States, most importantly the Environmental Protection Agency (EPA) and the US Geological Survey (USGS). A comparative data analysis task between the data collected by the ASV and the vast array of environmental data that are made available from these agencies can then give us an indication about the status of the aquatic degradation of the ecosystem in question by measuring the extent to which the current data deviates from relevant historical data trends.&lt;/p&gt;
&lt;h2 id=&#34;2-background-research-and-previous-work&#34;&gt;2. Background Research and Previous Work&lt;/h2&gt;
&lt;p&gt;After reviewing the necessary background literture and previous work that has been done in this field, it can be stated that most of such endeavors focussed majorly on continuous environmental data collection with the help of sensors attached to a stationary buoy in a particular location of a water-body. Some of the other endeavors did involve deploying a non-stationary vehicle that collected data from large swaths of geographical areas in various water bodies &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, while some others focussed on niche areas involving migration pattern exhibited by zooplanktons upon natural and aritifical irradiance &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. However, neither did such attempts focus much on the data analysis portion for multiple sensory input(s) nor did it involve an intricate procedure to compare the collected data with historical trends so as to arrive at a suitable conclusion regarding the extent of environmental degradation.&lt;/p&gt;
&lt;p&gt;As mentioned in the previous section, this research project will exhaustively focus not just on the data-collection portion by a non-stationary vehicle, but it will also involve employing deeper study towards the subject of big-data analysis of both the current data of the system in question and the past data obtained for similar aquatic profiles. In this way, it would be possible to learn more about the toxicological aspects of the ecosystem in question.&lt;/p&gt;
&lt;h2 id=&#34;3-choice-of-data-sets&#34;&gt;3. Choice of Data-sets&lt;/h2&gt;
&lt;p&gt;Upon exploring a wide array of available datasets, the following data repositories were chosen to get the required water quality based data over a particular period of time and for a particular geographical region.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;USGS Water Quality Data: &lt;a href=&#34;https://waterdata.usgs.gov/nwis/qw&#34;&gt;https://waterdata.usgs.gov/nwis/qw&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;EPA Water Quality Data: &lt;a href=&#34;https://www.epa.gov/waterdata/water-quality-data-download&#34;&gt;https://www.epa.gov/waterdata/water-quality-data-download&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To answer the questions involving existence of multiple data-sets and motivation of using multiple data-sets, we must keep in mind that the very nature of this study is based on historical trends of the nature of water-quality in a particular region from the past and how it relates to the current situation. Because of these reasons, multiple data-sets will be referred to from multiple sources so as to achieve robust data-analytical results. This would ensure that too much focus is not given on outlier cases, that may be relevant to just a particular geographical region or an aberration in the data that may only have arisen due to an unknown underlying phenomenon or some form of cataclysmic event from the past. Using multiple datasets from different sources would help to get a resultant data structure that is more likely to converge towards an approximate level of historical thresholds and which can then be used to find out how the current observed data deviates from such previous patterns.&lt;/p&gt;
&lt;h2 id=&#34;4-methodology&#34;&gt;4. Methodology&lt;/h2&gt;
&lt;h3 id=&#34;41-hardware-component&#34;&gt;4.1 Hardware Component&lt;/h3&gt;
&lt;p&gt;The rough outline of the autonomous surface vehicle (ASV) in question has been perceived in the Autodesk Fusion 360 software model. A preliminary model has been designed in this software so as to 3D print the system. It will then be interfaced with the appropriate sensors in question. Then system will be driven by an Arduino-Uno based microcontroller, and it will have different types of environmental sensors that will collect and log data. These sensors have been purchased from the vendor, &amp;ldquo;Atlas Scientific&amp;rdquo;. As of now, the sensors that have been chosen for this ASV are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PT-1000 Temperature sensor kit - &lt;a href=&#34;https://atlas-scientific.com/kits/pt-1000-temperature-kit/&#34;&gt;https://atlas-scientific.com/kits/pt-1000-temperature-kit/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Potential of Hydrogen (pH) sensor kit - &lt;a href=&#34;https://atlas-scientific.com/kits/ph-kit/&#34;&gt;https://atlas-scientific.com/kits/ph-kit/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Dissolved Oxygen (DO) sensor kit - &lt;a href=&#34;https://atlas-scientific.com/kits/dissolved-oxygen-kit/&#34;&gt;https://atlas-scientific.com/kits/dissolved-oxygen-kit/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Conductivity K 1.0 sensor kit - &lt;a href=&#34;https://atlas-scientific.com/kits/conductivity-k-1-0-kit/&#34;&gt;https://atlas-scientific.com/kits/conductivity-k-1-0-kit/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A very rudimentary framework of the system has been realized in the Autodesk Fusion 360 software architecture as shown below. The design provided below is a very simplistic platform but which will lay the foundation of the final structure of the system.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;images/ASVSS1.png&#34; alt=&#34;ASV from Fusion 360&#34;&gt;&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;path_to_image&#34; alt&gt;
    &lt;em&gt;Figure 1: Nascent framework of the ASV system in Fusion 360&lt;/em&gt;
&lt;/p&gt;
&lt;h3 id=&#34;42-software-component&#34;&gt;4.2 Software Component&lt;/h3&gt;
&lt;p&gt;After the data has been collected by the ASV either on a temporal scale (over a period of time) or a spatial scale (over a geographical area), it will then be analyzed to decipher the median convergent values of the water body for the four different parameters that have been measured (i.e. Temperature, pH, DO, and Conductivity). The results of this data analysis task will then be used to find out if such water quality parametric values manifested by the aquatic ecosystem in question deviates by a large proportion from the other result that is obtained after analyzing the historical data from USGS and EPA for a nearby and unpolluted source of water. The USGS and EPA websites make it easier to find data from a nearby geographical region by making it possible to enter the desired location prior to searching for water quality data in their huge databases. In this way, it can be figured out if the water quality parameters of the particular ecological system varies wildly from a neighboring system that has almost the same geographical and ecological attributes.&lt;/p&gt;
&lt;p&gt;The establishment of the degree of variance of the data from the historical data will be carried out by documenting the particular quartile range that the current data lies in with respect to the median data that is obtained from the past/historical datasets. For instance, if the current data resides in the second quartile, it can be demarcated as being more or less consistent with previously established values. However, if it resides in the first or third quartile then it might will that the system has aberrant aspects which might need to be investigated for possible levels of outside pollutants (viz. industrial effluents, agricultural wash-off, etc.), or presence of harmful invasive species that might be altering the delicate natural balance of the ecosystem in question.&lt;/p&gt;
&lt;h2 id=&#34;5-inference&#34;&gt;5. Inference&lt;/h2&gt;
&lt;p&gt;This section will be addressed upon project completion.&lt;/p&gt;
&lt;h2 id=&#34;6-conclusion&#34;&gt;6. Conclusion&lt;/h2&gt;
&lt;p&gt;This section will be addressed upon project completion.&lt;/p&gt;
&lt;h2 id=&#34;7-acknowledgements&#34;&gt;7. Acknowledgements&lt;/h2&gt;
&lt;p&gt;The author would like to thank Dr. Gregor Von Laszewski, Dr. Geoffrey Fox, and the associate instructors in the &lt;em&gt;FA20-BL-ENGR-E534-11530: Big Data Applications&lt;/em&gt; course (offered in the Fall 2020 semester at Indiana University, Bloomington) for their continued assistance and suggestions with regard to exploring this idea and also for their aid with preparing the various drafts of this article.&lt;/p&gt;
&lt;h2 id=&#34;8-references&#34;&gt;8. References&lt;/h2&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Valada A., Velagapudi P., Kannan B., Tomaszewski C., Kantor G., Scerri P. (2014) Development of a Low Cost Multi-Robot Autonomous Marine Surface Platform. In: Yoshida K., Tadokoro S. (eds) Field and Service Robotics. Springer Tracts in Advanced Robotics, vol 92. Springer, Berlin, Heidelberg. &lt;a href=&#34;https://doi.org/10.1007/978-3-642-40686-7_43&#34;&gt;https://doi.org/10.1007/978-3-642-40686-7_43&lt;/a&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;M. Ludvigsen, J. Berge, M. Geoffroy, J. H. Cohen, P. R. De La Torre, S. M. Nornes, H. Singh, A. J. Sørensen, M. Daase, G. Johnsen, Use of an Autonomous Surface Vehicle reveals small-scale diel vertical migrations of zooplankton and susceptibility to light pollution under low solar irradiance. Sci. Adv. 4, eaap9887 (2018). &lt;a href=&#34;https://advances.sciencemag.org/content/4/1/eaap9887/tab-pdf&#34;&gt;https://advances.sciencemag.org/content/4/1/eaap9887/tab-pdf&lt;/a&gt; &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-313/assignment5/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-313/assignment5/</guid>
      <description>
        
        
        &lt;h1 id=&#34;homework-5&#34;&gt;Homework 5&lt;/h1&gt;
&lt;h2 id=&#34;student-name-fauzan-isnaini&#34;&gt;Student Name: Fauzan Isnaini&lt;/h2&gt;
&lt;h1 id=&#34;predicting-stock-market-recovery-in-indonesia-after-covid-19-crash&#34;&gt;Predicting Stock Market Recovery in Indonesia after COVID-19 Crash&lt;/h1&gt;
&lt;h2 id=&#34;team&#34;&gt;Team&lt;/h2&gt;
&lt;p&gt;I will conduct this study by myself.&lt;/p&gt;
&lt;h2 id=&#34;topic&#34;&gt;Topic&lt;/h2&gt;
&lt;p&gt;The COVID-19 Pandemic is not just a crisis in the public health sector.
It also impacts unemployment rates, business revenues, and mass
psychology, which in the end lead to crashes in global stock markets.
While some stock indexes like the Dow Jones Industrial Average (DJIA)
and NASDAQ Composite already recovered, the Indonesian Stock Market
Index (IDX Composite) is still far below its price before the pandemic.
Some of the possible causes are: 1. Foreign investments represent about
50% of the total fund in the IDX stock exchange. In a pandemic
situation, foreign investors might choose to withdraw their stocks and
find another safer country to invest in. 2. Unpredictability of the
pandemic situation drives investors to reallocate their funds in safer
assets, such as cash, gold, or USD. 3. Changes in the macroeconomic
situation, such as unemployment rate, Indonesian Rupiah (IDR) exchange
rate, and interest rate. 4. Changes in the consumer buying power also
change the business revenues, thus changing fundamental data. 5. Mass
psychology of investors that the stock market is not safe in this
pandemic situation, holding them from returning to the stock market To
predict the time needed for IDX Composite to recover, there are two
indicators that can be utilized: 1. Fundamental indicators, which
represent the financial aspect. This can be in the form of macroeconomic
data and a company financial report 2. Technical indicators, which
represent the mass psychology of investors. This can be obtained from
news, social media, or statistical analysis of how the stock market
moves&lt;/p&gt;
&lt;h2 id=&#34;dataset&#34;&gt;Dataset&lt;/h2&gt;
&lt;p&gt;In predicting the outcome, I will utilize these datasets: 1. Yahoo
Finance (finance.yahoo.com). Yahoo Finance contains a lot of both
fundamental and technical data, and they are free of charge. 2. Twitter
(twitter.com). I can conduct a content analysis on Twitter to represent
the mass psychology regarding the economic condition in Indonesia 3.
News channel. I can also utilize data crawler software to conduct
content analysis to compare positive and negative news in Indonesia. 4.
Third party stock data feeder. There are some third parties who provide
more comprehensive stock data on a subscription basis. This is another
option if the above datasets are not sufficient&lt;/p&gt;
&lt;h2 id=&#34;what-needs-to-be-done-to-get-a-great-grade&#34;&gt;What needs to be done to get a great grade&lt;/h2&gt;
&lt;p&gt;I will build a Python program to learn these data and generate a
prediction on when the IDX Composite will recover. I will also consider
the recovery rate of each of these sectors: 1. Mining 2. Agriculture 3.
Finance 4. Infrastructure 5. Miscellaneous Industries 6. Consumers&amp;rsquo;
Goods 7. Property 8. Trading 9. Basic Industry While they are
incorporated in the IDX Composite, the recovery rate of each sector may
be different because of their respective nature of the industry. For
example, consumer&amp;rsquo;s goods may be impacted less in this COVID-19
pandemic, thus resulting in a faster recovery. On the other hand, the
property sector might be the most impacted sector in this pandemic, thus
resulting in a long time to recover. The program will be able to learn
continuously, so if new data is available, it can renew the analysis and
give a more accurate prediction.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-313/assignment6/assignment6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-313/assignment6/assignment6/</guid>
      <description>
        
        
        &lt;h1 id=&#34;homework-6&#34;&gt;Homework 6&lt;/h1&gt;
&lt;p&gt;Student Name: Fauzan Isnaini&lt;/p&gt;
&lt;h1 id=&#34;how-ai-helps-diagnosis-and-decision-making-in-health-care-facilities&#34;&gt;How AI Helps Diagnosis and Decision Making in Health Care Facilities&lt;/h1&gt;
&lt;h2 id=&#34;radiology-assistant&#34;&gt;Radiology Assistant&lt;/h2&gt;
&lt;p&gt;Radiology is a branch of medicine that uses imaging technology to diagnose and treat disease. Diagnostic radiology helps health care providers see structures inside your body. Using the diagnostic images, the radiologist or other physicians can often [3]:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Diagnose the cause of your symptoms&lt;/li&gt;
&lt;li&gt;Monitor how well your body is responding to a treatment you are receiving for your disease or condition&lt;/li&gt;
&lt;li&gt;Screen for different illnesses, such as breast cancer, colon cancer, or heart disease
Within radiology, trained physicians visually assess medical images and report findings to detect, characterize and monitor diseases. Such assessment is often based on education and experience and can be, at times, subjective. In contrast to such qualitative reasoning, AI excels at recognizing complex patterns in imaging data and can provide a quantitative assessment in an automated fashion. More accurate and reproducible radiology assessments can then be made when AI is integrated into the clinical workflow as a tool to assist physicians.[1]
Some examples of AI’s clinical application in radiology are [1]:&lt;/li&gt;
&lt;li&gt;Thoracic imaging
AI can help in identifying pulmonary nodules, which can be applied in early detection of lung cancer&lt;/li&gt;
&lt;li&gt;Abdominal and pelvic imaging
AI can help in detecting lesions in abdominal and pelvic. For example, AI can analyze data from computed topography (CT) and magnetic resonance imaging (MRI) to detect liver lesions, and characterize these lesions as benign or malignant. Furthermore, AI can also help in suggesting the follow-up actions for the patient.&lt;/li&gt;
&lt;li&gt;Colonoscopy
Colonic polyps that are undetected or misclassified pose a potential risk of colorectal cancer. AI can help in making an early detection and consistent monitoring of this risk.&lt;/li&gt;
&lt;li&gt;Mammography
Analyzing mammography is technically challenging, even for a trained expert. AI can assist in interpreting the image. For example, AI can identify and characterize microcalcifications. Microcalcifications are tiny deposits of calcium salts that are too small to be felt but can be detected by imaging, and can be an early sign of breast cancer. They can be scattered throughout the mammary gland, or occur in clusters. [4]&lt;/li&gt;
&lt;li&gt;Brain imaging
AI can help in making diagnostic prediction of brain tumors, which are characterized by abnormal growth of brain tissue.&lt;/li&gt;
&lt;li&gt;Radiation oncology
Radiation treatment planning can be automated by segmenting tumours for radiation dose optimization. Furthermore, assessing response to treatment by monitoring over time is essential for evaluating the success of radiation therapy efforts. AI is able to perform these assessments, thereby improving accuracy and speed.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;ai-in-clinical-decision-support&#34;&gt;AI in Clinical Decision Support&lt;/h2&gt;
&lt;p&gt;Other than analyzing radiology images, AI can also digest data from blood tests, electrocardiogram (EKG), genomics, and patient medical history do give a better treatment to the patient. AI-enabled clinical decision support includes diagnosis and prognosis, and involves classification or regression algorithms that can predict the probability of a medical outcome or the risk for a certain disease.[5]
Here are some examples of how AI helps clinical decision [6]:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Accumulation of medical histories from birth alongside linked maternal electronic health record (HER) information in a healthcare facility, enabled the prediction of high obesity risk children as early as two years after birth, possibly allowing life-altering preventative interventions.&lt;/li&gt;
&lt;li&gt;The Advanced Alert Monitoring system developed and deployed by Kaiser Permanente uses Intensive Care Unit (ICU) data to predict fatally deteriorating cases and alert staff to the need of life-saving interventions.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-313/broken/assignment5/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-313/broken/assignment5/</guid>
      <description>
        
        
        &lt;h1 id=&#34;homework-5&#34;&gt;Homework 5&lt;/h1&gt;
&lt;h2 id=&#34;student-name-fauzan-isnaini&#34;&gt;Student Name: Fauzan Isnaini&lt;/h2&gt;
&lt;h1 id=&#34;predicting-stock-market-recovery-in-indonesia-after-covid-19-crash&#34;&gt;Predicting Stock Market Recovery in Indonesia after COVID-19 Crash&lt;/h1&gt;
&lt;h2 id=&#34;team&#34;&gt;Team&lt;/h2&gt;
&lt;p&gt;I will conduct this study by myself.&lt;/p&gt;
&lt;h2 id=&#34;topic&#34;&gt;Topic&lt;/h2&gt;
&lt;p&gt;The COVID-19 Pandemic is not just a crisis in the public health sector.
It also impacts unemployment rates, business revenues, and mass
psychology, which in the end lead to crashes in global stock markets.
While some stock indexes like the Dow Jones Industrial Average (DJIA)
and NASDAQ Composite already recovered, the Indonesian Stock Market
Index (IDX Composite) is still far below its price before the pandemic.
Some of the possible causes are: 1. Foreign investments represent about
50% of the total fund in the IDX stock exchange. In a pandemic
situation, foreign investors might choose to withdraw their stocks and
find another safer country to invest in. 2. Unpredictability of the
pandemic situation drives investors to reallocate their funds in safer
assets, such as cash, gold, or USD. 3. Changes in the macroeconomic
situation, such as unemployment rate, Indonesian Rupiah (IDR) exchange
rate, and interest rate. 4. Changes in the consumer buying power also
change the business revenues, thus changing fundamental data. 5. Mass
psychology of investors that the stock market is not safe in this
pandemic situation, holding them from returning to the stock market To
predict the time needed for IDX Composite to recover, there are two
indicators that can be utilized: 1. Fundamental indicators, which
represent the financial aspect. This can be in the form of macroeconomic
data and a company financial report 2. Technical indicators, which
represent the mass psychology of investors. This can be obtained from
news, social media, or statistical analysis of how the stock market
moves&lt;/p&gt;
&lt;h2 id=&#34;dataset&#34;&gt;Dataset&lt;/h2&gt;
&lt;p&gt;In predicting the outcome, I will utilize these datasets: 1. Yahoo
Finance (finance.yahoo.com). Yahoo Finance contains a lot of both
fundamental and technical data, and they are free of charge. 2. Twitter
(twitter.com). I can conduct a content analysis on Twitter to represent
the mass psychology regarding the economic condition in Indonesia 3.
News channel. I can also utilize data crawler software to conduct
content analysis to compare positive and negative news in Indonesia. 4.
Third party stock data feeder. There are some third parties who provide
more comprehensive stock data on a subscription basis. This is another
option if the above datasets are not sufficient&lt;/p&gt;
&lt;h2 id=&#34;what-needs-to-be-done-to-get-a-great-grade&#34;&gt;What needs to be done to get a great grade&lt;/h2&gt;
&lt;p&gt;I will build a Python program to learn these data and generate a
prediction on when the IDX Composite will recover. I will also consider
the recovery rate of each of these sectors: 1. Mining 2. Agriculture 3.
Finance 4. Infrastructure 5. Miscellaneous Industries 6. Consumers&amp;rsquo;
Goods 7. Property 8. Trading 9. Basic Industry While they are
incorporated in the IDX Composite, the recovery rate of each sector may
be different because of their respective nature of the industry. For
example, consumer&amp;rsquo;s goods may be impacted less in this COVID-19
pandemic, thus resulting in a faster recovery. On the other hand, the
property sector might be the most impacted sector in this pandemic, thus
resulting in a long time to recover. The program will be able to learn
continuously, so if new data is available, it can renew the analysis and
give a more accurate prediction.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-313/project/project/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-313/project/project/</guid>
      <description>
        
        
        &lt;h1 id=&#34;predicting-stock-market-recovery-in-indonesia-after-covid-19-crash&#34;&gt;Predicting Stock Market Recovery in Indonesia after COVID-19 Crash&lt;/h1&gt;
&lt;p&gt;Fauzan Isnaini&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; please use our template, see piazza&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The COVID-19 Pandemic is not just a crisis in the public health sector.
It also impacts unemployment rates, business revenues, and mass
psychology, which in the end lead to crashes in global stock markets.
While some stock indexes like the Dow Jones Industrial Average (DJIA)
and NASDAQ Composite already recovered, the Indonesian Stock Market
Index (IDX Composite) is still far below its price before the pandemic.
Some of the possible causes are: 1. Foreign investments represent about
50% of the total fund in the IDX stock exchange. In a pandemic
situation, foreign investors might choose to withdraw their stocks and
find another safer country to invest in. 2. Unpredictability of the
pandemic situation drives investors to reallocate their funds in safer
assets, such as cash, gold, or USD. 3. Changes in the macroeconomic
situation, such as unemployment rate, Indonesian Rupiah (IDR) exchange
rate, and interest rate. 4. Changes in the consumer buying power also
change the business revenues, thus changing fundamental data. 5. Mass
psychology of investors that the stock market is not safe in this
pandemic situation, holding them from returning to the stock market To
predict the time needed for IDX Composite to recover, there are two
indicators that can be utilized: 1. Fundamental indicators, which
represent the financial aspect. This can be in the form of macroeconomic
data and a company financial report 2. Technical indicators, which
represent the mass psychology of investors. This can be obtained from
news, social media, or statistical analysis of how the stock market
moves&lt;/p&gt;
&lt;h2 id=&#34;dataset&#34;&gt;Dataset&lt;/h2&gt;
&lt;p&gt;In predicting the outcome, I will utilize these datasets: 1. Yahoo
Finance (finance.yahoo.com). Yahoo Finance contains a lot of both
fundamental and technical data, and they are free of charge. 2. Twitter
(twitter.com). I can conduct a content analysis on Twitter to represent
the mass psychology regarding the economic condition in Indonesia 3.
News channel. I can also utilize data crawler software to conduct
content analysis to compare positive and negative news in Indonesia. 4.
Third party stock data feeder. There are some third parties who provide
more comprehensive stock data on a subscription basis. This is another
option if the above datasets are not sufficient&lt;/p&gt;
&lt;h2 id=&#34;plan&#34;&gt;Plan&lt;/h2&gt;
&lt;p&gt;I will build a Python program to learn these data and generate a
prediction on when the IDX Composite will recover. I will also consider
the recovery rate of each of these sectors: 1. Mining 2. Agriculture 3.
Finance 4. Infrastructure 5. Miscellaneous Industries 6. Consumers&amp;rsquo;
Goods 7. Property 8. Trading 9. Basic Industry While they are
incorporated in the IDX Composite, the recovery rate of each sector may
be different because of their respective nature of the industry. For
example, consumer&amp;rsquo;s goods may be impacted less in this COVID-19
pandemic, thus resulting in a faster recovery. On the other hand, the
property sector might be the most impacted sector in this pandemic, thus
resulting in a long time to recover. The program will be able to learn
continuously, so if new data is available, it can renew the analysis and
give a more accurate prediction.&lt;/p&gt;
&lt;p&gt;asassasa&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-313/test/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-313/test/</guid>
      <description>
        
        
        &lt;h2 id=&#34;this-is-testmd&#34;&gt;This is Test.md&lt;/h2&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-314/project/project/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-314/project/project/</guid>
      <description>
        
        
        &lt;h1 id=&#34;residential-power-usage-prediction&#34;&gt;Residential Power Usage Prediction&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; please use our trivial template posted in piazza&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; please never use sapces in image names&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; please use footnotes for refernces, you must cite them&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; figure numbers are inconsitent, you have 3 figures&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; document not checked on content graders will do that&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Siny P Raphel, &lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-314/&#34;&gt;fa20-523-314&lt;/a&gt;, &lt;a href=&#34;https://github.com/cybertraining-dsc/fa20-523-314/blob/master/project/project.md&#34;&gt;Edit&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Electricity is an inevitable part of our day today life. Most of the electric service providers like duke, dominion provide customers their consumption data so that customers are aware of their usages. Some providers give predictions on their future usages so that they are prepared.&lt;/p&gt;
&lt;p&gt;This project is based on the dataset Residential Power Usage 3 years data in Kaggle datasets&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. The dataset contains data of hourly power consumption of a 2 storied house in Houston,Texas from 01-06-2016 to August 2020. It includes data during the Covid-19 lockdown and are marked as well. We are planning to build a model to predict future usage from available data.
Data is spread across two csv files.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;power_usage_2016_to_2020.csv&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This file contains basic details of the data like startdate with hour, value of power consumption in kwh, day of the week and notes. It has 4 features and 35953 instances.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-314/blob/master/project/images/fig-1.png&#34; alt=&#34;Figure 1&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Caption missing&lt;/p&gt;
&lt;p&gt;Day of the week is an integer value with 0 being Monday. Notes gives us details like whether that day is weekend, weekday, covid lockdown or vacation. The Figure 1 shows retrieval and first few rows of the data.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-314/blob/master/project/images/fig-2.png&#34; alt=&#34;Figure 2&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 2:&lt;/strong&gt; Caption missing&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;weather_2016_2020_daily.csv&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This file contains the weather conditions of that particular day. It has 19 features and 1553 instances. Figure 3 shows retrieval and first few rows and columns of this file.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/cybertraining-dsc/fa20-523-314/blob/master/project/images/fig-3.png&#34; alt=&#34;Figure 3&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 3:&lt;/strong&gt; Caption missing&lt;/p&gt;
&lt;p&gt;Units of features are given as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Temperature    - F deg&lt;/li&gt;
&lt;li&gt;Dew Point      - F deg&lt;/li&gt;
&lt;li&gt;Humidity       - %age&lt;/li&gt;
&lt;li&gt;Wind           - mph&lt;/li&gt;
&lt;li&gt;Pressure       - Hg&lt;/li&gt;
&lt;li&gt;Precipitation  – inch&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We will be using python to develop the model. Since the expected outputs are real numbers(power consumption in kWh) we might be using linear regression or similar ones. We will try using gradient descent for optimization. Since the weather data has 19 features we might use feature selection methods to select best features that increase the accuracy of the model.
The data spread across two files will have to be merged according to date. For that the StartDate feature will have to be first split to date and time. Then the two datasets will have to be merged according to the date only. From the initial inspection of the data, the date feature of datasets have some date format issues which will have to be resolved before starting cleaning.&lt;/p&gt;
&lt;p&gt;In this project we will be planning the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Analyze and clean the data&lt;/li&gt;
&lt;li&gt;Visualize the data- study the relationships between features etc.&lt;/li&gt;
&lt;li&gt;Plan one or two algorithms that can be used to model&lt;/li&gt;
&lt;li&gt;Optimize or feature selection of features&lt;/li&gt;
&lt;li&gt;Calculate accuracy of each model&lt;/li&gt;
&lt;li&gt;Conclusion on which algorithm will be best suited to use and the reason for it.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This dataset is chosen because,&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;There were datasets similar to this one. But this one has latest power usage data till August this year.&lt;/li&gt;
&lt;li&gt;It has marked covid lockdown, vacations, weekdays and weekends which is a challenge for prediction.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/srinuti/residential-power-usage-3years-data-timeseries&#34;&gt;https://www.kaggle.com/srinuti/residential-power-usage-3years-data-timeseries&lt;/a&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-314/test/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-314/test/</guid>
      <description>
        
        
        &lt;h1 id=&#34;this-is-to-test-markdown&#34;&gt;This is to test markdown&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;bullet&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-316/project/project/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-316/project/project/</guid>
      <description>
        
        
        &lt;h1 id=&#34;sentiment-analysis-and-visualization-using-an-us-election-dataset-for-the-2020-election&#34;&gt;Sentiment Analysis and Visualization using an US-election dataset for the 2020 Election&lt;/h1&gt;
&lt;p&gt;Sudheer Alluri Indiana University, fa20-523-316, &lt;a href=&#34;mailto:ngsudheer@gmail.com&#34;&gt;ngsudheer@gmail.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Vishwanadham Mandala, Indiana University, fa20-523-325, &lt;a href=&#34;mailto:vishwandh.mandala@gmail.com&#34;&gt;vishwandh.mandala@gmail.com&lt;/a&gt;&lt;/p&gt;


&lt;div class=&#34;pageinfo pageinfo-primary&#34;&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Sentiment analysis is an evaluation of the opinion of the speaker, writer or other subject with regard to some topic.We are going to use US-elections dataset and combining the tweets of people opninon for leading presidential candidates. We have various datasets from kallage and combining tweets and NY times datasets, by combining all data predication will be dervied.&lt;/p&gt;
&lt;p&gt;Contents&lt;/p&gt;
&lt;div class=&#34;toc&#34;&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#1-introduction&#34;&gt;1. Introduction&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#2-datasets&#34;&gt;2. DataSets&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#3-methodologyprocess&#34;&gt;3. Methodology/Process&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#4-technologies-used&#34;&gt;4. Technologies used&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#5-refernces&#34;&gt;5. Refernces&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Keywords:&lt;/strong&gt; sentiment,  US-election&lt;/p&gt;
&lt;h2 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;For our final project, we will be focusing on the upcoming U.S. presidential elections. We plan to use a US-elections dataset to predict the votes each contestant will attain, by area. With growing data, the prediction will be changing constantly. We are making the difference by selecting the latest dataset available and previous election data to predict the number of votes each contestant will get to the closest figure. A feature we are introducing to enhance the quality is predicting various area types like counties, towns, and/or big cities.
One might argue that these kinds of predictions will only be helping organizations and not individuals. We assure you that this project will be helping the general public in many ways. The most evident being, an individual knowing which contestant his/her community or the general public around him/her prefer. This project is strictly statistical and does not have a goal to sway the elections in any way or to pressure an individual&lt;/p&gt;
&lt;p&gt;into picking a candidate. Overall, this is just a small step towards a future that might hold an environment where the next president of the United States of America could be accurately guessed based on previous data and innovative Big Data Technologies.&lt;/p&gt;
&lt;h2 id=&#34;2-datasets&#34;&gt;2. DataSets&lt;/h2&gt;
&lt;p&gt;We will be going to use the dataset, &lt;a href=&#34;https://www.kaggle.com/tunguz/us-elections-dataset,&#34;&gt;https://www.kaggle.com/tunguz/us-elections-dataset,&lt;/a&gt; and we will create the filets based on location. If needed, we may download Twitter data from posts on and by Donald Trump, Joe Biden, and their associates. Which leads us to our objective for the project, based on the data we collected, we should be able to predict the winner of the 2020 United States of America’s presidential elections.&lt;/p&gt;
&lt;p&gt;All of the data will be location-based and if required we will download realtime campaigning and debate analysis data, giving us a live and updated prediction every time increment. To strengthen the prediction, even more, we may reuse some code from the 2016 election’s analysis, however, our main focus will be using the latest data we readily acquire during the time leading up to the 2020 election.
In conclusion, to make our predictions as realistic and as strong as we can get, we will be going to choose multiple data sets to integrate between the previous election and twitter data to predict the number of votes each candidate will acquire. Therefore, we will be predicting the winner of the 2020 presidential elections.&lt;/p&gt;
&lt;h2 id=&#34;3-methodologyprocess&#34;&gt;3. Methodology/Process&lt;/h2&gt;
&lt;p&gt;We will collect election data and twitter information and integrate both to predict the results. A lot of twitter or dataset data will be trimmed and parsed to build the model. We will calculate
Our data-gathering and preparation methodology is composed of the following steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Use the latest election dataset-2020, we will be creating the model.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Data cleaning and extraction.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We will try to download the latest data from twitter and campaigning.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;4-technologies-used&#34;&gt;4. Technologies used&lt;/h2&gt;
&lt;p&gt;Python, Jupyter notebook or collab, Pandas, Scikit-learn, PyTorch,&lt;/p&gt;
&lt;h2 id=&#34;5-refernces&#34;&gt;5. Refernces&lt;/h2&gt;
&lt;p&gt;TBD&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-319/assignments/rama_asuri_hw3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-319/assignments/rama_asuri_hw3/</guid>
      <description>
        
        
        &lt;h1 id=&#34;rama-asuri---homework-3-choice-1-square-kilometre-array-ska-usecase&#34;&gt;Rama Asuri - Homework 3 (choice 1): Square Kilometre Array (SKA) usecase&lt;/h1&gt;
&lt;h2 id=&#34;science-goals&#34;&gt;Science goals&lt;/h2&gt;
&lt;p&gt;According to &lt;a href=&#34;https://www.skatelescope.org&#34;&gt;2&lt;/a&gt;, here are the science goals -&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Galaxy Evolution, Cosmology And Dark Energy&lt;/li&gt;
&lt;li&gt;Strong-field tests of gravity using pulsars and black holes&lt;/li&gt;
&lt;li&gt;Probing the Cosmic Dawn&lt;/li&gt;
&lt;li&gt;The cradle of life&lt;/li&gt;
&lt;li&gt;Flexible design to enable exploration of the unknown&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;galaxy-evolution-cosmology-and-dark-energy&#34;&gt;Galaxy Evolution, Cosmology And Dark Energy&lt;/h3&gt;
&lt;p&gt;Universe is expanding but not at a constant rate. It is accelerating. We don&amp;rsquo;t know the reason why it is expanding in
the first place. We could understand the reasons for this expansion, if we can learn more about Hydrogen and dark
matter.&lt;/p&gt;
&lt;p&gt;Hydrogen is abundant throughout the Universe. It is considered to be the fuel for a star formation inside a galaxy.
Seventy percent of the Sun is Hydrogen and Hydrogen atoms emit radio waves which makes the Sun and other stars some of
the brightest objects in the cosmos.&lt;/p&gt;
&lt;p&gt;SKA will help understand how galaxies form and evolve by detecting the Hydrogen across the Universe. As of today, we
don&amp;rsquo;t have this kind of technology to detect farthest distance like edge of the Universe. SKA can detect new galaxy
formations at the edge of the Universe &lt;a href=&#34;https://www.skatelescope.org&#34;&gt;2&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;strong-field-tests-of-gravity-using-pulsars-and-black-holes&#34;&gt;Strong-field tests of gravity using pulsars and black holes&lt;/h3&gt;
&lt;p&gt;Einstein&amp;rsquo;s theory of relativity still holds true. SKA will test relativity principles where no exiting technology has
ever accessed the regions of the Universe that is dark and far &lt;a href=&#34;https://docs.google.com/document/d/1ZMrga5R_idBcFlhvvhlcOP9aX--bpqhJg4XSS3Qi3ws/edit#&#34; title=&#34;use case template&#34;&gt;1&lt;/a&gt;. Astronomers found quasars with the help of
radio data &lt;a href=&#34;https://science.nasa.gov/ems/05_radiowaves&#34;&gt;6&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;the-origin-and-evolution-of-cosmic-magnetism&#34;&gt;The origin and evolution of cosmic magnetism&lt;/h3&gt;
&lt;p&gt;According to &lt;a href=&#34;https://phys.org/news/2020-09-strongest-magnetic-field-universe-x-ray.html&#34;&gt;3&lt;/a&gt;, cosmic magnetism is everywhere in the Space. Learning about magnetic fields help us understand the
electric fields. The question is, where does the cosmic magnetism come from? We don&amp;rsquo;t know this but understanding
cosmic magnetism definitely helps us learn more about the Universe from the time of Big Bang. Besides gravity,
magnetism shapes the massive structures in the Universe &lt;a href=&#34;https://www.youtube.com/watch?v=QBsBUypc1VE&#34;&gt;4&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;probing-the-cosmic-dawn&#34;&gt;Probing the Cosmic Dawn&lt;/h3&gt;
&lt;p&gt;SKA is the most sensitive radio telescope. It will go beyond the distance of 13 billion light years.
Optical and infrared telescopes captured galaxies at a distance of 13 billion light years from earth.
Even before that, there was a time where the Universe was dark up until the formation of the first galaxy.
Radio Telescopes reach even when there is no light or light is being blocked&lt;a href=&#34;https://www.space.com/39837-first-stars-universe-fingerprints-dark-matter.html&#34;&gt;9&lt;/a&gt;.
SKA will provide insight into this early Universe before the formation of the first galaxy &lt;a href=&#34;https://docs.google.com/document/d/1ZMrga5R_idBcFlhvvhlcOP9aX--bpqhJg4XSS3Qi3ws/edit#&#34; title=&#34;use case template&#34;&gt;1&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;the-cradle-of-life&#34;&gt;The cradle of life&lt;/h3&gt;
&lt;p&gt;There are two ways SKA can help find the extraterrestrial life. One way is to detect weak extraterrestrial
radio signals if they were to exist. This will expand on the capabilities of projects such as
search for extraterrestrial intelligence (SETI). Other way is to search for organic molecules in the
outer space &lt;a href=&#34;https://en.wikipedia.org/wiki/Search_for_extraterrestrial_intelligence&#34;&gt;5&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;flexible-design-to-enable-exploration-of-the-unknown&#34;&gt;Flexible design to enable exploration of the unknown&lt;/h3&gt;
&lt;p&gt;We don&amp;rsquo;t know what we will be stumbling on while exploring the space. SKA is a high sensitivity radio detection.
Optical and infrared has limitations due to dust or poor visibility. But Radio waves can be detected even without the
light. Astronomers stumbled on asteroids hunting grounds in archived Hubble images &lt;a href=&#34;https://hubblesite.org/contents/news-releases/1998/news-1998-10.html&#34;&gt;10&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;current-status&#34;&gt;Current status&lt;/h2&gt;
&lt;p&gt;There are 3 phases. Phase 1 provides ~10% of the total collecting area at low
and mid frequencies by 2023 (SKA1). Phase 2 provides full array (SKA2) at low and mid frequencies by 2030 and
Phase 3 will extend the frequency range up to 50Ghz [&lt;a href=&#34;https://en.wikipedia.org/wiki/Square_Kilometre_Array&#34;&gt;7&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;Late last year, SKA successfully completed its System Critical Design Review. It also under went two additional reviews
related to Operations and Management &lt;a href=&#34;http://www.jodrellbank.manchester.ac.uk/research/research-centres/ska-project/&#34;&gt;8&lt;/a&gt;. At the of reviews, there were 250 observations and questions raised which
resulted in 40 recommendations. SKAO accepted all 40 of those recommendations
and currently being implemented. SKAO team fully understand the issues and
challenges. It helped them quickly move to implementation stage. After
successful completion of the reviews, decision makers approved the construction
to start later this year(2020). Initially, COVID-19 threatened the review
process but the reviews have completed on time as planned. This puts the SKA
project on track with no delays. Based on the reviews outcome, SKAO team
finalized the  SKA’s Construction Proposal and Observatory Establishment and
Delivery Plan [&lt;a href=&#34;https://www.skatelescope.org&#34;&gt;2&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;SKAO team is putting together new governance structure and planning for the
establishment of the Observatory later this year and for the commencement of
SKA1 construction activities as early in 2021 as possible. According to
Philip Diamond (SKA Director-General), they are taking various risk mitigation
steps as a precautionary measure by laying down various failure scenarios. He
also mentioned that this world&amp;rsquo;s major scientific endeavor will bring
employment, innovation and scientific exploration to benefit partner
countries [&lt;a href=&#34;https://www.skatelescope.org&#34;&gt;2&lt;/a&gt;].&lt;/p&gt;
&lt;h2 id=&#34;big-data-challenges-of-the-ska&#34;&gt;Big data challenges of the SKA&lt;/h2&gt;
&lt;p&gt;The data is huge and storage will be a problem. To tackle this problem, data size must be reduced in real time.
Only capture or filter relevant data and save it. Raw data generated would be around an exabyte a day and after
compression, it will reduce to 10 petabytes per day. An Tao, head of the SKA group of the Shanghai Astronomical
Observatory stated, &amp;ldquo;It will generate data streams far beyond the total Internet traffic worldwide.&amp;rdquo; [&lt;a href=&#34;https://en.wikipedia.org/wiki/Square_Kilometre_Array&#34;&gt;7&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;Raw data is coming from everything that is being observed by SKA. This data is collected by an array of dish receivers
located across different geographical locations around the world. In case of pulsars, this data is reduced to form
astronomical images or temporal data [&lt;a href=&#34;https://docs.google.com/document/d/1ZMrga5R_idBcFlhvvhlcOP9aX--bpqhJg4XSS3Qi3ws/edit#&#34; title=&#34;use case template&#34;&gt;1&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;Initially, the data is ran through quality checks and good data, free of radio frequency interference,  is stored in the
long term preservation system. Multiple copies of this data is save at different location as a backup. Later,
post processing and data analysis is performed. Once the data analysis is done, it can be visualized.
All of this data is also preserved for the long term. Every year, it is expected to generate 600 PB and store this
data for the observatory lifetime which is projected to be 50 years. This is just for the phase 1 of the project.
Phase 2 and phase 3 will increase the size of the day to even bigger scale. As the data gets processed, the velocity
of the data goes down from Petabytes to Terabytes. There is also data that is coming from logs, calibrations,
configurations, managing archival, simulation models related to the SKA itself. This data is used to run the SKA
without failures or downtime. Images are stored into databases of sources and time series data [&lt;a href=&#34;https://docs.google.com/document/d/1ZMrga5R_idBcFlhvvhlcOP9aX--bpqhJg4XSS3Qi3ws/edit#&#34; title=&#34;use case template&#34;&gt;1&lt;/a&gt;].&lt;/p&gt;
&lt;h2 id=&#34;footnotes&#34;&gt;Footnotes&lt;/h2&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-319/project/project/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-319/project/project/</guid>
      <description>
        
        
        &lt;h1 id=&#34;detect-and-classify-pathologies-in-chest-x-rays-using-pytorch-library&#34;&gt;Detect and classify pathologies in chest X-rays using PyTorch library&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; please use oure template as posted in piazza&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; please improve citations&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;| Rama Asuri
| &lt;a href=&#34;mailto:asurirk@gmail.com&#34;&gt;asurirk@gmail.com&lt;/a&gt;, &lt;a href=&#34;mailto:rasuri@indiana.edu&#34;&gt;rasuri@indiana.edu&lt;/a&gt;
| Indiana University
| hid: fa20-523-319&lt;/p&gt;
&lt;p&gt;Keywords: PyTorch, CheXpert&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Deep learning gains expert level performance by training on large datasets.  Our project uses chest x-rays dataset 
called CheXpert and apply deep  learning to detect and diagnose a condition based on image data. CheXpert dataset 
contain 224,316 chest radiographs of 65,240 patients. There are 14 observations in a radiology report and labels are 
mapped to these 14 observations. We train our models to use the labels that corresponds to different diseases. Our 
deep learning models are based on PyTorch library.&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Deep learning methods are becoming very reliable at achieving expert level performance using large labeled datasets. 
CheXpert is a large dataset that contains 224,316 chest radiographs of 65,240 patients. This dataset contains
14 observations in radiology reports and also capturing uncertainties inherent in radiograph interpretation using
uncertainty labels. There is also a validation set of 200 chest radiographic studies which were manually annotated 
by 3 board-certified radiologists. Deep learning model should detect different pathologies. CheXpert is a public 
dataset [3] and has a strong radiologist-annotated ground truth and expert scores against which we can compare 
the model [1].&lt;/p&gt;
&lt;p&gt;We will use PyTorch library which is an open source machine learning library based on the Torch library [2]. It is 
used for applications such as computer vision and natural language processing which is primarily developed by 
Facebook&amp;rsquo;s AI Research lab (FAIR). It is free and open-source software released under the Modified BSD license.
Although the Python interface is more polished and the primary focus of development, PyTorch also has a C++ interface. 
There are number of Deep Learning applications that are built on top of PyTorch, including Tesla Autopilot, Uber&amp;rsquo;s 
Pyro etc [4].&lt;/p&gt;
&lt;h2 id=&#34;requirements&#34;&gt;Requirements&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Work in progress&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;we develop the Deep Learning Software using PyTorch library to perform the following.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Load raw Chest X-rays and put raw data into PyTorch&lt;/li&gt;
&lt;li&gt;Identify the one of the five pathologies&lt;/li&gt;
&lt;li&gt;Group the Pathologies&lt;/li&gt;
&lt;li&gt;Classify the pathology&lt;/li&gt;
&lt;li&gt;Diagnose the patient&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;design&#34;&gt;Design&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Missing design&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;
&lt;h2 id=&#34;dataset&#34;&gt;Dataset&lt;/h2&gt;
&lt;h2 id=&#34;implementation&#34;&gt;Implementation&lt;/h2&gt;
&lt;h2 id=&#34;benchmark&#34;&gt;Benchmark&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;TBD - Other deep learning models data is available online for benchmarking.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;h2 id=&#34;acknowledgement&#34;&gt;Acknowledgement&lt;/h2&gt;
&lt;h2 id=&#34;project-plan&#34;&gt;Project plan&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Missing - Adding detailed project plan&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This project has following deliverables&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Design documents&lt;/li&gt;
&lt;li&gt;Final Project report&lt;/li&gt;
&lt;li&gt;Working Software demo&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;refernces&#34;&gt;Refernces&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;[1] &lt;a href=&#34;https://stanfordmlgroup.github.io/competitions/chexpert/&#34;&gt;https://stanfordmlgroup.github.io/competitions/chexpert/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[2] &lt;a href=&#34;https://pytorch.org/deep-learning-with-pytorch&#34;&gt;https://pytorch.org/deep-learning-with-pytorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[3] &lt;a href=&#34;https://arxiv.org/pdf/1901.07031.pdf&#34;&gt;https://arxiv.org/pdf/1901.07031.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[4] &lt;a href=&#34;https://en.wikipedia.org/wiki/PyTorch&#34;&gt;https://en.wikipedia.org/wiki/PyTorch&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-319/test/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-319/test/</guid>
      <description>
        
        
        &lt;h1 id=&#34;test&#34;&gt;Test&lt;/h1&gt;
&lt;h2 id=&#34;rama-asuri&#34;&gt;Rama Asuri&lt;/h2&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-323/assignments/assignment-1/atugman-assignment1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-323/assignments/assignment-1/atugman-assignment1/</guid>
      <description>
        
        
        &lt;p&gt;Anthony Tugman&lt;br&gt;
E534&lt;br&gt;
9/7/20&lt;br&gt;
Assignment 1&lt;/p&gt;
&lt;p&gt;America is an incredibly diverse community, a vast pool of individuals from various
backgrounds, experiences, and beliefs striving towards the elusive American dream. In stark
contrast, the cities of America are one in the same. Not in the sense of architecture, food, and culture but rather in the way that they are run. Policy makers always seem to be playing “catch up” whether it be in reacting to environmental disasters, managing services for ever shifting populations, or even attempting to respond to crime. What if our cities were smarter? Imagine being alerted in real time to hotspots of pollution, being able to visualize traffic patterns throughout the day, and being able to identify large gatherings calling for an increase in police presence. By implementing big data and analytics into our cities this, and more, is possible.&lt;/p&gt;
&lt;p&gt;By definition a smart city is “an urban area that uses different types of electronic sensors
to collect data” [1]&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. This data is then analyzed and insights gained from the analysis are used to manage the city more efficiently, or to recognize patterns that would not be realized by typical human observation. This application relies on big data although physical in nature at its heart. What I mean by this is that the sensors themselves are physical, and a wide variety must be used to truly understand what is occurring in the environment. The data itself is a side effect of the sensors, but this is where the heart of the benefit is held. With the technology readily available, and the potential benefit from the investment of deployment unlimited, it is surprising to me that we do not see such systems deployed across cities around the world. One such project, The
Array of Things, has slowly been implemented across parts of Chicago over the last four years.&lt;/p&gt;
&lt;p&gt;The Array of Things is self described as an experimental urban measurement project
comprising a network of interactive, modular devices, to collect real-time data on the city’s
environment, infrastructure, and activity [2]&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. With the goal of providing this data to the public as well as various engineers, policy makers, and residents, The Array of Things now features a network of 150 sensor locations strategically placed across the city [2]&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. It is important to note that the creators of this project claim this to be an experiment, which answers my earlier question of why more cities do not follow suit. It seems that many issues arise from collecting such data including integrity, privacy, and usability. This seems to be a common concern across all areas of big data and finding solutions to these concerns will make individuals use the technology more often. If The Array of Things is able to ultimately solve the above problems, I do not see why this technology would not be a template for applying across the world.&lt;/p&gt;
&lt;p&gt;Although the hardware behind the nodes is fascinating, it is more applicable to focus on
the data they produce for the scope of this course. At the current iteration the devices collect information on pollution, traffic, and other ambient information. The collected information is publicly available and the team behind The Array of things has done its best to mitigate privacy concerns.&lt;/p&gt;
&lt;p&gt;In reflection, I believe that The Array of Things has immense potential to become life
changing in the way our cities are managed. However, this does not come without its downfalls.  The way collected data is currently being published is not appropriate for the target audience. In fact, it seems that an advanced degree would be required to make sense of any of the data. For The Array of Things to gain the public’s trust as well as be used by various industries it must become more user friendly. For widespread adoption I propose the information be available in two forms. Firstly in a way that the technically inclined can analyze the data in ways the creators did not imagine possible. Second, the data should be available in an easy to digest, visual format that the common man can decipher. In any case, I see The Array of Things as the forefront of implementing big data into cities around the world and will continue to monitor updates to the project in hopes of sifting through the data myself one day and informing policy decision with my results.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;“Smart city” Wikipedia, 11-Sep-2020. [Online]. Available:
&lt;a href=&#34;https://en.wikipedia.org/wiki/Smart_city&#34;&gt;https://en.wikipedia.org/wiki/Smart_city&lt;/a&gt;. [Accessed: 12-Sep-2020]. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Array of Things. [Online]. Available: &lt;a href=&#34;https://arrayofthings.github.io/index.html&#34;&gt;https://arrayofthings.github.io/index.html&lt;/a&gt;.
[Accessed: 12-Sep-2020]. &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-323/assignments/assignment-3/atugman-assignment3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-323/assignments/assignment-3/atugman-assignment3/</guid>
      <description>
        
        
        &lt;p&gt;Anthony Tugman&lt;br&gt;
E534&lt;br&gt;
9/21/20&lt;br&gt;
Assignment 3&lt;/p&gt;
&lt;p&gt;The Square Kilometer Array (SKA) is a global, cross-disciplinary partnership striving to
produce the world’s largest radio telescope. As the name correctly describes, the field of sensors will stretch across a full square kilometer and be located across western Australia and south Africa which are considered to be some of the most remote areas in the world [1]&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. As proposed in 2018 the SKA’s goals include studying the various types of energy that exist in the universe, deepening the understanding of gravitational waves, studying the birth of the universe, and searching for extraterrestrial communication. The developers of SKA have also made clear that this radio telescope will not only be larger than any in existence, it will also collect, analyze, and store data in speed and quantity that has not been seen before. The project is in the design phase and is constantly evolving to overcome technological and legislative barriers. As of September 2020 the SKA had received final approval and is preparing to enter the construction phase [2]&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;The future achievement of this project is highlighted by the amount of the data processed,
as well as how the data is being processed. While still in the design phase the global
collaboration is attempting to predict any problems that may occur and plan accordingly. SKA
must use custom designed hardware to handle the amount and speed of data processing necessary
for this project to be successful. The matter in which this data is to be processed is entirely different then existing systems. Instead of storing the data collected for processing, the SKA will process data in real time and only store the results. In doing so, the intermediate data is disposed of after the processing process has been completed. Even in doing so the SKA will produce 700PB of usable data annually between the two telescope arrays. This scale processing will require 2 supercomputers each 25% more powerful than what was available in 2019, as well as broadband capabilities 100,000 times faster than what is expected to be available in 2022 [1]&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;In the scope of this course the big data challenges this systems construction creates is
most important to focus on. Undoubtedly a custom software and computing system must be
developed. Although the data storage pipeline was previously described, it is important to note that small research stations across the globe will communicate with the large data center for each field of telescopes. To accomplish this task the designers expect to use various computer systems including custom PCBs, FPGAs, and commodity servers. The SKA team hopes to piggyback on existing infrastructure in the host countries to avoid starting from the ground up [3]&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;. The first big data challenge stems from the volume of the data that will be collected and stored. In the use case data was to be collected at 700 petabytes per year, stored indefinitely (approximately 50 years) [3]&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;. The velocity of the data being transferred is staggering as well. In this application the data is time dependent and needed at a constant pace for computation and refining. As proposed, data will be transferred at approximately 1TB/sec [3]&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;. According to the use case, there is not much variety in the type of data that will be transferred. In most cases the data will be in the form of images produced by the telescope with attached time data. If the researchers at the satellite office need access to other types of data, or more raw data, they will be able to do so through a virtual protocol as the data will not be stored on site [3]&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;. The project designers seemed less sure in the variability of the data. In this case variability differs from variety as it refers to variation in the rate and nature of the data gathered. Here the variability was described as occasional, most often in the case when the telescope is changing the type of observation it is performing. It is also noted that there may be variability when the system is subject to certain strains, such as during system maintenance [3]&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;Additionally, the project designers stressed maintaining the quality of the data as well as
building a troubleshooting pipeline for when errors occurred. All systems of the SKA will store remnants of data and proper logs to insure the integrity of the data. This concept is true not only for the raw data being collected, but also for all other processes including the telescope equipment, sensors, and transmission equipment. Overall, the data will be verified by a combination of physical human checks and automated checks by the computer [3]&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;. Unlike the previous sections the project designers did not make entirely clear the type of data they expect to receive after all collection and processing has been complete. Out of curiosity I did a quick web search to see what data is typically generated by radio telescopes and it is a variety. In any case, the project designers are confident that their system will be able to handle any requests/queries the researchers make. As a final consideration, the quality and security of the data must be preserved. In this situation the SKA governing board seems most likely and appropriate to manage the data generated. The project designers point out that the data is initially available only to member agencies for the first 18 months, after the data is released to the general science community. In certain situations or countries however, the data is subject to ruling by the local governing board, and the SKA governing board will comply appropriately [3]&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;. In conclusion of the analysis of big data problems faced by the SKA project, it seems apparent that they are common to most projects involving big data. Primary concerns arise from data collection, storage, integrity, and security. Through analysis of the provided use case it is clear that the SKA project understands many of the roadblocks that they must overcome, however in many situations they did not spell out a clear plan or solution. Many of their provided responses were vague but I am confident that with a coalition of scientists across the international community in support, the team will be able to reach their goals.&lt;/p&gt;
&lt;p&gt;The science goals of the SKA project were briefly mentioned, but it is worth looking into
each in more detail. The first use case is research into the evolution of the galaxy, dark energy, and the rapid acceleration in size of the universe. To accomplish this goal, the SKA will monitor hydrogen distribution throughout the galaxy, specifically at the edge of the known galaxy in hopes of watching new ones be born. Initially discovered in 1930 by Karl Jansky, hydrogen can be monitored as it returns a specific frequency back to the radio telescope. The SKA has the added feature of being able to see further and more precisely to gather a better understanding of what is occurring in the galaxy [4]&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;. In this section, the project designers make a careful clarification into what is meant by the sensitivity and resolution of the SKA. Sensitivity is defined as the measure of the minimum signal that a telescope can distinguish above background noise. The SKA sensitivity comes from the combination of radio receivers at the low, mid, and high range frequencies combined to effectively create a single radio telescope 1km wide. Resolution is the measure of the minimum size that a telescope can distinguish, or the cutoff when the telescope produces a blurry image compared to a clear image. The receivers of the SKA are relatively spread out which helps to increase the telescope&amp;rsquo;s resolution [4]&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;The second use case is further exploring the magnetic field that exists in our galaxy.
Magnetic fields are entirely invisible, even to the largest telescopes, so instead the researchers look at various concentrations of radiation. In this section the researchers make an interesting note, they can anticipate the performance of the telescope in studying the known world however they expect that the SKA will open new research questions never before considered. As in the previous use case, the SKA’s resolution will allow researchers a view not previously achieved. The overall goal is to develop a map of the magnetic field through the known universe [5]&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;The third use case is to study the cosmic dawn. With our current technology, such as the
hubble telescope, researchers have only been able to study the first 300,000 years after the big bang. By studying the cosmic microwave background of this time period they are able to get a better understanding of how the universe developed. However the next half billion years gives an even better insight into the scale of the structures created as well as how they began to form and collapse under gravity. The SKA will allow researchers to see into this time period. For this use case, the sensitivity of the SKA is most important [6]&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;]&amp;ldquo;The SKA Project - Public Website&amp;rdquo;, Public Website, 2020. [Online]. Available:
&lt;a href=&#34;https://www.skatelescope.org/the-ska-project/&#34;&gt;https://www.skatelescope.org/the-ska-project/&lt;/a&gt;. [Accessed: 24- Sep- 2020]. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&amp;ldquo;SKA completes final reviews ahead of construction - Public Website&amp;rdquo;, Public Website, 2020.
[Online]. Available:
&lt;a href=&#34;https://www.skatelescope.org/news/ska-completes-final-reviews-ahead-of-construction/&#34;&gt;https://www.skatelescope.org/news/ska-completes-final-reviews-ahead-of-construction/&lt;/a&gt;.
[Accessed: 24- Sep- 2020]. &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&amp;ldquo;Use Case Survey_SKA&amp;rdquo;, Google Docs, 2020. [Online]. Available:
&lt;a href=&#34;https://docs.google.com/document/d/1ZMrga5R_idBcFlhvvhlcOP9aX--bpqhJg4XSS3Qi3ws/edit&#34;&gt;https://docs.google.com/document/d/1ZMrga5R_idBcFlhvvhlcOP9aX--bpqhJg4XSS3Qi3ws/edit&lt;/a&gt;. [Accessed: 24- Sep- 2020]. &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&amp;ldquo;Galaxy Evolution, Cosmology and Dark Energy - Public Website&amp;rdquo;, Public Website, 2020.
[Online]. Available: &lt;a href=&#34;https://www.skatelescope.org/galaxyevolution/&#34;&gt;https://www.skatelescope.org/galaxyevolution/&lt;/a&gt;. [Accessed: 24- Sep- 2020]. &lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&amp;ldquo;Cosmic Magnetism - Public Website&amp;rdquo;, Public Website, 2020. [Online]. Available:
&lt;a href=&#34;https://www.skatelescope.org/magnetism/&#34;&gt;https://www.skatelescope.org/magnetism/&lt;/a&gt;. [Accessed: 24- Sep- 2020]. &lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&amp;ldquo;Probing the Cosmic Dawn - Public Website&amp;rdquo;, Public Website, 2020. [Online]. Available:
&lt;a href=&#34;https://www.skatelescope.org/cosmicdawn/&#34;&gt;https://www.skatelescope.org/cosmicdawn/&lt;/a&gt;. [Accessed: 24- Sep- 2020]. &lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;

      </description>
    </item>
    
    <item>
      <title>Report: </title>
      <link>/report/fa20-523-323/assignments/assignment-6/atugman-assignment6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/report/fa20-523-323/assignments/assignment-6/atugman-assignment6/</guid>
      <description>
        
        
        &lt;p&gt;Anthony Tugman&lt;br&gt;
E534&lt;br&gt;
9/30/20&lt;br&gt;
Assignment 6&lt;/p&gt;
&lt;p&gt;The Covid-19 pandemic has brought a focus to tele-health due to its ability to promote social distancing as well as its ability to handle the sheer number of patients.  The focus on tele-health is most often in its use of connecting patients virtually with healthcare providers.  While this task is accomplished successfully, largely due to the high quality cameras and internet that are widely available, the increased use of tele-health and implementation of big data can bring further benefits.  In fact, it is already making large contributions in the healthcare industry improving care, accuracy, and efficiency for both patients and healthcare providers.  Prior to Covid-19 tele-health adoption was at 11%, increasing to 45% during the pandemic[1]&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.  The increase in adoption has led to a flood in big data.  Through the use of AI analytics, this data can provide countless meaningful insights.&lt;/p&gt;
&lt;p&gt;The technology surrounding tele-health has been under development and deployment for many years, however the pandemic increased adoption and provided a catalyst for this industry.  The overall goal of tele-health is to provide healthcare to underserved individuals through increased connectivity.  With the increase in connectivity comes the increase in big data and connected monitoring devices.  The information and insights produced by these monitoring devices is an untapped trove of medical patterns, statistics, and associated information that can inform healthcare and policymakers in their decisions.  The most important outcome from this is the newly sourced data that can be used in a variety of ways.&lt;/p&gt;
&lt;p&gt;To explore how this technology is being applied to tele-health an example use case of photo recognition for skin conditions will be described.  In itself, the creation of an algorithm to detect skin conditions is nothing novel.  But when the algorithm is making potentially life or death decisions there are some additional intricacies built in.  Google has created an algorithm that is able to identify 26 skin conditions with as much accuracy as trained dermatologists [2]&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;.  To achieve such accuracy the AI system was designed to mimic human dermatologist behaviors.  This means using associated metadata such as region, race, gender, health history, etc. as well as the image itself to make an informed list of possible conditions, just as a dermatologist would.  That is, an actual diagnosis is not being made, but rather suggestions as to how to narrow down lab tests for result determination [2]&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;.  And while the success rate is impressive, this system has drawbacks predicting rare or undertrained conditions.  These drawbacks arise from the limited training datasets available.  In fact HIPAA and additional privacy protections have made it difficult for tele-health with AI to become more mainstream [3]&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;It seems that the patient privacy protections laws are standing in the way between tele-health and its true potential.  While the technology is ready to go, as it has been developed for other fields and would be easy to transfer, time needs to be taken to stand back and establish trust with the public.  To establish public trust, especially concerning sensitive health information, standards, transparency, and accountability must be established.  With the increased adoption of tele-health, now is a better time than ever to gain the public&amp;rsquo;s trust.  Once this has been accomplished it will be possible to amass larger, more organic datasets that contain even the rarest ailments making diagnoses more accurate.&lt;/p&gt;
&lt;p&gt;Although not a goal of the discussed algorithm, one of the most interesting facets of this form of data collection is the patterns that can be observed.  If suddenly all health information was available to researchers it would be possible to see where certain diseases are concentrated, what factors may be contributing, as well as how resources should be distributed.  With the addition of an exponential amount of datasets, these AI systems could even become predictive in use.  That is, they could be used to monitor conditions against a standard to alert of certain abnormalities in the healthcare system such as an incoming pandemic.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&amp;ldquo;How intelligent data transforms health in the time of COVID-19&amp;rdquo;, MobiHealthNews, 2020. [Online]. Available: &lt;a href=&#34;https://www.mobihealthnews.com/news/how-intelligent-data-transforms-health-time-covid&#34;&gt;https://www.mobihealthnews.com/news/how-intelligent-data-transforms-health-time-covid&lt;/a&gt;. [Accessed: 15- Oct- 2020]. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;K. Wiggers, &amp;ldquo;Google says its AI detects 26 skin conditions as accurately as dermatologists&amp;rdquo;, VentureBeat, 2020. [Online]. Available: &lt;a href=&#34;https://venturebeat.com/2019/09/13/googles-ai-detects-26-skin-conditions-as-accurately-as-dermatologists/&#34;&gt;https://venturebeat.com/2019/09/13/googles-ai-detects-26-skin-conditions-as-accurately-as-dermatologists/&lt;/a&gt;. [Accessed: 15- Oct- 2020]. &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&amp;ldquo;How HIPAA Is Undermining IT and AI’s Potential To Make Healthcare Better - Electronic Health Reporter&amp;rdquo;, Electronichealthreporter.com, 2020. [Online]. Available: &lt;a href=&#34;https://electronichealthreporter.com/how-hipaa-is-undermining-it-and-ais-potential-to-make-healthcare-better/&#34;&gt;https://electronichealthreporter.com/how-hipaa-is-undermining-it-and-ais-potential-to-make-healthcare-better/&lt;/a&gt;. [Accessed: 19- Oct- 2020]. &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;

      </description>
    </item>
    
  </channel>
</rss>
